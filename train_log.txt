| Reloading config from: pretrained_models/OmniAvatar-1.3B/config.json
{'config': 'configs/train_1.3B.yaml', 'exp_path': 'pretrained_models/OmniAvatar-1.3B', 'input_file': None, 'debug': True, 'infer': False, 'hparams': '', 'dtype': '16', 'text_encoder_path': 'pretrained_models/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth', 'image_encoder_path': 'None', 'dit_path': 'pretrained_models/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors', 'vae_path': 'pretrained_models/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth', 'wav2vec_path': 'pretrained_models/wav2vec2-base-960h', 'num_persistent_param_in_dit': None, 'reload_cfg': True, 'sp_size': 1, 'seed': 42, 'image_sizes_720': [[400, 720], [720, 720], [720, 400]], 'image_sizes_1280': [[720, 720], [528, 960], [960, 528], [720, 1280], [1280, 720]], 'max_hw': 720, 'max_tokens': 30000, 'seq_len': 200, 'overlap_frame': 13, 'guidance_scale': 4.5, 'audio_scale': None, 'num_steps': 50, 'fps': 20, 'sample_rate': 16000, 'negative_prompt': 'Vivid color tones, background/camera moving quickly, screen switching, subtitles and special effects, mutation, overexposed, static, blurred details, subtitles, style, work, painting, image, still, overall grayish, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn face, deformed, disfigured, malformed limbs, fingers merging, motionless image, chaotic background, three legs, crowded background with many people, walking backward', 'silence_duration_s': 0.3, 'use_fsdp': False, 'tea_cache_l1_thresh': 0, 'dataset_base_path': '/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1', 'name': 'train_1.3B', 'savedir': '/mnt/hdd2/huanglingyu/vgg/OmniAvatar/outputs', 'batch_size': 1, 'nodes': 1, 'devices': 1, 'num_train_epochs': 1, 'mode': 'train', 'checkpoint_path': '', 'lr': '1e-4', 'max_frames': 120, 'debug_data_len': 100, 'rank': 0, 'world_size': 1, 'local_rank': 0, 'device': 'cuda:0', 'num_nodes': 1, 'i2v': True, 'use_audio': True, 'random_prefix_frames': True, 'model_config': {'in_dim': 33}, 'lora_target_modules': 'q,k,v,o,ffn.0,ffn.2', 'init_lora_weights': 'kaiming', 'lora_rank': 128, 'lora_alpha': 64.0, 'use_gradient_checkpointing': True, 'use_gradient_checkpointing_offload': False, 'train_architecture': 'lora'}
[2025-08-14 23:15:19,246] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-14 23:15:20,361] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[train_pl.py]-main-] config: {'dtype': '16', 'text_encoder_path': 'pretrained_models/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth', 'image_encoder_path': 'None', 'dit_path': 'pretrained_models/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors', 'vae_path': 'pretrained_models/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth', 'wav2vec_path': 'pretrained_models/wav2vec2-base-960h', 'exp_path': 'pretrained_models/OmniAvatar-1.3B', 'num_persistent_param_in_dit': None, 'reload_cfg': True, 'sp_size': 1, 'seed': 42, 'image_sizes_720': [[400, 720], [720, 720], [720, 400]], 'image_sizes_1280': [[720, 720], [528, 960], [960, 528], [720, 1280], [1280, 720]], 'max_hw': 720, 'max_tokens': 30000, 'seq_len': 200, 'overlap_frame': 13, 'guidance_scale': 4.5, 'audio_scale': None, 'num_steps': 50, 'fps': 20, 'sample_rate': 16000, 'negative_prompt': 'Vivid color tones, background/camera moving quickly, screen switching, subtitles and special effects, mutation, overexposed, static, blurred details, subtitles, style, work, painting, image, still, overall grayish, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn face, deformed, disfigured, malformed limbs, fingers merging, motionless image, chaotic background, three legs, crowded background with many people, walking backward', 'silence_duration_s': 0.3, 'use_fsdp': False, 'tea_cache_l1_thresh': 0, 'dataset_base_path': '/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1', 'name': 'train_1.3B', 'savedir': '/mnt/hdd2/huanglingyu/vgg/OmniAvatar/outputs', 'batch_size': 1, 'nodes': 1, 'devices': 1, 'num_train_epochs': 1, 'mode': 'train', 'checkpoint_path': '', 'lr': 0.0001, 'max_frames': 120, 'debug': True, 'debug_data_len': 100}
[OmniTrainingModule] __init__
Loading models from: ['pretrained_models/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors']
    model_name: wan_video_dit model_class: WanModel
        This model is initialized with extra kwargs: {'has_image_input': False, 'patch_size': [1, 2, 2], 'in_dim': 33, 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'text_dim': 4096, 'out_dim': 16, 'num_heads': 12, 'num_layers': 30, 'eps': 1e-06}
Using WanModel with dim=1536, in_dim=33, ffn_dim=8960, out_dim=16, text_dim=4096, freq_dim=256, eps=1e-06, patch_size=[1, 2, 2], num_heads=12, num_layers=30, has_image_input=False, audio_hidden_size=32
[AudioPack] in_channels: 10752, t, h, w: 4, 1, 1
[AudioPack] patch_size: (4, 1, 1)
[AudioPack] proj: Linear(in_features=43008, out_features=32, bias=True)
[AudioPack] norm_out: LayerNorm((32,), eps=1e-05, elementwise_affine=True)

[WanModel] patch_embedding: Conv3d(33, 1536, kernel_size=(1, 2, 2), stride=(1, 2, 2))
[WanModel] text_embedding: Sequential(
  (0): Linear(in_features=4096, out_features=1536, bias=True)
  (1): GELU(approximate='tanh')
  (2): Linear(in_features=1536, out_features=1536, bias=True)
)
[WanModel] time_embedding: Sequential(
  (0): Linear(in_features=256, out_features=1536, bias=True)
  (1): SiLU()
  (2): Linear(in_features=1536, out_features=1536, bias=True)
)
[WanModel] time_projection: Sequential(
  (0): SiLU()
  (1): Linear(in_features=1536, out_features=9216, bias=True)
)
[WanModel] blocks (DiTBlock):
  Block 0: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 1: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 2: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 3: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 4: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 5: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 6: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 7: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 8: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 9: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 10: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 11: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 12: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 13: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 14: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 15: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 16: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 17: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 18: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 19: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 20: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 21: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 22: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 23: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 24: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 25: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 26: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 27: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 28: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 29: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
[WanModel] head: Head(
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (head): Linear(in_features=1536, out_features=64, bias=True)
)
[WanModel] RoPE freqs shape: [torch.Size([1024, 22]), torch.Size([1024, 21]), torch.Size([1024, 21])]
[WanModel] audio_proj: AudioPack(
  (proj): Linear(in_features=43008, out_features=32, bias=True)
  (norm_out): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
)
[WanModel] audio_cond_projs:
  Audio Cond Proj 0: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 1: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 2: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 3: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 4: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 5: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 6: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 7: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 8: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 9: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 10: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 11: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 12: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 13: Linear(in_features=32, out_features=1536, bias=True)
[Truncate] patch_embedding.weight: ckpt torch.Size([1536, 16, 1, 2, 2]) -> model torch.Size([1536, 33, 1, 2, 2])
    The following models are loaded: ['wan_video_dit'].
Loading models from: pretrained_models/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth
    model_name: wan_video_text_encoder model_class: WanTextEncoder
[WanTextEncoder] token_embedding: Embedding(256384, 4096)
[WanTextEncoder] dropout: Dropout(p=0.1, inplace=False)
[WanTextEncoder] blocks (T5SelfAttention):
  Block 0: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 1: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 2: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 3: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 4: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 5: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 6: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 7: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 8: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 9: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 10: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 11: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 12: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 13: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 14: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 15: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 16: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 17: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 18: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 19: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 20: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 21: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 22: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 23: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
[WanTextEncoder] norm: T5LayerNorm()
    The following models are loaded: ['wan_video_text_encoder'].
Loading models from: pretrained_models/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth
    model_name: wan_video_vae model_class: WanVideoVAE

[VideoVAE_] encoder: Encoder3d(
  (conv1): CausalConv3d(3, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
  (downsamples): Sequential(
    (0): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (1): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (2): Resample(
      (resample): Sequential(
        (0): ZeroPad2d((0, 1, 0, 1))
        (1): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2))
      )
    )
    (3): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(96, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): CausalConv3d(96, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    )
    (4): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (5): Resample(
      (resample): Sequential(
        (0): ZeroPad2d((0, 1, 0, 1))
        (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2))
      )
      (time_conv): CausalConv3d(192, 192, kernel_size=(3, 1, 1), stride=(2, 1, 1))
    )
    (6): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(192, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): CausalConv3d(192, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    )
    (7): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (8): Resample(
      (resample): Sequential(
        (0): ZeroPad2d((0, 1, 0, 1))
        (1): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2))
      )
      (time_conv): CausalConv3d(384, 384, kernel_size=(3, 1, 1), stride=(2, 1, 1))
    )
    (9): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (10): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
  )
  (middle): Sequential(
    (0): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (1): AttentionBlock(
      (norm): RMS_norm()
      (to_qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
      (proj): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
    )
    (2): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
  )
  (head): Sequential(
    (0): RMS_norm()
    (1): SiLU()
    (2): CausalConv3d(384, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))
  )
)
[VideoVAE_] conv1: CausalConv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))
[VideoVAE_] conv2: CausalConv3d(16, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))
[VideoVAE_] decoder: Decoder3d(
  (conv1): CausalConv3d(16, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
  (middle): Sequential(
    (0): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (1): AttentionBlock(
      (norm): RMS_norm()
      (to_qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
      (proj): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
    )
    (2): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
  )
  (upsamples): Sequential(
    (0): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (1): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (2): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (3): Resample(
      (resample): Sequential(
        (0): Upsample(scale_factor=(2.0, 2.0), mode='nearest-exact')
        (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (time_conv): CausalConv3d(384, 768, kernel_size=(3, 1, 1), stride=(1, 1, 1))
    )
    (4): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(192, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): CausalConv3d(192, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    )
    (5): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (6): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (7): Resample(
      (resample): Sequential(
        (0): Upsample(scale_factor=(2.0, 2.0), mode='nearest-exact')
        (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (time_conv): CausalConv3d(384, 768, kernel_size=(3, 1, 1), stride=(1, 1, 1))
    )
    (8): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (9): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (10): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (11): Resample(
      (resample): Sequential(
        (0): Upsample(scale_factor=(2.0, 2.0), mode='nearest-exact')
        (1): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (12): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (13): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (14): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
  )
  (head): Sequential(
    (0): RMS_norm()
    (1): SiLU()
    (2): CausalConv3d(96, 3, kernel_size=(3, 3, 3), stride=(1, 1, 1))
  )
)
    The following models are loaded: ['wan_video_vae'].
Using wan_video_text_encoder from pretrained_models/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth.
Using wan_video_dit from ['pretrained_models/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors'].
Using wan_video_vae from pretrained_models/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth.
No wan_video_image_encoder models available.
[OmniTrainingModule load_model] -> Use LoRA: lora rank: 128, lora alpha: 64.0, pretrained_lora_path: pretrained_models/OmniAvatar-1.3B/pytorch_model.pt
[OmniTrainingModule add_lora_to_model] -> lora_config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'q', 'v', 'k', 'o', 'ffn.0', 'ffn.2'}, exclude_modules=None, lora_alpha=64.0, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
[OmniTrainingModule add_lora_to_model] -> 634 parameters are loaded from pretrained_models/OmniAvatar-1.3B/pytorch_model.pt. 0 parameters are unexpected.
[OmniTrainingModule __init__]: Model loaded on cpu, dtype: torch.float32
===================[train_pl.py]-main-] model summary================================
[OmniTrainingModule] - name: pipe.text_encoder.token_embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.norm.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.patch_embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.patch_embedding.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.text_embedding.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.text_embedding.0.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.text_embedding.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.text_embedding.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.time_embedding.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.time_embedding.0.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.time_embedding.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.time_embedding.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.time_projection.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.time_projection.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.head.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.head.head.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.head.head.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_proj.proj.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_proj.proj.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_proj.norm_out.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_proj.norm_out.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.0.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.4.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.4.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.5.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.5.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.7.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.7.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.8.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.8.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.9.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.9.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.10.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.10.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.11.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.11.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.12.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.12.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.13.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.13.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.conv1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.conv1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.0.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.0.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.0.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.0.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.0.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.0.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.1.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.1.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.1.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.1.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.1.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.1.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.2.resample.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.2.resample.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.shortcut.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.shortcut.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.4.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.4.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.4.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.4.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.4.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.4.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.5.resample.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.5.resample.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.5.time_conv.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.5.time_conv.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.shortcut.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.shortcut.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.7.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.7.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.7.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.7.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.7.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.7.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.8.resample.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.8.resample.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.8.time_conv.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.8.time_conv.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.9.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.9.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.9.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.9.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.9.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.9.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.10.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.10.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.10.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.10.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.10.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.10.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.0.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.0.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.0.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.0.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.0.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.0.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.1.norm.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.1.to_qkv.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.1.to_qkv.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.1.proj.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.1.proj.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.2.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.2.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.2.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.2.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.2.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.2.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.head.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.head.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.head.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.conv1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.conv1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.conv2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.conv2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.conv1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.conv1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.0.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.0.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.0.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.0.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.0.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.0.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.1.norm.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.1.to_qkv.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.1.to_qkv.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.1.proj.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.1.proj.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.2.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.2.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.2.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.2.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.2.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.2.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.0.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.0.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.0.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.0.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.0.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.0.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.1.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.1.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.1.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.1.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.1.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.1.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.2.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.2.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.2.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.2.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.2.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.2.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.3.resample.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.3.resample.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.3.time_conv.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.3.time_conv.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.shortcut.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.shortcut.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.5.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.5.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.5.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.5.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.5.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.5.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.6.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.6.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.6.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.6.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.6.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.6.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.7.resample.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.7.resample.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.7.time_conv.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.7.time_conv.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.8.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.8.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.8.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.8.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.8.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.8.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.9.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.9.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.9.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.9.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.9.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.9.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.10.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.10.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.10.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.10.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.10.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.10.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.11.resample.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.11.resample.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.12.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.12.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.12.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.12.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.12.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.12.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.13.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.13.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.13.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.13.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.13.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.13.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.14.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.14.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.14.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.14.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.14.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.14.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.head.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.head.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.head.2.bias, requires_grad: False
[OmniTrainingModule] - name: audio_encoder.masked_spec_embed, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.0.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.0.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.0.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.1.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.2.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.3.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.4.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.5.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.6.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_projection.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_projection.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_projection.projection.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_projection.projection.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.pos_conv_embed.conv.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.pos_conv_embed.conv.parametrizations.weight.original0, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.pos_conv_embed.conv.parametrizations.weight.original1, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.final_layer_norm.bias, requires_grad: True
===================================================================================
[OmniTrainingModule]: start training with config: {'dtype': '16', 'text_encoder_path': 'pretrained_models/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth', 'image_encoder_path': 'None', 'dit_path': 'pretrained_models/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors', 'vae_path': 'pretrained_models/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth', 'wav2vec_path': 'pretrained_models/wav2vec2-base-960h', 'exp_path': 'pretrained_models/OmniAvatar-1.3B', 'num_persistent_param_in_dit': None, 'reload_cfg': True, 'sp_size': 1, 'seed': 42, 'image_sizes_720': [[400, 720], [720, 720], [720, 400]], 'image_sizes_1280': [[720, 720], [528, 960], [960, 528], [720, 1280], [1280, 720]], 'max_hw': 720, 'max_tokens': 30000, 'seq_len': 200, 'overlap_frame': 13, 'guidance_scale': 4.5, 'audio_scale': None, 'num_steps': 50, 'fps': 20, 'sample_rate': 16000, 'negative_prompt': 'Vivid color tones, background/camera moving quickly, screen switching, subtitles and special effects, mutation, overexposed, static, blurred details, subtitles, style, work, painting, image, still, overall grayish, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn face, deformed, disfigured, malformed limbs, fingers merging, motionless image, chaotic background, three legs, crowded background with many people, walking backward', 'silence_duration_s': 0.3, 'use_fsdp': False, 'tea_cache_l1_thresh': 0, 'dataset_base_path': '/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1', 'name': 'train_1.3B', 'savedir': '/mnt/hdd2/huanglingyu/vgg/OmniAvatar/outputs/train_1.3B', 'batch_size': 1, 'nodes': 1, 'devices': 1, 'num_train_epochs': 1, 'mode': 'train', 'checkpoint_path': '', 'lr': 0.0001, 'max_frames': 120, 'debug': True, 'debug_data_len': 100}
[OmniTrainingModule] configure_optimizers
[OmniTrainingModule] on_fit_start -> device: cuda:0, param device: cuda:0
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s][OmniTrainingModule] validation_step, args: ({'video_id': ['3Xa0YD9VhIE_11'], 'prompt': ["an animated character, resembling Finn from the animated series Adventure Time, navigating through a forest filled with trees and scattered debris. The character is seen pushing and pulling wooden logs, creating a makeshift bridge or pathway. The scene is set in a whimsical, cartoonish forest with vibrant pink trees and scattered objects, including a cake and a small animal. The character's movements are deliberate and careful as they maneuver the logs, showcasing a sense of determination and resourcefulness. The background consists of a whimsical forest with tall, pink-colored trees and scattered debris. The ground is covered with grass and small rocks, and there are additional trees and objects scattered throughout the scene. The forest appears to be in a cartoonish, stylized setting, with vibrant colors and a playful atmosphere. The main subject is Finn, an animated character with blue shorts, a green backpack, and a light blue shirt. He is interacting with wooden logs, pushing and pulling them to create a pathway. The character is positioned centrally in the frame, moving from left to right, and is focused on his task. The logs are positioned horizontally across the ground, and Finn is seen pushing and pulling them to create a makeshift bridge. The camera is stationary, providing a wide-angle view of the scene, capturing the entirety of Finn's actions and the surrounding forest environment."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/3Xa0YD9VhIE_11/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/3Xa0YD9VhIE_11/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/3Xa0YD9VhIE_11/first_frame.png'], 'video': tensor([[[[[0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6157, 0.6157],
           [0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6157, 0.6157],
           [0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6157, 0.6157],
           ...,
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845]],

          [[0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6157, 0.6157],
           [0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6157, 0.6157],
           [0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6157, 0.6157],
           ...,
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845]],

          [[0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6157, 0.6157],
           [0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6157, 0.6157],
           [0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6157, 0.6157],
           ...,
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845]],

          ...,

          [[0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6157, 0.6157],
           [0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6157, 0.6157],
           [0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6157, 0.6157],
           ...,
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845]],

          [[0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6157, 0.6157],
           [0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6157, 0.6157],
           [0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6157, 0.6157],
           ...,
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845]],

          [[0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6157, 0.6157],
           [0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6157, 0.6157],
           [0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6157, 0.6157],
           ...,
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845]]],


         [[[0.2825, 0.2825, 0.2825,  ..., 0.1569, 0.1530, 0.1530],
           [0.2825, 0.2825, 0.2825,  ..., 0.1569, 0.1530, 0.1530],
           [0.2825, 0.2825, 0.2825,  ..., 0.1569, 0.1530, 0.1530],
           ...,
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119],
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119],
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119]],

          [[0.2825, 0.2825, 0.2825,  ..., 0.1569, 0.1530, 0.1530],
           [0.2825, 0.2825, 0.2825,  ..., 0.1569, 0.1530, 0.1530],
           [0.2825, 0.2825, 0.2825,  ..., 0.1569, 0.1530, 0.1530],
           ...,
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119],
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119],
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119]],

          [[0.2825, 0.2825, 0.2825,  ..., 0.1569, 0.1530, 0.1530],
           [0.2825, 0.2825, 0.2825,  ..., 0.1569, 0.1530, 0.1530],
           [0.2825, 0.2825, 0.2825,  ..., 0.1569, 0.1530, 0.1530],
           ...,
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119],
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119],
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119]],

          ...,

          [[0.2825, 0.2825, 0.2825,  ..., 0.1569, 0.1530, 0.1530],
           [0.2825, 0.2825, 0.2825,  ..., 0.1569, 0.1530, 0.1530],
           [0.2825, 0.2825, 0.2825,  ..., 0.1569, 0.1530, 0.1530],
           ...,
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119],
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119],
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119]],

          [[0.2825, 0.2825, 0.2825,  ..., 0.1569, 0.1530, 0.1530],
           [0.2825, 0.2825, 0.2825,  ..., 0.1569, 0.1530, 0.1530],
           [0.2825, 0.2825, 0.2825,  ..., 0.1569, 0.1530, 0.1530],
           ...,
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119],
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119],
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119]],

          [[0.2825, 0.2825, 0.2825,  ..., 0.1569, 0.1530, 0.1530],
           [0.2825, 0.2825, 0.2825,  ..., 0.1569, 0.1530, 0.1530],
           [0.2825, 0.2825, 0.2825,  ..., 0.1569, 0.1530, 0.1530],
           ...,
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119],
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119],
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119]]],


         [[[0.3687, 0.3687, 0.3687,  ..., 0.2825, 0.2903, 0.2903],
           [0.3687, 0.3687, 0.3687,  ..., 0.2825, 0.2903, 0.2903],
           [0.3687, 0.3687, 0.3687,  ..., 0.2825, 0.2903, 0.2903],
           ...,
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471],
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471],
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471]],

          [[0.3687, 0.3687, 0.3687,  ..., 0.2825, 0.2903, 0.2903],
           [0.3687, 0.3687, 0.3687,  ..., 0.2825, 0.2903, 0.2903],
           [0.3687, 0.3687, 0.3687,  ..., 0.2825, 0.2903, 0.2903],
           ...,
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471],
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471],
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471]],

          [[0.3687, 0.3687, 0.3687,  ..., 0.2825, 0.2903, 0.2903],
           [0.3687, 0.3687, 0.3687,  ..., 0.2825, 0.2903, 0.2903],
           [0.3687, 0.3687, 0.3687,  ..., 0.2825, 0.2903, 0.2903],
           ...,
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471],
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471],
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471]],

          ...,

          [[0.3687, 0.3687, 0.3687,  ..., 0.2825, 0.2903, 0.2903],
           [0.3687, 0.3687, 0.3687,  ..., 0.2825, 0.2903, 0.2903],
           [0.3687, 0.3687, 0.3687,  ..., 0.2825, 0.2903, 0.2903],
           ...,
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471],
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471],
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471]],

          [[0.3687, 0.3687, 0.3687,  ..., 0.2825, 0.2903, 0.2903],
           [0.3687, 0.3687, 0.3687,  ..., 0.2825, 0.2903, 0.2903],
           [0.3687, 0.3687, 0.3687,  ..., 0.2825, 0.2903, 0.2903],
           ...,
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471],
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471],
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471]],

          [[0.3687, 0.3687, 0.3687,  ..., 0.2825, 0.2903, 0.2903],
           [0.3687, 0.3687, 0.3687,  ..., 0.2825, 0.2903, 0.2903],
           [0.3687, 0.3687, 0.3687,  ..., 0.2825, 0.2903, 0.2903],
           ...,
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471],
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471],
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471]]]]],
       device='cuda:0', dtype=torch.float16), 'audio': tensor([[-0.9980, -1.5186, -2.3379,  ...,  1.3828,  1.0234,  0.5913]],
       device='cuda:0', dtype=torch.float16), 'L': tensor([25], device='cuda:0'), 'T': tensor([7], device='cuda:0')}, 0), kwargs keys: dict_keys([])
Sanity Checking DataLoader 0:  50%|     | 1/2 [00:00<00:00,  5.13it/s][OmniTrainingModule] validation_step, args: ({'video_id': ['CNPcFgo8StY_26'], 'prompt': ["A young woman with long blonde hair is featured in a series of video frames, showcasing various poses and expressions. She is dressed in a white blouse with a playful pattern of cartoon characters and is seen smiling, making heart shapes with her hands, and adjusting her hair. The background is minimalistic, featuring a softly lit, white wall with string lights that add a cozy and warm ambiance to the scene. The woman's movements are smooth and deliberate, involving slight head tilts, hand gestures, and hair adjustments. She transitions from a neutral expression to smiling and making heart shapes with her hands, then adjusting her hair. The movements are slow and graceful, emphasizing her relaxed and confident demeanor. The background remains static throughout the video, with no changes or movements. The main subject is a young woman with long blonde hair, wearing a white blouse with cartoon character prints. She is positioned centrally in the frame and is the focal point of the video. Her expressions range from neutral to smiling, and she engages in various poses, including making heart shapes with her hands and adjusting her hair. The camera is stationary, maintaining a medium close-up view of the woman, capturing her upper body and face clearly."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/CNPcFgo8StY_26/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/CNPcFgo8StY_26/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/CNPcFgo8StY_26/first_frame.png'], 'video': tensor([[[[[0.8823, 0.8823, 0.8823,  ..., 0.1569, 0.1569, 0.1569],
           [0.8823, 0.8823, 0.8823,  ..., 0.1569, 0.1569, 0.1569],
           [0.8823, 0.8823, 0.8823,  ..., 0.1569, 0.1598, 0.1598],
           ...,
           [0.7881, 0.7881, 0.7881,  ..., 0.9272, 0.9272, 0.9272],
           [0.7881, 0.7881, 0.7881,  ..., 0.9292, 0.9292, 0.9292],
           [0.7881, 0.7881, 0.7881,  ..., 0.9058, 0.9058, 0.9058]],

          [[0.8823, 0.8823, 0.8823,  ..., 0.1569, 0.1569, 0.1569],
           [0.8823, 0.8823, 0.8823,  ..., 0.1569, 0.1569, 0.1569],
           [0.8823, 0.8823, 0.8823,  ..., 0.1569, 0.1598, 0.1598],
           ...,
           [0.7881, 0.7881, 0.7881,  ..., 0.9272, 0.9272, 0.9272],
           [0.7881, 0.7881, 0.7881,  ..., 0.9292, 0.9292, 0.9292],
           [0.7881, 0.7881, 0.7881,  ..., 0.9058, 0.9058, 0.9058]],

          [[0.8823, 0.8823, 0.8823,  ..., 0.1569, 0.1569, 0.1569],
           [0.8823, 0.8823, 0.8823,  ..., 0.1569, 0.1569, 0.1569],
           [0.8823, 0.8823, 0.8823,  ..., 0.1569, 0.1598, 0.1598],
           ...,
           [0.7881, 0.7881, 0.7881,  ..., 0.9351, 0.9351, 0.9351],
           [0.7881, 0.7881, 0.7881,  ..., 0.9375, 0.9375, 0.9375],
           [0.7881, 0.7881, 0.7881,  ..., 0.9175, 0.9175, 0.9175]],

          ...,

          [[0.8823, 0.8823, 0.8823,  ..., 0.1530, 0.1608, 0.1647],
           [0.8823, 0.8823, 0.8823,  ..., 0.1530, 0.1608, 0.1647],
           [0.8823, 0.8823, 0.8823,  ..., 0.1559, 0.1637, 0.1676],
           ...,
           [0.7959, 0.7959, 0.7959,  ..., 0.9414, 0.9414, 0.9414],
           [0.7959, 0.7959, 0.7959,  ..., 0.9390, 0.9390, 0.9390],
           [0.7959, 0.7959, 0.7959,  ..., 0.9253, 0.9253, 0.9253]],

          [[0.8823, 0.8823, 0.8823,  ..., 0.1530, 0.1608, 0.1647],
           [0.8823, 0.8823, 0.8823,  ..., 0.1530, 0.1608, 0.1647],
           [0.8823, 0.8823, 0.8823,  ..., 0.1559, 0.1637, 0.1676],
           ...,
           [0.7959, 0.7959, 0.7959,  ..., 0.9414, 0.9414, 0.9414],
           [0.7959, 0.7959, 0.7959,  ..., 0.9390, 0.9390, 0.9390],
           [0.7959, 0.7959, 0.7959,  ..., 0.9253, 0.9253, 0.9253]],

          [[0.8823, 0.8823, 0.8823,  ..., 0.1530, 0.1608, 0.1647],
           [0.8823, 0.8823, 0.8823,  ..., 0.1530, 0.1608, 0.1647],
           [0.8823, 0.8823, 0.8823,  ..., 0.1559, 0.1637, 0.1676],
           ...,
           [0.7959, 0.7959, 0.7959,  ..., 0.9414, 0.9414, 0.9414],
           [0.7959, 0.7959, 0.7959,  ..., 0.9390, 0.9390, 0.9390],
           [0.7959, 0.7959, 0.7959,  ..., 0.9253, 0.9253, 0.9253]]],


         [[[0.8862, 0.8862, 0.8862,  ..., 0.1372, 0.1372, 0.1372],
           [0.8862, 0.8862, 0.8862,  ..., 0.1372, 0.1372, 0.1372],
           [0.8862, 0.8862, 0.8862,  ..., 0.1372, 0.1401, 0.1401],
           ...,
           [0.7686, 0.7686, 0.7686,  ..., 0.9116, 0.9116, 0.9116],
           [0.7686, 0.7686, 0.7686,  ..., 0.9136, 0.9136, 0.9136],
           [0.7686, 0.7686, 0.7686,  ..., 0.8901, 0.8901, 0.8901]],

          [[0.8862, 0.8862, 0.8862,  ..., 0.1372, 0.1372, 0.1372],
           [0.8862, 0.8862, 0.8862,  ..., 0.1372, 0.1372, 0.1372],
           [0.8862, 0.8862, 0.8862,  ..., 0.1372, 0.1401, 0.1401],
           ...,
           [0.7686, 0.7686, 0.7686,  ..., 0.9116, 0.9116, 0.9116],
           [0.7686, 0.7686, 0.7686,  ..., 0.9136, 0.9136, 0.9136],
           [0.7686, 0.7686, 0.7686,  ..., 0.8901, 0.8901, 0.8901]],

          [[0.8862, 0.8862, 0.8862,  ..., 0.1372, 0.1372, 0.1372],
           [0.8862, 0.8862, 0.8862,  ..., 0.1372, 0.1372, 0.1372],
           [0.8862, 0.8862, 0.8862,  ..., 0.1372, 0.1401, 0.1401],
           ...,
           [0.7686, 0.7686, 0.7686,  ..., 0.9194, 0.9194, 0.9194],
           [0.7686, 0.7686, 0.7686,  ..., 0.9219, 0.9219, 0.9219],
           [0.7686, 0.7686, 0.7686,  ..., 0.9019, 0.9019, 0.9019]],

          ...,

          [[0.8862, 0.8862, 0.8862,  ..., 0.1333, 0.1412, 0.1451],
           [0.8862, 0.8862, 0.8862,  ..., 0.1333, 0.1412, 0.1451],
           [0.8862, 0.8862, 0.8862,  ..., 0.1362, 0.1442, 0.1481],
           ...,
           [0.7646, 0.7646, 0.7646,  ..., 0.9136, 0.9136, 0.9136],
           [0.7646, 0.7646, 0.7646,  ..., 0.9111, 0.9111, 0.9111],
           [0.7646, 0.7646, 0.7646,  ..., 0.8979, 0.8979, 0.8979]],

          [[0.8862, 0.8862, 0.8862,  ..., 0.1333, 0.1412, 0.1451],
           [0.8862, 0.8862, 0.8862,  ..., 0.1333, 0.1412, 0.1451],
           [0.8862, 0.8862, 0.8862,  ..., 0.1362, 0.1442, 0.1481],
           ...,
           [0.7646, 0.7646, 0.7646,  ..., 0.9136, 0.9136, 0.9136],
           [0.7646, 0.7646, 0.7646,  ..., 0.9111, 0.9111, 0.9111],
           [0.7646, 0.7646, 0.7646,  ..., 0.8979, 0.8979, 0.8979]],

          [[0.8862, 0.8862, 0.8862,  ..., 0.1333, 0.1412, 0.1451],
           [0.8862, 0.8862, 0.8862,  ..., 0.1333, 0.1412, 0.1451],
           [0.8862, 0.8862, 0.8862,  ..., 0.1362, 0.1442, 0.1481],
           ...,
           [0.7646, 0.7646, 0.7646,  ..., 0.9136, 0.9136, 0.9136],
           [0.7646, 0.7646, 0.7646,  ..., 0.9111, 0.9111, 0.9111],
           [0.7646, 0.7646, 0.7646,  ..., 0.8979, 0.8979, 0.8979]]],


         [[[0.8550, 0.8550, 0.8550,  ..., 0.1255, 0.1255, 0.1255],
           [0.8550, 0.8550, 0.8550,  ..., 0.1255, 0.1255, 0.1255],
           [0.8550, 0.8550, 0.8550,  ..., 0.1255, 0.1284, 0.1284],
           ...,
           [0.7217, 0.7217, 0.7217,  ..., 0.8452, 0.8452, 0.8452],
           [0.7217, 0.7217, 0.7217,  ..., 0.8467, 0.8467, 0.8467],
           [0.7217, 0.7217, 0.7217,  ..., 0.8237, 0.8237, 0.8237]],

          [[0.8550, 0.8550, 0.8550,  ..., 0.1255, 0.1255, 0.1255],
           [0.8550, 0.8550, 0.8550,  ..., 0.1255, 0.1255, 0.1255],
           [0.8550, 0.8550, 0.8550,  ..., 0.1255, 0.1284, 0.1284],
           ...,
           [0.7217, 0.7217, 0.7217,  ..., 0.8452, 0.8452, 0.8452],
           [0.7217, 0.7217, 0.7217,  ..., 0.8467, 0.8467, 0.8467],
           [0.7217, 0.7217, 0.7217,  ..., 0.8237, 0.8237, 0.8237]],

          [[0.8550, 0.8550, 0.8550,  ..., 0.1255, 0.1255, 0.1255],
           [0.8550, 0.8550, 0.8550,  ..., 0.1255, 0.1255, 0.1255],
           [0.8550, 0.8550, 0.8550,  ..., 0.1255, 0.1284, 0.1284],
           ...,
           [0.7217, 0.7217, 0.7217,  ..., 0.8530, 0.8530, 0.8530],
           [0.7217, 0.7217, 0.7217,  ..., 0.8555, 0.8555, 0.8555],
           [0.7217, 0.7217, 0.7217,  ..., 0.8354, 0.8354, 0.8354]],

          ...,

          [[0.8550, 0.8550, 0.8550,  ..., 0.1216, 0.1294, 0.1333],
           [0.8550, 0.8550, 0.8550,  ..., 0.1216, 0.1294, 0.1333],
           [0.8550, 0.8550, 0.8550,  ..., 0.1245, 0.1323, 0.1362],
           ...,
           [0.7217, 0.7217, 0.7217,  ..., 0.8354, 0.8354, 0.8354],
           [0.7217, 0.7217, 0.7217,  ..., 0.8330, 0.8330, 0.8330],
           [0.7217, 0.7217, 0.7217,  ..., 0.8198, 0.8198, 0.8198]],

          [[0.8550, 0.8550, 0.8550,  ..., 0.1216, 0.1294, 0.1333],
           [0.8550, 0.8550, 0.8550,  ..., 0.1216, 0.1294, 0.1333],
           [0.8550, 0.8550, 0.8550,  ..., 0.1245, 0.1323, 0.1362],
           ...,
           [0.7217, 0.7217, 0.7217,  ..., 0.8354, 0.8354, 0.8354],
           [0.7217, 0.7217, 0.7217,  ..., 0.8330, 0.8330, 0.8330],
           [0.7217, 0.7217, 0.7217,  ..., 0.8198, 0.8198, 0.8198]],

          [[0.8550, 0.8550, 0.8550,  ..., 0.1216, 0.1294, 0.1333],
           [0.8550, 0.8550, 0.8550,  ..., 0.1216, 0.1294, 0.1333],
           [0.8550, 0.8550, 0.8550,  ..., 0.1245, 0.1323, 0.1362],
           ...,
           [0.7217, 0.7217, 0.7217,  ..., 0.8354, 0.8354, 0.8354],
           [0.7217, 0.7217, 0.7217,  ..., 0.8330, 0.8330, 0.8330],
           [0.7217, 0.7217, 0.7217,  ..., 0.8198, 0.8198, 0.8198]]]]],
       device='cuda:0', dtype=torch.float16), 'audio': tensor([[1.3525, 1.3086, 1.2920,  ..., 0.3030, 0.3140, 0.4233]],
       device='cuda:0', dtype=torch.float16), 'L': tensor([25], device='cuda:0'), 'T': tensor([7], device='cuda:0')}, 1), kwargs keys: dict_keys([])
Sanity Checking DataLoader 0: 100%|| 2/2 [00:01<00:00,  1.18it/s]                                                                           Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/100 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/100 [00:00<?, ?it/s] [OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 0, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[Timer] VAE: 7.795 
[Timer] forward_preprocess : 8.200 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=True, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([917.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['a conversation between two animated characters in a simple, minimalistic office setting. The character on the left, wearing glasses and a white lab coat, appears to be explaining something to the character on the right, who is dressed in a black shirt. The scene is set against a backdrop of a bookshelf filled with books, creating a professional and intellectual atmosphere. The main subjects are two animated characters. The character on the left has short black hair, wears glasses, a white lab coat, and a black shirt underneath. He is seated and appears to be speaking or explaining something. The character on the right has short hair, wears a black shirt, and is seated facing the left character. The two characters are positioned side by side, with the left character facing the camera and the right character facing the left character. The background consists of a simple office setting with a bookshelf filled with books. The bookshelf is made of wood and has a light brown color. The wall behind the bookshelf is a light yellow color, providing a clean and professional backdrop. The scene is well-lit, suggesting an indoor environment. The camera is stationary, providing a medium shot of the two characters from a frontal view. There is no camera movement, maintaining a steady and focused perspective on the conversation.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=True, inf=False, value=nan
[OmniTrainingModule] training_step -> loss: nan
Epoch 0:   1%|          | 1/100 [00:49<1:22:02,  0.02it/s]Epoch 0:   1%|          | 1/100 [00:49<1:22:02,  0.02it/s, v_num=45, train_loss_step=nan.0][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 1, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[Timer] VAE: 2.600 
[Timer] forward_preprocess : 2.823 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=True, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([692.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['a whimsical scene with three animated characters drawn in a simple, cartoonish style. The characters are situated in a kitchen setting, with various kitchen items and decorations visible in the background. The characters interact with each other, engaging in playful and humorous actions. their movements and expressions, creating a light-hearted and entertaining atmosphere. The main subjects are three animated characters. The first character is a large, smiling creature with a round body and a large, cheerful smile. The second character is a smaller, more reserved creature with a round body and a simple, content expression. The third character is a small, round creature with a large, surprised expression. These characters are positioned close to each other, interacting and moving around. The background features a kitchen setting with various kitchen items and decorations. There are pots, pans, a stove, and other kitchen utensils. The scene is set against a light, beige background, giving it a warm and cozy feel. The kitchen items are arranged in a way that suggests a well-used and lived-in space. The camera is stationary, providing a fixed view of the scene. The perspective is slightly angled, capturing the characters and the background in a single, continuous shot.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=True, inf=False, value=nan
[OmniTrainingModule] training_step -> loss: nan
Epoch 0:   2%|         | 2/100 [00:55<45:23,  0.04it/s, v_num=45, train_loss_step=nan.0]  Epoch 0:   2%|         | 2/100 [00:55<45:23,  0.04it/s, v_num=45, train_loss_step=nan.0][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 2, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[Timer] VAE: 2.878 
[Timer] forward_preprocess : 2.896 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=True, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([448.7500], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['a playful and colorful scene of two animated characters, a red and yellow snail and a green and yellow fish, interacting with a black trash can filled with various items. The snail and fish are positioned on the ground next to the trash can, which is covered in a mix of trash and colorful items. The scene is set against a simple, industrial background with a gray wall and a concrete floor. The characters exhibit animated expressions and movements, creating a lively and engaging atmosphere. The background consists of a simple, industrial setting with a gray wall and a concrete floor. The trash can is the main object in the scene, filled with various items including a green and yellow fish, a red and yellow snail, a blue and yellow cup, a green and yellow spoon, and a black plastic bag. The scene is well-lit, with a focus on the characters and the trash can, creating a clear and detailed visual. The main subjects are a red and yellow snail and a green and yellow fish. The snail is positioned on the left side of the frame, while the fish is on the right. Both characters have expressive faces and are animated, with the snail appearing to be excited and the fish looking surprised. The trash can, which is filled with various items, is positioned behind the characters, with the items spilling out onto the ground. The characters interact with each other and the trash can, creating a dynamic scene. The camera is stationary, providing a fixed view of the scene from a slightly elevated angle, capturing the characters and the trash can in a clear and detailed manner.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=True, inf=False, value=nan
[OmniTrainingModule] training_step -> loss: nan
Epoch 0:   3%|         | 3/100 [01:01<33:05,  0.05it/s, v_num=45, train_loss_step=nan.0]Epoch 0:   3%|         | 3/100 [01:01<33:05,  0.05it/s, v_num=45, train_loss_step=nan.0][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 3, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[Timer] VAE: 2.614 
[Timer] forward_preprocess : 2.636 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=True, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([886.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['A person is standing on a stage, presenting or speaking in front of an audience.The individual is wearing a purple hoodie with a white logo on the left chest area, dark pants, and black shoes. The person\'s hair is dark and appears to be shoulder-length. The attire suggests a casual yet possibly branded look, possibly indicating a gaming or entertainment context.The setting is a stage with a red backdrop, which is illuminated by bright stage lights. There is a large screen displaying a cartoon-style character with the text "HIBYMC" in the background. The stage has a patterned carpet, and the audience is seated in the dark, indicating that the event is likely indoors and possibly during the evening.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=True, inf=False, value=nan
[OmniTrainingModule] training_step -> loss: nan
Epoch 0:   4%|         | 4/100 [01:20<32:14,  0.05it/s, v_num=45, train_loss_step=nan.0]Epoch 0:   4%|         | 4/100 [01:20<32:14,  0.05it/s, v_num=45, train_loss_step=nan.0][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 4, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[Timer] VAE: 2.538 
[Timer] forward_preprocess : 2.563 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=True, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([943.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["A man wearing a yellow face mask with a cartoon character design is sitting in the back seat of a car, engaging in a conversation with someone outside the frame. He is wearing a red shirt and gesturing with his hands, occasionally pointing towards the window. The car is parked in a sunny outdoor setting with trees and a bright sky visible through the window. The man's expressions and hand movements suggest he is explaining or discussing something with the person outside the car. The main subject is a man wearing a yellow face mask with a cartoon character design and a red shirt. He is seated in the back seat of a car, facing towards the front. His hands are frequently in motion, gesturing and pointing towards the window. The man's expressions and hand movements indicate he is actively communicating or explaining something. The background consists of the interior of a car, with the back seat visible. The car is parked outdoors, with trees and a bright sky visible through the window. The sunlight creates a bright and warm atmosphere inside the car. The man's movements are primarily hand gestures, including pointing and waving. His hands move in various directions, primarily towards the window. The background remains static, with no significant changes or movements. The camera is stationary, capturing a medium shot of the man from the back seat of the car. The view is slightly angled to include both the man and the window, providing a clear view of his gestures and expressions."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=True, inf=False, value=nan
[OmniTrainingModule] training_step -> loss: nan
Epoch 0:   5%|         | 5/100 [01:25<27:08,  0.06it/s, v_num=45, train_loss_step=nan.0]Epoch 0:   5%|         | 5/100 [01:25<27:08,  0.06it/s, v_num=45, train_loss_step=nan.0][WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 256 to 281
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 172 to 197
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 37 to 62
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 13 to 38
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 68 to 93
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 80 to 105
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 273 to 298
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 161 to 186
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 49 to 74
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 36 to 61

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['3Xa0YD9VhIE_11'], 'prompt': ["an animated character, resembling Finn from the animated series Adventure Time, navigating through a forest filled with trees and scattered debris. The character is seen pushing and pulling wooden logs, creating a makeshift bridge or pathway. The scene is set in a whimsical, cartoonish forest with vibrant pink trees and scattered objects, including a cake and a small animal. The character's movements are deliberate and careful as they maneuver the logs, showcasing a sense of determination and resourcefulness. The background consists of a whimsical forest with tall, pink-colored trees and scattered debris. The ground is covered with grass and small rocks, and there are additional trees and objects scattered throughout the scene. The forest appears to be in a cartoonish, stylized setting, with vibrant colors and a playful atmosphere. The main subject is Finn, an animated character with blue shorts, a green backpack, and a light blue shirt. He is interacting with wooden logs, pushing and pulling them to create a pathway. The character is positioned centrally in the frame, moving from left to right, and is focused on his task. The logs are positioned horizontally across the ground, and Finn is seen pushing and pulling them to create a makeshift bridge. The camera is stationary, providing a wide-angle view of the scene, capturing the entirety of Finn's actions and the surrounding forest environment."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/3Xa0YD9VhIE_11/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/3Xa0YD9VhIE_11/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/3Xa0YD9VhIE_11/first_frame.png'], 'video': tensor([[[[[0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6079, 0.6079],
           [0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6079, 0.6079],
           [0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6079, 0.6079],
           ...,
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845]],

          [[0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6079, 0.6079],
           [0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6079, 0.6079],
           [0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6079, 0.6079],
           ...,
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845]],

          [[0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6079, 0.6079],
           [0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6079, 0.6079],
           [0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6079, 0.6079],
           ...,
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845]],

          ...,

          [[0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6079, 0.6079],
           [0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6079, 0.6079],
           [0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6079, 0.6079],
           ...,
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845]],

          [[0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6079, 0.6079],
           [0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6079, 0.6079],
           [0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6079, 0.6079],
           ...,
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845]],

          [[0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6079, 0.6079],
           [0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6079, 0.6079],
           [0.6982, 0.6982, 0.6982,  ..., 0.6079, 0.6079, 0.6079],
           ...,
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],
           [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845]]],


         [[[0.2825, 0.2825, 0.2825,  ..., 0.1608, 0.1530, 0.1530],
           [0.2825, 0.2825, 0.2825,  ..., 0.1608, 0.1530, 0.1530],
           [0.2825, 0.2825, 0.2825,  ..., 0.1608, 0.1530, 0.1530],
           ...,
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119],
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119],
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119]],

          [[0.2825, 0.2825, 0.2825,  ..., 0.1608, 0.1530, 0.1530],
           [0.2825, 0.2825, 0.2825,  ..., 0.1608, 0.1530, 0.1530],
           [0.2825, 0.2825, 0.2825,  ..., 0.1608, 0.1530, 0.1530],
           ...,
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119],
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119],
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119]],

          [[0.2825, 0.2825, 0.2825,  ..., 0.1608, 0.1530, 0.1530],
           [0.2825, 0.2825, 0.2825,  ..., 0.1608, 0.1530, 0.1530],
           [0.2825, 0.2825, 0.2825,  ..., 0.1608, 0.1530, 0.1530],
           ...,
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119],
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119],
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119]],

          ...,

          [[0.2825, 0.2825, 0.2825,  ..., 0.1608, 0.1530, 0.1530],
           [0.2825, 0.2825, 0.2825,  ..., 0.1608, 0.1530, 0.1530],
           [0.2825, 0.2825, 0.2825,  ..., 0.1608, 0.1530, 0.1530],
           ...,
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119],
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119],
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119]],

          [[0.2825, 0.2825, 0.2825,  ..., 0.1608, 0.1530, 0.1530],
           [0.2825, 0.2825, 0.2825,  ..., 0.1608, 0.1530, 0.1530],
           [0.2825, 0.2825, 0.2825,  ..., 0.1608, 0.1530, 0.1530],
           ...,
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119],
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119],
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119]],

          [[0.2825, 0.2825, 0.2825,  ..., 0.1608, 0.1530, 0.1530],
           [0.2825, 0.2825, 0.2825,  ..., 0.1608, 0.1530, 0.1530],
           [0.2825, 0.2825, 0.2825,  ..., 0.1608, 0.1530, 0.1530],
           ...,
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119],
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119],
           [0.4119, 0.4119, 0.4119,  ..., 0.4119, 0.4119, 0.4119]]],


         [[[0.3687, 0.3687, 0.3687,  ..., 0.2942, 0.2981, 0.2981],
           [0.3687, 0.3687, 0.3687,  ..., 0.2942, 0.2981, 0.2981],
           [0.3687, 0.3687, 0.3687,  ..., 0.2942, 0.2981, 0.2981],
           ...,
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471],
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471],
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471]],

          [[0.3687, 0.3687, 0.3687,  ..., 0.2942, 0.2981, 0.2981],
           [0.3687, 0.3687, 0.3687,  ..., 0.2942, 0.2981, 0.2981],
           [0.3687, 0.3687, 0.3687,  ..., 0.2942, 0.2981, 0.2981],
           ...,
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471],
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471],
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471]],

          [[0.3687, 0.3687, 0.3687,  ..., 0.2942, 0.2981, 0.2981],
           [0.3687, 0.3687, 0.3687,  ..., 0.2942, 0.2981, 0.2981],
           [0.3687, 0.3687, 0.3687,  ..., 0.2942, 0.2981, 0.2981],
           ...,
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471],
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471],
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471]],

          ...,

          [[0.3687, 0.3687, 0.3687,  ..., 0.2942, 0.2981, 0.2981],
           [0.3687, 0.3687, 0.3687,  ..., 0.2942, 0.2981, 0.2981],
           [0.3687, 0.3687, 0.3687,  ..., 0.2942, 0.2981, 0.2981],
           ...,
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471],
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471],
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471]],

          [[0.3687, 0.3687, 0.3687,  ..., 0.2942, 0.2981, 0.2981],
           [0.3687, 0.3687, 0.3687,  ..., 0.2942, 0.2981, 0.2981],
           [0.3687, 0.3687, 0.3687,  ..., 0.2942, 0.2981, 0.2981],
           ...,
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471],
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471],
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471]],

          [[0.3687, 0.3687, 0.3687,  ..., 0.2942, 0.2981, 0.2981],
           [0.3687, 0.3687, 0.3687,  ..., 0.2942, 0.2981, 0.2981],
           [0.3687, 0.3687, 0.3687,  ..., 0.2942, 0.2981, 0.2981],
           ...,
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471],
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471],
           [0.2471, 0.2471, 0.2471,  ..., 0.2471, 0.2471, 0.2471]]]]],
       device='cuda:0', dtype=torch.float16), 'audio': tensor([[ 1.2520,  0.9414, -0.2412,  ...,  3.4062,  3.1445,  2.0879]],
       device='cuda:0', dtype=torch.float16), 'L': tensor([25], device='cuda:0'), 'T': tensor([7], device='cuda:0')}, 0), kwargs keys: dict_keys([])

Validation DataLoader 0:  10%|         | 1/10 [00:00<00:03,  2.60it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['CNPcFgo8StY_26'], 'prompt': ["A young woman with long blonde hair is featured in a series of video frames, showcasing various poses and expressions. She is dressed in a white blouse with a playful pattern of cartoon characters and is seen smiling, making heart shapes with her hands, and adjusting her hair. The background is minimalistic, featuring a softly lit, white wall with string lights that add a cozy and warm ambiance to the scene. The woman's movements are smooth and deliberate, involving slight head tilts, hand gestures, and hair adjustments. She transitions from a neutral expression to smiling and making heart shapes with her hands, then adjusting her hair. The movements are slow and graceful, emphasizing her relaxed and confident demeanor. The background remains static throughout the video, with no changes or movements. The main subject is a young woman with long blonde hair, wearing a white blouse with cartoon character prints. She is positioned centrally in the frame and is the focal point of the video. Her expressions range from neutral to smiling, and she engages in various poses, including making heart shapes with her hands and adjusting her hair. The camera is stationary, maintaining a medium close-up view of the woman, capturing her upper body and face clearly."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/CNPcFgo8StY_26/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/CNPcFgo8StY_26/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/CNPcFgo8StY_26/first_frame.png'], 'video': tensor([[[[[0.7451, 0.7451, 0.7451,  ..., 0.7568, 0.7568, 0.7568],
           [0.7451, 0.7451, 0.7451,  ..., 0.7568, 0.7568, 0.7568],
           [0.7451, 0.7451, 0.7451,  ..., 0.7568, 0.7568, 0.7568],
           ...,
           [0.7373, 0.7373, 0.7373,  ..., 0.7578, 0.7656, 0.7695],
           [0.7373, 0.7373, 0.7373,  ..., 0.7607, 0.7686, 0.7725],
           [0.7373, 0.7373, 0.7373,  ..., 0.7607, 0.7686, 0.7725]],

          [[0.7451, 0.7451, 0.7451,  ..., 0.7568, 0.7568, 0.7568],
           [0.7451, 0.7451, 0.7451,  ..., 0.7568, 0.7568, 0.7568],
           [0.7451, 0.7451, 0.7451,  ..., 0.7568, 0.7568, 0.7568],
           ...,
           [0.7334, 0.7334, 0.7334,  ..., 0.7578, 0.7656, 0.7695],
           [0.7334, 0.7334, 0.7334,  ..., 0.7607, 0.7686, 0.7725],
           [0.7334, 0.7334, 0.7334,  ..., 0.7607, 0.7686, 0.7725]],

          [[0.7451, 0.7451, 0.7451,  ..., 0.7568, 0.7568, 0.7568],
           [0.7451, 0.7451, 0.7451,  ..., 0.7568, 0.7568, 0.7568],
           [0.7451, 0.7451, 0.7451,  ..., 0.7568, 0.7568, 0.7568],
           ...,
           [0.7334, 0.7334, 0.7334,  ..., 0.7578, 0.7656, 0.7695],
           [0.7334, 0.7334, 0.7334,  ..., 0.7607, 0.7686, 0.7725],
           [0.7334, 0.7334, 0.7334,  ..., 0.7607, 0.7686, 0.7725]],

          ...,

          [[0.7451, 0.7451, 0.7451,  ..., 0.7568, 0.7568, 0.7568],
           [0.7451, 0.7451, 0.7451,  ..., 0.7568, 0.7568, 0.7568],
           [0.7451, 0.7451, 0.7451,  ..., 0.7568, 0.7568, 0.7568],
           ...,
           [0.7295, 0.7295, 0.7295,  ..., 0.9414, 0.9414, 0.9414],
           [0.7295, 0.7295, 0.7295,  ..., 0.9414, 0.9414, 0.9414],
           [0.7295, 0.7295, 0.7295,  ..., 0.9414, 0.9414, 0.9414]],

          [[0.7451, 0.7451, 0.7451,  ..., 0.7568, 0.7568, 0.7568],
           [0.7451, 0.7451, 0.7451,  ..., 0.7568, 0.7568, 0.7568],
           [0.7451, 0.7451, 0.7451,  ..., 0.7568, 0.7568, 0.7568],
           ...,
           [0.7295, 0.7295, 0.7295,  ..., 0.9414, 0.9414, 0.9414],
           [0.7295, 0.7295, 0.7295,  ..., 0.9414, 0.9414, 0.9414],
           [0.7295, 0.7295, 0.7295,  ..., 0.9414, 0.9414, 0.9414]],

          [[0.7451, 0.7451, 0.7451,  ..., 0.7568, 0.7568, 0.7568],
           [0.7451, 0.7451, 0.7451,  ..., 0.7568, 0.7568, 0.7568],
           [0.7451, 0.7451, 0.7451,  ..., 0.7568, 0.7568, 0.7568],
           ...,
           [0.7295, 0.7295, 0.7295,  ..., 0.9414, 0.9414, 0.9414],
           [0.7295, 0.7295, 0.7295,  ..., 0.9414, 0.9414, 0.9414],
           [0.7295, 0.7295, 0.7295,  ..., 0.9414, 0.9414, 0.9414]]],


         [[[0.7178, 0.7178, 0.7178,  ..., 0.7334, 0.7334, 0.7334],
           [0.7178, 0.7178, 0.7178,  ..., 0.7334, 0.7334, 0.7334],
           [0.7178, 0.7178, 0.7178,  ..., 0.7334, 0.7334, 0.7334],
           ...,
           [0.6982, 0.6982, 0.6982,  ..., 0.7188, 0.7266, 0.7305],
           [0.6982, 0.6982, 0.6982,  ..., 0.7217, 0.7295, 0.7334],
           [0.6982, 0.6982, 0.6982,  ..., 0.7217, 0.7295, 0.7334]],

          [[0.7178, 0.7178, 0.7178,  ..., 0.7334, 0.7334, 0.7334],
           [0.7178, 0.7178, 0.7178,  ..., 0.7334, 0.7334, 0.7334],
           [0.7178, 0.7178, 0.7178,  ..., 0.7334, 0.7334, 0.7334],
           ...,
           [0.6943, 0.6943, 0.6943,  ..., 0.7188, 0.7266, 0.7305],
           [0.6943, 0.6943, 0.6943,  ..., 0.7217, 0.7295, 0.7334],
           [0.6943, 0.6943, 0.6943,  ..., 0.7217, 0.7295, 0.7334]],

          [[0.7178, 0.7178, 0.7178,  ..., 0.7334, 0.7334, 0.7334],
           [0.7178, 0.7178, 0.7178,  ..., 0.7334, 0.7334, 0.7334],
           [0.7178, 0.7178, 0.7178,  ..., 0.7334, 0.7334, 0.7334],
           ...,
           [0.6943, 0.6943, 0.6943,  ..., 0.7188, 0.7266, 0.7305],
           [0.6943, 0.6943, 0.6943,  ..., 0.7217, 0.7295, 0.7334],
           [0.6943, 0.6943, 0.6943,  ..., 0.7217, 0.7295, 0.7334]],

          ...,

          [[0.7178, 0.7178, 0.7178,  ..., 0.7334, 0.7334, 0.7334],
           [0.7178, 0.7178, 0.7178,  ..., 0.7334, 0.7334, 0.7334],
           [0.7178, 0.7178, 0.7178,  ..., 0.7334, 0.7334, 0.7334],
           ...,
           [0.6982, 0.6982, 0.6982,  ..., 0.9331, 0.9331, 0.9331],
           [0.6982, 0.6982, 0.6982,  ..., 0.9331, 0.9331, 0.9331],
           [0.6982, 0.6982, 0.6982,  ..., 0.9331, 0.9331, 0.9331]],

          [[0.7178, 0.7178, 0.7178,  ..., 0.7334, 0.7334, 0.7334],
           [0.7178, 0.7178, 0.7178,  ..., 0.7334, 0.7334, 0.7334],
           [0.7178, 0.7178, 0.7178,  ..., 0.7334, 0.7334, 0.7334],
           ...,
           [0.6982, 0.6982, 0.6982,  ..., 0.9331, 0.9331, 0.9331],
           [0.6982, 0.6982, 0.6982,  ..., 0.9331, 0.9331, 0.9331],
           [0.6982, 0.6982, 0.6982,  ..., 0.9331, 0.9331, 0.9331]],

          [[0.7178, 0.7178, 0.7178,  ..., 0.7334, 0.7334, 0.7334],
           [0.7178, 0.7178, 0.7178,  ..., 0.7334, 0.7334, 0.7334],
           [0.7178, 0.7178, 0.7178,  ..., 0.7334, 0.7334, 0.7334],
           ...,
           [0.6982, 0.6982, 0.6982,  ..., 0.9331, 0.9331, 0.9331],
           [0.6982, 0.6982, 0.6982,  ..., 0.9331, 0.9331, 0.9331],
           [0.6982, 0.6982, 0.6982,  ..., 0.9331, 0.9331, 0.9331]]],


         [[[0.7100, 0.7100, 0.7100,  ..., 0.7139, 0.7139, 0.7139],
           [0.7100, 0.7100, 0.7100,  ..., 0.7139, 0.7139, 0.7139],
           [0.7100, 0.7100, 0.7100,  ..., 0.7139, 0.7139, 0.7139],
           ...,
           [0.6587, 0.6587, 0.6587,  ..., 0.6792, 0.6870, 0.6914],
           [0.6587, 0.6587, 0.6587,  ..., 0.6821, 0.6904, 0.6943],
           [0.6587, 0.6587, 0.6587,  ..., 0.6821, 0.6904, 0.6943]],

          [[0.7100, 0.7100, 0.7100,  ..., 0.7139, 0.7139, 0.7139],
           [0.7100, 0.7100, 0.7100,  ..., 0.7139, 0.7139, 0.7139],
           [0.7100, 0.7100, 0.7100,  ..., 0.7139, 0.7139, 0.7139],
           ...,
           [0.6548, 0.6548, 0.6548,  ..., 0.6792, 0.6870, 0.6914],
           [0.6548, 0.6548, 0.6548,  ..., 0.6821, 0.6904, 0.6943],
           [0.6548, 0.6548, 0.6548,  ..., 0.6821, 0.6904, 0.6943]],

          [[0.7100, 0.7100, 0.7100,  ..., 0.7139, 0.7139, 0.7139],
           [0.7100, 0.7100, 0.7100,  ..., 0.7139, 0.7139, 0.7139],
           [0.7100, 0.7100, 0.7100,  ..., 0.7139, 0.7139, 0.7139],
           ...,
           [0.6548, 0.6548, 0.6548,  ..., 0.6792, 0.6870, 0.6914],
           [0.6548, 0.6548, 0.6548,  ..., 0.6821, 0.6904, 0.6943],
           [0.6548, 0.6548, 0.6548,  ..., 0.6821, 0.6904, 0.6943]],

          ...,

          [[0.7100, 0.7100, 0.7100,  ..., 0.7139, 0.7139, 0.7139],
           [0.7100, 0.7100, 0.7100,  ..., 0.7139, 0.7139, 0.7139],
           [0.7100, 0.7100, 0.7100,  ..., 0.7139, 0.7139, 0.7139],
           ...,
           [0.6548, 0.6548, 0.6548,  ..., 0.8550, 0.8550, 0.8550],
           [0.6548, 0.6548, 0.6548,  ..., 0.8550, 0.8550, 0.8550],
           [0.6548, 0.6548, 0.6548,  ..., 0.8550, 0.8550, 0.8550]],

          [[0.7100, 0.7100, 0.7100,  ..., 0.7139, 0.7139, 0.7139],
           [0.7100, 0.7100, 0.7100,  ..., 0.7139, 0.7139, 0.7139],
           [0.7100, 0.7100, 0.7100,  ..., 0.7139, 0.7139, 0.7139],
           ...,
           [0.6548, 0.6548, 0.6548,  ..., 0.8550, 0.8550, 0.8550],
           [0.6548, 0.6548, 0.6548,  ..., 0.8550, 0.8550, 0.8550],
           [0.6548, 0.6548, 0.6548,  ..., 0.8550, 0.8550, 0.8550]],

          [[0.7100, 0.7100, 0.7100,  ..., 0.7139, 0.7139, 0.7139],
           [0.7100, 0.7100, 0.7100,  ..., 0.7139, 0.7139, 0.7139],
           [0.7100, 0.7100, 0.7100,  ..., 0.7139, 0.7139, 0.7139],
           ...,
           [0.6548, 0.6548, 0.6548,  ..., 0.8550, 0.8550, 0.8550],
           [0.6548, 0.6548, 0.6548,  ..., 0.8550, 0.8550, 0.8550],
           [0.6548, 0.6548, 0.6548,  ..., 0.8550, 0.8550, 0.8550]]]]],
       device='cuda:0', dtype=torch.float16), 'audio': tensor([[2.0254, 2.0195, 1.2402,  ..., 0.8828, 0.9648, 0.9004]],
       device='cuda:0', dtype=torch.float16), 'L': tensor([25], device='cuda:0'), 'T': tensor([7], device='cuda:0')}, 1), kwargs keys: dict_keys([])

Validation DataLoader 0:  20%|        | 2/10 [00:06<00:27,  0.29it/s][A[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 127 to 152
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 68 to 93
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 54 to 79
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 60 to 85
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 34 to 59
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 49 to 74
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 36 to 61
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 125 to 150
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 359 to 384
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 0 to 25
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 172 to 197
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 68 to 93
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 272 to 297
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 75 to 100
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 80 to 105
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 271 to 296
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 103 to 128
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 37 to 62
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 100 to 125
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 210 to 235
[WanVideoDataset __getitem__] -> Video longer than max_frame, crop from 273 to 298
