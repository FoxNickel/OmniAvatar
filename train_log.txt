| Reloading config from: pretrained_models/OmniAvatar-1.3B/config.json
{'config': 'configs/train_1.3B.yaml', 'exp_path': 'pretrained_models/OmniAvatar-1.3B', 'input_file': None, 'debug': False, 'infer': False, 'hparams': '', 'dtype': '16', 'text_encoder_path': 'pretrained_models/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth', 'image_encoder_path': 'None', 'dit_path': 'pretrained_models/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors', 'vae_path': 'pretrained_models/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth', 'wav2vec_path': 'pretrained_models/wav2vec2-base-960h', 'num_persistent_param_in_dit': None, 'reload_cfg': True, 'sp_size': 1, 'seed': 42, 'image_sizes_720': [[400, 720], [720, 720], [720, 400]], 'image_sizes_1280': [[720, 720], [528, 960], [960, 528], [720, 1280], [1280, 720]], 'max_hw': 720, 'max_tokens': 30000, 'seq_len': 200, 'overlap_frame': 13, 'guidance_scale': 4.5, 'audio_scale': None, 'num_steps': 50, 'fps': 20, 'sample_rate': 16000, 'negative_prompt': 'Vivid color tones, background/camera moving quickly, screen switching, subtitles and special effects, mutation, overexposed, static, blurred details, subtitles, style, work, painting, image, still, overall grayish, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn face, deformed, disfigured, malformed limbs, fingers merging, motionless image, chaotic background, three legs, crowded background with many people, walking backward', 'silence_duration_s': 0.3, 'use_fsdp': False, 'tea_cache_l1_thresh': 0, 'dataset_base_path': '/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1', 'name': 'train_1.3B', 'savedir': '/mnt/hdd2/huanglingyu/vgg/OmniAvatar/outputs', 'batch_size': 1, 'nodes': 1, 'devices': 1, 'num_train_epochs': 1, 'mode': 'train', 'checkpoint_path': '', 'lr': '1e-4', 'max_frames': 120, 'debug_data_len': 10, 'rank': 0, 'world_size': 1, 'local_rank': 0, 'device': 'cuda:0', 'num_nodes': 1, 'i2v': True, 'use_audio': True, 'random_prefix_frames': True, 'model_config': {'in_dim': 33}, 'lora_target_modules': 'q,k,v,o,ffn.0,ffn.2', 'init_lora_weights': 'kaiming', 'lora_rank': 128, 'lora_alpha': 64.0, 'use_gradient_checkpointing': True, 'use_gradient_checkpointing_offload': False, 'train_architecture': 'lora'}
[2025-08-14 22:01:29,098] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-14 22:01:30,244] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[train_pl.py]-main-] config: {'dtype': '16', 'text_encoder_path': 'pretrained_models/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth', 'image_encoder_path': 'None', 'dit_path': 'pretrained_models/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors', 'vae_path': 'pretrained_models/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth', 'wav2vec_path': 'pretrained_models/wav2vec2-base-960h', 'exp_path': 'pretrained_models/OmniAvatar-1.3B', 'num_persistent_param_in_dit': None, 'reload_cfg': True, 'sp_size': 1, 'seed': 42, 'image_sizes_720': [[400, 720], [720, 720], [720, 400]], 'image_sizes_1280': [[720, 720], [528, 960], [960, 528], [720, 1280], [1280, 720]], 'max_hw': 720, 'max_tokens': 30000, 'seq_len': 200, 'overlap_frame': 13, 'guidance_scale': 4.5, 'audio_scale': None, 'num_steps': 50, 'fps': 20, 'sample_rate': 16000, 'negative_prompt': 'Vivid color tones, background/camera moving quickly, screen switching, subtitles and special effects, mutation, overexposed, static, blurred details, subtitles, style, work, painting, image, still, overall grayish, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn face, deformed, disfigured, malformed limbs, fingers merging, motionless image, chaotic background, three legs, crowded background with many people, walking backward', 'silence_duration_s': 0.3, 'use_fsdp': False, 'tea_cache_l1_thresh': 0, 'dataset_base_path': '/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1', 'name': 'train_1.3B', 'savedir': '/mnt/hdd2/huanglingyu/vgg/OmniAvatar/outputs', 'batch_size': 1, 'nodes': 1, 'devices': 1, 'num_train_epochs': 1, 'mode': 'train', 'checkpoint_path': '', 'lr': 0.0001, 'max_frames': 120, 'debug': False, 'debug_data_len': 10}
[OmniTrainingModule] __init__
Loading models from: ['pretrained_models/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors']
    model_name: wan_video_dit model_class: WanModel
        This model is initialized with extra kwargs: {'has_image_input': False, 'patch_size': [1, 2, 2], 'in_dim': 33, 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'text_dim': 4096, 'out_dim': 16, 'num_heads': 12, 'num_layers': 30, 'eps': 1e-06}
Using WanModel with dim=1536, in_dim=33, ffn_dim=8960, out_dim=16, text_dim=4096, freq_dim=256, eps=1e-06, patch_size=[1, 2, 2], num_heads=12, num_layers=30, has_image_input=False, audio_hidden_size=32
[AudioPack] in_channels: 10752, t, h, w: 4, 1, 1
[AudioPack] patch_size: (4, 1, 1)
[AudioPack] proj: Linear(in_features=43008, out_features=32, bias=True)
[AudioPack] norm_out: LayerNorm((32,), eps=1e-05, elementwise_affine=True)

[WanModel] patch_embedding: Conv3d(33, 1536, kernel_size=(1, 2, 2), stride=(1, 2, 2))
[WanModel] text_embedding: Sequential(
  (0): Linear(in_features=4096, out_features=1536, bias=True)
  (1): GELU(approximate='tanh')
  (2): Linear(in_features=1536, out_features=1536, bias=True)
)
[WanModel] time_embedding: Sequential(
  (0): Linear(in_features=256, out_features=1536, bias=True)
  (1): SiLU()
  (2): Linear(in_features=1536, out_features=1536, bias=True)
)
[WanModel] time_projection: Sequential(
  (0): SiLU()
  (1): Linear(in_features=1536, out_features=9216, bias=True)
)
[WanModel] blocks (DiTBlock):
  Block 0: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 1: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 2: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 3: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 4: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 5: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 6: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 7: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 8: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 9: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 10: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 11: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 12: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 13: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 14: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 15: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 16: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 17: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 18: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 19: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 20: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 21: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 22: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 23: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 24: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 25: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 26: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 27: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 28: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 29: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
[WanModel] head: Head(
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (head): Linear(in_features=1536, out_features=64, bias=True)
)
[WanModel] RoPE freqs shape: [torch.Size([1024, 22]), torch.Size([1024, 21]), torch.Size([1024, 21])]
[WanModel] audio_proj: AudioPack(
  (proj): Linear(in_features=43008, out_features=32, bias=True)
  (norm_out): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
)
[WanModel] audio_cond_projs:
  Audio Cond Proj 0: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 1: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 2: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 3: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 4: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 5: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 6: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 7: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 8: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 9: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 10: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 11: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 12: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 13: Linear(in_features=32, out_features=1536, bias=True)
[Truncate] patch_embedding.weight: ckpt torch.Size([1536, 16, 1, 2, 2]) -> model torch.Size([1536, 33, 1, 2, 2])
    The following models are loaded: ['wan_video_dit'].
Loading models from: pretrained_models/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth
    model_name: wan_video_text_encoder model_class: WanTextEncoder
[WanTextEncoder] token_embedding: Embedding(256384, 4096)
[WanTextEncoder] dropout: Dropout(p=0.1, inplace=False)
[WanTextEncoder] blocks (T5SelfAttention):
  Block 0: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 1: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 2: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 3: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 4: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 5: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 6: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 7: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 8: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 9: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 10: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 11: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 12: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 13: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 14: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 15: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 16: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 17: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 18: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 19: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 20: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 21: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 22: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 23: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
[WanTextEncoder] norm: T5LayerNorm()
    The following models are loaded: ['wan_video_text_encoder'].
Loading models from: pretrained_models/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth
    model_name: wan_video_vae model_class: WanVideoVAE

[VideoVAE_] encoder: Encoder3d(
  (conv1): CausalConv3d(3, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
  (downsamples): Sequential(
    (0): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (1): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (2): Resample(
      (resample): Sequential(
        (0): ZeroPad2d((0, 1, 0, 1))
        (1): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2))
      )
    )
    (3): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(96, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): CausalConv3d(96, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    )
    (4): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (5): Resample(
      (resample): Sequential(
        (0): ZeroPad2d((0, 1, 0, 1))
        (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2))
      )
      (time_conv): CausalConv3d(192, 192, kernel_size=(3, 1, 1), stride=(2, 1, 1))
    )
    (6): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(192, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): CausalConv3d(192, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    )
    (7): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (8): Resample(
      (resample): Sequential(
        (0): ZeroPad2d((0, 1, 0, 1))
        (1): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2))
      )
      (time_conv): CausalConv3d(384, 384, kernel_size=(3, 1, 1), stride=(2, 1, 1))
    )
    (9): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (10): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
  )
  (middle): Sequential(
    (0): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (1): AttentionBlock(
      (norm): RMS_norm()
      (to_qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
      (proj): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
    )
    (2): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
  )
  (head): Sequential(
    (0): RMS_norm()
    (1): SiLU()
    (2): CausalConv3d(384, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))
  )
)
[VideoVAE_] conv1: CausalConv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))
[VideoVAE_] conv2: CausalConv3d(16, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))
[VideoVAE_] decoder: Decoder3d(
  (conv1): CausalConv3d(16, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
  (middle): Sequential(
    (0): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (1): AttentionBlock(
      (norm): RMS_norm()
      (to_qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
      (proj): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
    )
    (2): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
  )
  (upsamples): Sequential(
    (0): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (1): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (2): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (3): Resample(
      (resample): Sequential(
        (0): Upsample(scale_factor=(2.0, 2.0), mode='nearest-exact')
        (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (time_conv): CausalConv3d(384, 768, kernel_size=(3, 1, 1), stride=(1, 1, 1))
    )
    (4): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(192, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): CausalConv3d(192, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    )
    (5): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (6): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (7): Resample(
      (resample): Sequential(
        (0): Upsample(scale_factor=(2.0, 2.0), mode='nearest-exact')
        (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (time_conv): CausalConv3d(384, 768, kernel_size=(3, 1, 1), stride=(1, 1, 1))
    )
    (8): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (9): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (10): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (11): Resample(
      (resample): Sequential(
        (0): Upsample(scale_factor=(2.0, 2.0), mode='nearest-exact')
        (1): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (12): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (13): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (14): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
  )
  (head): Sequential(
    (0): RMS_norm()
    (1): SiLU()
    (2): CausalConv3d(96, 3, kernel_size=(3, 3, 3), stride=(1, 1, 1))
  )
)
    The following models are loaded: ['wan_video_vae'].
Using wan_video_text_encoder from pretrained_models/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth.
Using wan_video_dit from ['pretrained_models/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors'].
Using wan_video_vae from pretrained_models/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth.
No wan_video_image_encoder models available.
[OmniTrainingModule load_model] -> Use LoRA: lora rank: 128, lora alpha: 64.0, pretrained_lora_path: pretrained_models/OmniAvatar-1.3B/pytorch_model.pt
[OmniTrainingModule add_lora_to_model] -> lora_config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'v', 'q', 'ffn.2', 'k', 'ffn.0', 'o'}, exclude_modules=None, lora_alpha=64.0, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
[OmniTrainingModule add_lora_to_model] -> 634 parameters are loaded from pretrained_models/OmniAvatar-1.3B/pytorch_model.pt. 0 parameters are unexpected.
[OmniTrainingModule __init__]: Model loaded on cpu, dtype: torch.float32
===================[train_pl.py]-main-] model summary================================
[OmniTrainingModule] - name: pipe.text_encoder.token_embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.norm.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.patch_embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.patch_embedding.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.text_embedding.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.text_embedding.0.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.text_embedding.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.text_embedding.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.time_embedding.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.time_embedding.0.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.time_embedding.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.time_embedding.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.time_projection.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.time_projection.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.head.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.head.head.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.head.head.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_proj.proj.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_proj.proj.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_proj.norm_out.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_proj.norm_out.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.0.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.4.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.4.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.5.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.5.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.7.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.7.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.8.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.8.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.9.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.9.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.10.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.10.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.11.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.11.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.12.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.12.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.13.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.13.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.conv1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.conv1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.0.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.0.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.0.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.0.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.0.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.0.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.1.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.1.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.1.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.1.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.1.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.1.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.2.resample.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.2.resample.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.shortcut.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.shortcut.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.4.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.4.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.4.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.4.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.4.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.4.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.5.resample.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.5.resample.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.5.time_conv.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.5.time_conv.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.shortcut.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.shortcut.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.7.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.7.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.7.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.7.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.7.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.7.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.8.resample.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.8.resample.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.8.time_conv.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.8.time_conv.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.9.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.9.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.9.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.9.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.9.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.9.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.10.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.10.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.10.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.10.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.10.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.10.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.0.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.0.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.0.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.0.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.0.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.0.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.1.norm.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.1.to_qkv.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.1.to_qkv.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.1.proj.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.1.proj.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.2.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.2.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.2.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.2.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.2.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.2.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.head.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.head.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.head.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.conv1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.conv1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.conv2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.conv2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.conv1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.conv1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.0.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.0.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.0.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.0.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.0.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.0.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.1.norm.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.1.to_qkv.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.1.to_qkv.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.1.proj.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.1.proj.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.2.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.2.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.2.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.2.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.2.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.2.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.0.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.0.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.0.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.0.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.0.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.0.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.1.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.1.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.1.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.1.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.1.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.1.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.2.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.2.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.2.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.2.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.2.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.2.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.3.resample.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.3.resample.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.3.time_conv.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.3.time_conv.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.shortcut.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.shortcut.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.5.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.5.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.5.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.5.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.5.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.5.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.6.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.6.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.6.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.6.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.6.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.6.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.7.resample.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.7.resample.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.7.time_conv.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.7.time_conv.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.8.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.8.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.8.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.8.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.8.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.8.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.9.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.9.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.9.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.9.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.9.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.9.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.10.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.10.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.10.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.10.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.10.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.10.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.11.resample.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.11.resample.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.12.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.12.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.12.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.12.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.12.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.12.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.13.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.13.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.13.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.13.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.13.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.13.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.14.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.14.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.14.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.14.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.14.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.14.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.head.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.head.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.head.2.bias, requires_grad: False
[OmniTrainingModule] - name: audio_encoder.masked_spec_embed, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.0.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.0.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.0.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.1.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.2.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.3.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.4.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.5.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.6.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_projection.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_projection.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_projection.projection.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_projection.projection.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.pos_conv_embed.conv.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.pos_conv_embed.conv.parametrizations.weight.original0, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.pos_conv_embed.conv.parametrizations.weight.original1, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.final_layer_norm.bias, requires_grad: True
===================================================================================
[OmniTrainingModule]: start training with config: {'dtype': '16', 'text_encoder_path': 'pretrained_models/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth', 'image_encoder_path': 'None', 'dit_path': 'pretrained_models/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors', 'vae_path': 'pretrained_models/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth', 'wav2vec_path': 'pretrained_models/wav2vec2-base-960h', 'exp_path': 'pretrained_models/OmniAvatar-1.3B', 'num_persistent_param_in_dit': None, 'reload_cfg': True, 'sp_size': 1, 'seed': 42, 'image_sizes_720': [[400, 720], [720, 720], [720, 400]], 'image_sizes_1280': [[720, 720], [528, 960], [960, 528], [720, 1280], [1280, 720]], 'max_hw': 720, 'max_tokens': 30000, 'seq_len': 200, 'overlap_frame': 13, 'guidance_scale': 4.5, 'audio_scale': None, 'num_steps': 50, 'fps': 20, 'sample_rate': 16000, 'negative_prompt': 'Vivid color tones, background/camera moving quickly, screen switching, subtitles and special effects, mutation, overexposed, static, blurred details, subtitles, style, work, painting, image, still, overall grayish, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn face, deformed, disfigured, malformed limbs, fingers merging, motionless image, chaotic background, three legs, crowded background with many people, walking backward', 'silence_duration_s': 0.3, 'use_fsdp': False, 'tea_cache_l1_thresh': 0, 'dataset_base_path': '/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1', 'name': 'train_1.3B', 'savedir': '/mnt/hdd2/huanglingyu/vgg/OmniAvatar/outputs/train_1.3B', 'batch_size': 1, 'nodes': 1, 'devices': 1, 'num_train_epochs': 1, 'mode': 'train', 'checkpoint_path': '', 'lr': 0.0001, 'max_frames': 120, 'debug': False, 'debug_data_len': 10}
[OmniTrainingModule] configure_optimizers
[OmniTrainingModule] on_fit_start -> device: cuda:0, param device: cuda:0
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/224174 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/224174 [00:00<?, ?it/s] [OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 0, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 5.249 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.274 
[Timer] : 0.004 
[Timer] forward_preprocess : 5.527 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([917.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["A person with long, wavy, auburn hair is engaging in various actions in a room with a colorful wallpaper featuring anime-style characters. The individual is wearing a white, lace-trimmed top and white shorts. They are initially seen with their mouth open, possibly in mid-conversation or reacting to something. The person then leans forward, placing their hands on the back of a gray chair, and appears to be either sitting down or preparing to sit. Subsequently, they are seen holding a black object, possibly a remote control, and seem to be interacting with it, possibly adjusting settings or preparing to use it. The setting suggests a casual, possibly domestic environment with a focus on the individual's actions and movements."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 1/224174 [00:14<914:39:34,  0.07it/s]Epoch 0:   0%|          | 1/224174 [00:14<914:47:39,  0.07it/s, v_num=42, train_loss_step=0.000][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 1, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.439 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.120 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.560 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([692.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["a playful and animated scene set in a desert-like environment. A large, menacing cat with a snarling expression and sharp claws is positioned on the left side of the frame, while a smaller, cheerful dog with a pink body and a brown belt appears on the right. The dog initially looks at the cat with a surprised expression, then begins to run towards the cat, causing the cat to react with a startled look. The dog continues to run, and the cat follows, creating a dynamic and engaging interaction between the two characters. The main subjects are a large, menacing cat and a smaller, cheerful dog. The cat has a brown fur coat, sharp claws, and a snarling expression, positioned on the left side of the frame. The dog has a pink body with a brown belt and a cheerful expression, initially looking surprised but then running towards the cat. The dog's movements are energetic and playful, while the cat's movements are more cautious and defensive. The background is a desert-like setting with large, rugged rock formations and a sandy ground. The scene is set in a bright, sunny environment with a warm, orange color palette. The rocks are textured and have a natural, rugged appearance, enhancing the desert atmosphere. The camera is static, providing a wide-angle view of the scene. There is no camera movement, focusing entirely on the interaction between the cat and the dog."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=1.675796914735494e-35
[OmniTrainingModule] training_step -> loss: 1.675796914735494e-35
Epoch 0:   0%|          | 2/224174 [00:19<613:22:49,  0.10it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 2/224174 [00:19<613:26:17,  0.10it/s, v_num=42, train_loss_step=1.68e-35][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 2, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.453 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.018 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.471 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([448.7500], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['two animated characters, a pink bear and a red fox, sitting next to each other on a grassy field. The pink bear, with a cheerful expression, holds a yellow wicker basket and a jar of honey. The red fox, with a more serious expression, holds a jar of honey and occasionally looks at the pink bear. The scene is set in a bright, colorful, and cheerful environment with trees and a blue background. The background is a bright, cheerful, and colorful environment with green grass, yellow trees, and a blue sky. The scene is set outdoors, likely in a park or a garden. The overall atmosphere is vibrant and playful, with no other objects or characters present. The main subjects are the pink bear and the red fox. The pink bear has large, expressive eyes and a cheerful demeanor, wearing a pink dress with a red bow. The red fox has a more serious expression, wearing a red sweater with a white collar. The pink bear holds a yellow wicker basket and a jar of honey, while the red fox holds a jar of honey. They are positioned next to each other, with the pink bear on the left and the red fox on the right. The camera is static, providing a medium shot of the two characters from a slightly elevated angle, capturing their expressions and interactions clearly.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=2.0249839131902814e-13
[OmniTrainingModule] training_step -> loss: 2.0249839131902814e-13
Epoch 0:   0%|          | 3/224174 [00:24<506:00:09,  0.12it/s, v_num=42, train_loss_step=1.68e-35]Epoch 0:   0%|          | 3/224174 [00:24<506:03:26,  0.12it/s, v_num=42, train_loss_step=2.02e-13][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 3, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.526 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.021 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.547 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([886.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['A man stands on a stage in front of a large screen displaying a colorful and dynamic animation of various characters performing ballet moves. The man gestures animatedly, emphasizing the text displayed on the screen, which reads "BORING! MAKE IT PERFORM!". The audience, seated in rows, watches attentively as the man interacts with the screen, occasionally pointing and moving his hands to draw attention to different parts of the animation. The main subject is a man standing on a stage, dressed in a black shirt and jeans. He is positioned centrally in front of the screen, which displays a vibrant animation of ballet dancers performing various poses and movements. The man gestures with his hands, pointing and moving them to highlight different parts of the animation. The characters in the animation are colorful and dynamic, dressed in ballet costumes and performing ballet poses. The background consists of a stage with a large screen displaying an animated ballet performance. The screen is divided into multiple sections, each showing a different character performing ballet moves. The stage is set with a simple, clean design, featuring white and blue panels that create a modern and minimalist look. The audience is seated in rows, facing the stage, and their attention is focused on the man and the screen. The camera is stationary, capturing a wide-angle view of the stage and the screen. The focus remains on the man and the screen, with no noticeable camera movement.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 4/224174 [00:29<461:01:19,  0.14it/s, v_num=42, train_loss_step=2.02e-13]Epoch 0:   0%|          | 4/224174 [00:29<461:05:45,  0.14it/s, v_num=42, train_loss_step=0.000]   [OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 4, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.532 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.017 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.550 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([943.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['A cartoon character, a cheerful girl with green hair and a green and white striped helmet, is floating in the air above a snowy landscape. She is wearing a green and white outfit and is smiling. In the background, a man with a purple beanie and sunglasses is standing on the snow, holding a comic book. The scene is set against a bright blue sky with fluffy white clouds and snow-covered mountains. The girl floats gently in the air, moving slightly up and down and side to side. The man remains stationary, holding the comic book and pointing towards the girl. The background remains static, with no noticeable changes or movements. The main subjects are the cheerful girl and the man. The girl is floating in the air, smiling, and wearing a green and white striped helmet and outfit. She is positioned centrally in the frame. The man is standing on the snow, wearing a purple beanie and sunglasses, and holding a comic book. He is positioned to the right of the girl and is pointing towards her. The camera is stationary, capturing the scene from a fixed viewpoint, focusing on the floating girl and the standing man.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 5/224174 [00:35<436:22:18,  0.14it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 5/224174 [00:35<436:23:32,  0.14it/s, v_num=42, train_loss_step=0.000][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 5, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.528 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.018 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.547 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([331.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['A man wearing a grey sweatshirt with the "Great Outdoors" logo and a blue cap is standing in a backyard. He is speaking animatedly, using hand gestures to emphasize his points. The background features a wooden fence, a green patio umbrella, and some gardening tools. The scene is set in a suburban backyard with a wooden fence and a green patio umbrella. There are also some gardening tools and plants visible, indicating a well-maintained outdoor space. The weather appears to be overcast, with no direct sunlight visible. The main subject is a man wearing a grey sweatshirt with the "Great Outdoors" logo and a blue cap. He is positioned centrally in the frame and is actively gesturing with his hands while speaking. His facial expressions and hand movements suggest he is engaged in a conversation or explaining something. The man\'s movements are primarily focused on his hands and facial expressions. He uses hand gestures to emphasize his speech, moving his hands in various directions. His head and eyes also move slightly as he speaks, indicating engagement and animation. The background remains static throughout the video. The camera is stationary, capturing the man from a medium close-up view. There is no noticeable camera movement, and the focus remains steady on the subject throughout the video.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=2.397204980297829e-06
[OmniTrainingModule] training_step -> loss: 2.397204980297829e-06
Epoch 0:   0%|          | 6/224174 [00:40<417:09:19,  0.15it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 6/224174 [00:40<417:10:52,  0.15it/s, v_num=42, train_loss_step=2.4e-6][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 6, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.542 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.019 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.562 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([729.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["two animated characters, a man and a woman, sitting side by side. The man has short brown hair and a neutral expression, while the woman has long orange hair and a concerned look. The scene is set against a plain, light-colored background, emphasizing the characters' expressions and interactions. subtle changes in their facial expressions, suggesting a conversation or interaction between them. The main subjects are the man and the woman. The man has short brown hair and wears a light pink shirt. He has a neutral expression with slight changes in his facial muscles. The woman has long orange hair and wears a light purple sweater. She has a concerned and slightly worried expression. They are positioned side by side, with the man on the left and the woman on the right. Their expressions and slight movements suggest they are engaged in a conversation or interaction. The background is a plain, light-colored wall with no distinct features or objects. The simplicity of the background helps to focus attention on the characters and their expressions. There are no indications of the location or time, as the setting is minimalistic and nondescript. The camera is static, focusing on a close-up view of the two characters. There is no noticeable camera movement, maintaining a steady and consistent frame throughout the video."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=9.963077938518374e-40
[OmniTrainingModule] training_step -> loss: 9.963077938518374e-40
Epoch 0:   0%|          | 7/224174 [00:45<406:30:16,  0.15it/s, v_num=42, train_loss_step=2.4e-6]Epoch 0:   0%|          | 7/224174 [00:45<406:31:09,  0.15it/s, v_num=42, train_loss_step=9.96e-40][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 7, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.394 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.016 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.410 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([949.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['a whimsical animated scene set in a cozy, vintage-style room with a yellow wall and a large, ornate chandelier hanging from the ceiling. The room is furnished with a neatly made bed, a chair, and a dining table. The main characters are three animated animals, a duck, a bear, and a frog, who are engaged in a lively conversation while sitting around the table. The duck, dressed in a red suit, is seen interacting with the bear, who is wearing a blue suit, and the frog, who is wearing a green suit. The scene is filled with humor and playful interactions as the characters engage in animated conversations and gestures. The background is a cozy, vintage-style room with a yellow wall and a large, ornate chandelier hanging from the ceiling. The room features a window with a purple curtain, a chair with a red cushion, and a neatly made bed with a white bedspread. The floor is a light orange color, and the overall setting is warm and inviting. The main subjects are three animated animals, a duck, a bear, and a frog. The duck is wearing a red suit and is positioned on the left side of the table, often gesturing with its hands. The bear is wearing a blue suit and is seated in the middle of the table, often looking at the duck and frog. The frog is wearing a green suit and is seated on the right side of the table, often looking at the bear and duck. The animals are engaged in animated conversations and gestures, creating a lively and humorous atmosphere. The camera is stationary, providing a wide view of the room and the animated characters. The view is at eye level, capturing the entire scene in a single, steady shot.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 8/224174 [00:50<390:54:28,  0.16it/s, v_num=42, train_loss_step=9.96e-40]Epoch 0:   0%|          | 8/224174 [00:50<390:55:13,  0.16it/s, v_num=42, train_loss_step=0.000]   [OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 8, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.383 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.131 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.514 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([976.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['a whimsical scene with three animated characters, each with distinctive features and expressions. The characters are positioned on a rough, textured surface, possibly a brick wall or concrete ground. The characters are engaged in a playful interaction, with one character appearing to be in a state of distress or surprise, while the other two characters seem to be observing or reacting to the situation. The scene is filled with a sense of humor and lightheartedness, as the characters exhibit exaggerated facial expressions and movements. The main subjects are three animated characters. The first character is a yellow snail with a large, expressive face and a small, round body. The second character is a red, cylindrical creature with a wide, open mouth and large, round eyes. The third character is a red, spiky creature with a large, open mouth and a surprised expression. The characters are positioned close to each other, with the yellow snail on the left, the red cylindrical creature in the middle, and the red spiky creature on the right. The yellow snail appears to be in a state of distress, while the red cylindrical creature and the red spiky creature seem to be observing or reacting to the situation. The background consists of a rough, textured surface, possibly a brick wall or concrete ground. The surface is light-colored, with visible bricks or concrete blocks. There are small patches of greenery, such as grass or moss, scattered around the characters. The scene is set outdoors, with natural light illuminating the characters and the background. The camera is stationary, providing a fixed view of the characters and their interactions. The camera angle is slightly elevated, capturing the characters from a slightly above-eye level perspective.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 9/224174 [00:54<379:53:37,  0.16it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 9/224174 [00:54<379:54:16,  0.16it/s, v_num=42, train_loss_step=0.000][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 9, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.371 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.019 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.390 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([702.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['two animated characters, a blonde man and a man with glasses, standing side by side in a dimly lit room. The blonde man, dressed in a gray hoodie and jeans, appears to be speaking or gesturing with his hands, while the man with glasses, wearing a red shirt and blue jeans, stands calmly beside him. The scene is set in a dark, industrial-looking room with large, dark windows and a door in the background. The lighting is low, creating a moody atmosphere. The background consists of a dark, industrial-style room with large, dark windows and a door. The walls are dark, and the overall lighting is dim, contributing to a somewhat mysterious or tense atmosphere. There are no additional objects or elements in the background, keeping the focus on the two main characters. The main subjects are two animated characters. The blonde man has a distinctive hairstyle and is dressed in a gray hoodie and jeans. He is positioned on the left side of the frame and appears to be speaking or gesturing with his hands. The man with glasses, who is positioned on the right side of the frame, is wearing a red shirt and blue jeans. He stands still and observes the blonde man. The camera is stationary, capturing the scene from a medium shot that includes both characters in the frame. There is no noticeable camera movement, maintaining a steady and focused view of the subjects.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=1.1286844352474133e-36
[OmniTrainingModule] training_step -> loss: 1.1286844352474133e-36
Epoch 0:   0%|          | 10/224174 [00:59<371:57:27,  0.17it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 10/224174 [00:59<371:58:07,  0.17it/s, v_num=42, train_loss_step=1.13e-36][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 10, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.410 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.021 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.431 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([903.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['a group of colorful animated characters dancing and singing in a cheerful and lively kitchen setting. The characters are dressed in vibrant costumes with bright colors and playful patterns, including yellow, blue, and red. They are engaged in a fun and energetic dance routine, moving rhythmically and joyfully to the music. The background is a brightly colored kitchen with various appliances and decorations, including a stove, oven, and refrigerator. The kitchen has a cheerful and playful atmosphere, with colorful tiles and appliances that add to the lively scene. The characters are dancing in a synchronized and rhythmic manner, moving their arms, legs, and bodies in a coordinated fashion. Their movements are lively and energetic, with a mix of steps, jumps, and arm gestures. The background remains static, emphasizing the dynamic movements of the characters. The main subjects are four animated characters, each with distinctive costumes and vibrant colors. One character wears a yellow shirt with a blue hat, another wears a blue shirt with a yellow hat, a third character wears a yellow shirt with a red hat, and the fourth character wears a blue shirt with a yellow hat. They are positioned in the center of the frame, dancing and singing together, with their movements synchronized and lively.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 11/224174 [01:04<365:51:06,  0.17it/s, v_num=42, train_loss_step=1.13e-36]Epoch 0:   0%|          | 11/224174 [01:04<365:51:39,  0.17it/s, v_num=42, train_loss_step=0.000]   [OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 11, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.401 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.016 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.417 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([816.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["a sequence of animated visuals, likely from a film or a television show, featuring a character in a fantastical setting. The sequence begins with a serene, cloudy sky, transitioning to a scene where a character, possibly a fairy or a magical creature, is seen flying through the clouds. The character is depicted in a graceful, floating motion, with a crescent moon in the background. The scene then shifts to a close-up of the character's face, emphasizing its ethereal and otherworldly appearance. The background consists of a cloudy sky with a crescent moon, creating a dreamy and ethereal atmosphere. The clouds are fluffy and white, adding to the magical feel of the scene. The setting appears to be outdoors, possibly during the evening or night, as indicated by the moon's presence. The main subject is a character with a fairy-like appearance, characterized by its small size, delicate features, and ethereal aura. The character is seen floating through the clouds, with a crescent moon in the background. The character's face is prominently featured in the latter part of the video, emphasizing its otherworldly features. The character's movements are smooth and graceful, suggesting a sense of weightlessness and freedom. The camera remains mostly static, focusing on the character's movements and expressions. The view is primarily a medium shot, capturing the character's upper body and the surrounding environment. The camera occasionally zooms in on the character's face, emphasizing its ethereal features."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 12/224174 [01:09<358:58:56,  0.17it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 12/224174 [01:09<358:59:31,  0.17it/s, v_num=42, train_loss_step=0.000][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 12, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.404 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.016 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.420 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([903.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["a person navigating through a computer interface, specifically focusing on a webpage with a profile section. The person interacts with the interface by scrolling down the page, highlighting different sections such as a profile picture, a brief description, and a list of social media accounts. the user's actions in a close-up view, emphasizing the details of the computer screen and the interface elements. The main subject is a computer screen displaying a webpage with a profile section. The profile includes a profile picture, a brief description, and a list of social media accounts. The user's hand is visible, interacting with the mouse and scrolling down the page. The profile picture is a cartoon character, and the description is in green text. The social media icons are arranged in a list below the description. The background consists of a computer screen displaying a webpage with a profile section. The interface elements include a profile picture, a brief description, and a list of social media accounts. The background is mostly white, with some reflections and shadows visible due to the lighting. The scene appears to be indoors, likely in an office or home setting. The camera is stationary, providing a close-up view of the computer screen. The focus remains on the profile section of the webpage, capturing the user's interactions with the interface."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 13/224174 [01:14<354:50:16,  0.18it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 13/224174 [01:14<354:50:42,  0.18it/s, v_num=42, train_loss_step=0.000][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 13, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.413 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.015 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.428 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([357.2500], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["A man wearing a Justice League t-shirt stands in front of a plain gray background and engages in a lively conversation. He gestures with his hands, occasionally pointing and making expressive facial expressions. The man's demeanor is animated and enthusiastic, suggesting he is passionately discussing a topic related to the Justice League characters. The background is a plain, light gray wall with a subtle texture, providing a neutral and unobtrusive backdrop that keeps the focus on the man. There are no additional objects or elements in the background, ensuring that the viewer's attention remains on the subject. The main subject is a man with short brown hair, wearing a black t-shirt with a Justice League graphic on the front. He is positioned centrally in the frame and is actively gesturing with his hands, sometimes pointing and other times making open-handed gestures. His facial expressions change throughout the video, indicating he is speaking and emphasizing his points. The camera is stationary, providing a fixed frontal view of the man. There is no noticeable camera movement, ensuring the focus remains on the subject's gestures and expressions."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=9.052445903989792e-08
[OmniTrainingModule] training_step -> loss: 9.052445903989792e-08
Epoch 0:   0%|          | 14/224174 [02:37<701:12:46,  0.09it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 14/224174 [02:37<701:13:13,  0.09it/s, v_num=42, train_loss_step=9.05e-8][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 14, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.483 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.069 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.553 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([971.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["a playful interaction between two animated pig characters. The main pig character, wearing glasses and a blue shirt, is seen holding a yellow block and engaging in a friendly conversation with a smaller pig character. The smaller pig character, dressed in a blue diaper, is being held and comforted by the main pig. The scene is filled with joyful expressions and gestures, creating a warm and endearing atmosphere. The main pig character moves its arms and head to engage with the smaller pig character, who remains mostly stationary but occasionally moves slightly. The main pig character's movements are gentle and affectionate, while the smaller pig character's movements are minimal and mostly involve slight adjustments in position. The background remains static throughout the video. The main subjects are two animated pig characters. The main pig character is larger, wearing glasses and a blue shirt, and is holding a yellow block. The smaller pig character is smaller, wearing a blue diaper, and is being held and comforted by the main pig. The main pig character is positioned on the left side of the frame, while the smaller pig character is on the right side, being held by the main pig. The background is a plain, light blue color, providing a simple and clean backdrop that keeps the focus on the main characters. There are no additional objects or elements in the background, emphasizing the interaction between the two pig characters."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 15/224174 [02:43<677:16:06,  0.09it/s, v_num=42, train_loss_step=9.05e-8]Epoch 0:   0%|          | 15/224174 [02:43<677:17:21,  0.09it/s, v_num=42, train_loss_step=0.000]  [OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 15, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.523 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.025 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.549 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([899.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["a whimsical animated character, a cartoon dog with a blue hat and red feather, standing on a yellow surface. The dog appears to be in a playful and animated state, with its eyes wide open and a cheerful expression. The background is a soft, pastel blue sky with a subtle texture, giving a dreamy and lighthearted atmosphere. The dog's movements are lively and expressive, with its head and ears moving in various directions, suggesting a sense of curiosity and joy. The main subject is a cartoon dog character, characterized by its brown fur, blue hat with a gold emblem, and a red feather on the hat. The dog has large, expressive eyes and a cheerful demeanor. It is positioned on a yellow surface, possibly a platform or a ship's deck, and its movements are animated and lively. The dog's head and ears move frequently, indicating its curiosity and excitement. The dog's movements are dynamic and expressive, with its head and ears moving in various directions. The dog occasionally tilts its head, looks around, and appears to be curious about its surroundings. The movements are moderate in amplitude and speed, contributing to the playful and animated nature of the scene. The background remains static, emphasizing the dog's movements. The camera remains stationary with a fixed view, focusing on the dog from a slightly elevated angle, capturing its expressions and movements clearly."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 16/224174 [02:48<656:08:06,  0.09it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 16/224174 [02:48<656:09:19,  0.09it/s, v_num=42, train_loss_step=0.000][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 16, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.545 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.017 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.562 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([839.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["a group of animated characters seated in a row, with one character, a man in a blue shirt, standing and speaking. The characters are engaged in a discussion, with some looking at the speaker and others looking around or at the camera. The setting appears to be a formal meeting or a courtroom, indicated by the presence of a judge's bench and a portrait of a judge on the wall. The characters exhibit various expressions and gestures, indicating a dynamic and engaging conversation. The main subjects are the animated characters. The central character, a man in a blue shirt, is standing and speaking, while the other characters are seated and listening. The seated characters include a man with gray hair, a woman with gray hair, a man with a beard, a woman with red hair, and a young boy. The characters are positioned in a row, facing the standing man. The standing man is gesturing with his hands, emphasizing his speech. The seated characters are attentive, with some looking at the speaker and others looking around or at the camera. The background is a formal setting, likely a courtroom or a meeting room. The judge's bench is visible, and there is a portrait of a judge on the wall. The lighting is warm and focused on the characters, with a slight blur in the background to emphasize the subjects. The setting is indoors, with a dark and somewhat dim atmosphere. The camera is stationary, capturing a medium shot of the characters from a frontal view. The focus is on the standing character, with the seated characters slightly blurred in the background."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 17/224174 [02:54<638:28:06,  0.10it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 17/224174 [02:54<638:29:14,  0.10it/s, v_num=42, train_loss_step=0.000][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 17, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.402 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.014 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.417 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([208.3750], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["a lively scene set in a vibrant, cartoon-style environment with a group of animated characters. The characters include Mario, Luigi, and a variety of other characters from the Mario universe, such as Peach, Bowser, and Toad. The characters are engaged in playful and dynamic interactions, with Mario and Luigi standing side by side, while other characters appear in the background, interacting with each other and the environment. The scene is filled with colorful, whimsical elements, including green grass, trees, and various Mario-themed structures like green pipes and mushrooms. The background features a lush, green landscape with rolling hills and scattered trees. The scene is filled with various Mario-themed structures, such as green pipes, mushrooms, and a large, menacing-looking character resembling Bowser. The background also includes a few other characters, like Peach and Toad, who are interacting with the environment. The overall setting is bright and cheerful, with a clear, sunny sky and a few scattered clouds. The main subjects are Mario and Luigi, who are positioned centrally in the frame. Mario is wearing a red cap and blue overalls, while Luigi is dressed in a green cap and blue overalls. They are standing close to each other, with Mario holding Luigi's hand. In the background, various other characters from the Mario universe are visible, including Peach, Bowser, and Toad. These characters are engaged in different activities, such as flying, standing on platforms, and interacting with the environment. The camera is static, providing a wide-angle view of the scene, capturing the entire group of characters and their interactions within the lush, green environment."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.09479717165231705
[OmniTrainingModule] training_step -> loss: 0.09479717165231705
Epoch 0:   0%|          | 18/224174 [02:58<618:58:00,  0.10it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 18/224174 [02:58<618:58:18,  0.10it/s, v_num=42, train_loss_step=0.0948][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 18, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.368 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.012 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.380 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([660.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["a sequence of animated characters interacting with each other. The main characters are a blue character with large eyes and a red character with a hat and glasses. The blue character initially looks surprised and then interacts with the red character, who is holding a lit match. The red character then opens a door and the blue character follows her inside. The scene takes place in front of a building with a yellow-tinted window. The background consists of a building with a yellow-tinted window, suggesting a warm, possibly evening or nighttime setting. The ground is a simple, flat surface with a slight gradient from light to dark green, indicating grass or a similar ground cover. The sky is a gradient of purple to dark blue, suggesting a nighttime or twilight scene. The main subjects are two animated characters. The blue character has large, round eyes and a simple, round face, with a surprised expression. The red character has a large, round head with a hat and glasses, and she holds a lit match in her hand. The blue character follows the red character as she opens a door and leads the way inside. The blue character's movements are slow and deliberate, indicating surprise and curiosity. The red character moves with a confident and purposeful gait, opening the door and leading the blue character inside. The background remains static, with no visible movement or changes. The camera is static, providing a consistent view of the characters and their interactions. The view is a medium shot, capturing both characters and the door they interact with."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=4.075298736599535e-32
[OmniTrainingModule] training_step -> loss: 4.075298736599535e-32
Epoch 0:   0%|          | 19/224174 [03:03<601:55:56,  0.10it/s, v_num=42, train_loss_step=0.0948]Epoch 0:   0%|          | 19/224174 [03:03<601:56:17,  0.10it/s, v_num=42, train_loss_step=4.08e-32][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 19, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.333 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.015 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.349 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([273.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["a cute, animated character wearing a green hat with antennae and a white face with pink cheeks. The character is sitting at a desk in front of a computer monitor, appearing to be tired or sleepy. The character's eyes are closed, and it occasionally opens them, looks around, and yawns. The background is a simple, light blue gradient, giving a calm and clean atmosphere. The main subject is a cartoon character with a green hat featuring antennae and a white face with pink cheeks. The character has large, expressive eyes and is positioned at a desk with a computer monitor in front of it. The character's movements include closing and opening its eyes, looking around, and yawning. The character's position remains relatively static, with slight movements of the head and arms. The background is a simple, light blue gradient, providing a clean and minimalistic setting. There are no additional objects or elements in the background, keeping the focus on the character. The scene appears to be indoors, likely in an office or study environment. The camera is stationary, providing a consistent view of the character from a slightly angled side perspective. There is no noticeable camera movement, maintaining a steady and focused view of the character."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0005019257660023868
[OmniTrainingModule] training_step -> loss: 0.0005019257660023868
Epoch 0:   0%|          | 20/224174 [03:08<585:48:58,  0.11it/s, v_num=42, train_loss_step=4.08e-32]Epoch 0:   0%|          | 20/224174 [03:08<585:49:14,  0.11it/s, v_num=42, train_loss_step=0.000502][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 20, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.331 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.013 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.344 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([914.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['Two individuals are standing close to each other, with one person gesturing with their hand.The person on the left is wearing a blue sweatshirt with a cartoon character on the front and a black strap over the shoulder. The person on the right is dressed in a dark blue uniform with visible badges and a black strap over the shoulder. Both individuals are wearing face masks.The setting appears to be an outdoor area with a metal fence in the background, suggesting a controlled environment such as a zoo or wildlife sanctuary. There is a large sheet or tarp hanging in the background, partially obscuring the view.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 21/224174 [03:12<572:00:29,  0.11it/s, v_num=42, train_loss_step=0.000502]Epoch 0:   0%|          | 21/224174 [03:12<572:00:45,  0.11it/s, v_num=42, train_loss_step=0.000]   [OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 21, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.406 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.016 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.422 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([963.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["a lively animated scene featuring a group of anthropomorphic animals in a room with a futuristic setting. The main characters are a dog, a cat, and a mouse, each with distinct personalities and costumes. The dog, dressed in a blue suit with a tie, is seen interacting playfully with the other characters. The cat, wearing a green suit and a hat, is also involved in the playful activities. The mouse, dressed in a yellow bow tie and a blue hat, is the center of attention as it is being held and manipulated by the dog. The background is a dark, futuristic room with various electronic devices and machinery, adding to the overall sci-fi atmosphere. The main subjects are the dog, the cat, and the mouse. The dog is wearing a blue suit with a tie and is holding a toy gun. The cat is wearing a green suit and a hat, and it is being held by the dog. The mouse is dressed in a yellow bow tie and a blue hat, and it is being manipulated by the dog. The dog and the cat are positioned in the center of the frame, while the mouse is often in the foreground, being held or interacted with by the dog. The movements in the video are dynamic and playful. The dog moves around the room, holding and manipulating the mouse, while the cat remains mostly stationary but reacts to the dog's actions. The mouse is frequently in motion, being held, swung, and manipulated by the dog. The background remains static, with no significant changes or movements. The camera is mostly static, with occasional slight pans and zooms to follow the movements of the characters. The view is a medium shot, capturing the main characters and their interactions within the room."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 22/224174 [03:17<559:25:43,  0.11it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 22/224174 [03:17<559:26:05,  0.11it/s, v_num=42, train_loss_step=0.000][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 22, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.467 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.016 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.483 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([797.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['a colorful and playful animation of a cartoon bunny character in a bright, cheerful environment. The bunny is positioned in the center of the frame, and various elements such as balloons and clouds float around it. The background is a vibrant blue sky with white clouds, and there are additional animated elements like a pink bunny and a blue and white ball. The main subject is a cartoon bunny character with a light brown body, white face, and large, expressive eyes. The bunny is positioned centrally in the frame and remains relatively stationary throughout the video. Surrounding the bunny are various animated elements, including balloons in different colors (green, blue, and yellow) and a pink bunny character on the left side. Additionally, a blue and white ball floats in the background. The background is a bright blue sky with fluffy white clouds, creating a cheerful and whimsical atmosphere. The clouds are scattered around the frame, adding depth and dimension to the scene. The overall setting is simple and clean, focusing on the playful elements and the main character. The camera is static, with no movement or changes in angle, maintaining a consistent view of the bunny and its surroundings.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 23/224174 [03:22<548:26:00,  0.11it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 23/224174 [03:22<548:26:15,  0.11it/s, v_num=42, train_loss_step=0.000][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 23, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.532 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.019 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.552 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([357.2500], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["a close-up view of a snowman wearing a black knitted sweater with a festive winter design. The snowman has large, expressive eyes and a small, orange carrot nose. It is wearing a black top hat with a red band. The snowman's face is animated, with its eyes moving and its mouth opening and closing as if it is speaking or reacting to something. The background is a plain, dark brown wall, which contrasts with the white snowman and the black sweater. The scene is indoors, likely in a living room or a similar cozy space. The lighting is warm and soft, highlighting the snowman's features. The main subject is a snowman, characterized by its large, round eyes, a small orange carrot nose, and a black top hat with a red band. The snowman is positioned centrally in the frame, and its facial expressions change throughout the video, indicating it is animated. The snowman's eyes move and its mouth opens and closes, suggesting it is speaking or reacting to something. The camera is stationary, providing a close-up view of the snowman's face. The camera angle is slightly tilted upwards, capturing the snowman's eyes and the top hat."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=4.148754939592436e-08
[OmniTrainingModule] training_step -> loss: 4.148754939592436e-08
Epoch 0:   0%|          | 24/224174 [04:40<727:47:58,  0.09it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 24/224174 [04:40<727:48:29,  0.09it/s, v_num=42, train_loss_step=4.15e-8][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 24, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.491 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.018 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.510 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([921.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['a series of animated characters in a stylized, cartoonish cityscape. The characters, which include a yellow and a brown character, interact with each other and the environment. The yellow character is initially seen lying down, then it starts to move and interact with the brown character, who is initially in a similar position. The brown character then stands up and the two characters engage in a playful interaction, culminating in a dramatic explosion. The scene then transitions to a calm, empty cityscape with the characters lying down again. The main subjects are two animated characters. The yellow character is initially lying down, then it starts to move and interact with the brown character. The brown character is initially lying down as well but then stands up. The characters interact with each other through gestures and expressions, culminating in a dramatic explosion. The yellow character is positioned on the left side of the frame, while the brown character is on the right. The background is a stylized, cartoonish cityscape with various buildings, a bridge, and a traffic light. The scene is set during the daytime with a clear sky. The buildings are simple and stylized, with no intricate details, giving the scene a minimalist and cartoonish feel. The camera is static, providing a wide view of the entire scene. There is no camera movement, and the focus remains on the animated characters and their interactions.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 25/224174 [04:45<711:14:36,  0.09it/s, v_num=42, train_loss_step=4.15e-8]Epoch 0:   0%|          | 25/224174 [04:45<711:14:51,  0.09it/s, v_num=42, train_loss_step=0.000]  [OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 25, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.462 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.015 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.477 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([869.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["A cartoonish vehicle, resembling a small van, is driving through a tropical jungle. The van is being driven by a character with a large, expressive face, wearing a red jacket and a black hat. The vehicle is moving forward, and the driver appears to be focused on the road ahead. The background is filled with lush green foliage, tall palm trees, and a clear sky, creating a vibrant and colorful scene. The main subject is a small, white van with a large, clear, see-through window. The driver of the van is a character with a large, expressive face, wearing a red jacket and a black hat. The driver is positioned in the driver's seat, holding the steering wheel, and appears to be focused on driving. The van is moving forward, and the driver's facial expressions change slightly as they navigate the jungle. The van moves steadily forward through the jungle, with the driver maintaining a consistent speed. The background remains static, with the only movement being the forward motion of the van. The driver's facial expressions change slightly as they navigate the jungle, indicating a sense of focus and determination. The camera is stationary, providing a consistent view of the van and the jungle background from a side angle. The view is slightly elevated, giving a clear perspective of the van's movement and the surrounding environment."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 26/224174 [04:50<694:59:48,  0.09it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 26/224174 [04:50<695:00:01,  0.09it/s, v_num=42, train_loss_step=0.000][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 26, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.401 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.012 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.413 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([612.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['three animated cats standing in front of a blue fence. The cats are positioned in a row, with the largest cat on the left wearing a green hat and green sweater, the middle cat wearing a red hat and red sweater, and the smallest cat on the right wearing a blue hat and blue sweater. The cats are smiling and appear to be interacting with each other, creating a cheerful and friendly atmosphere. The background consists of a yellow wall with a red door, and the ground is covered with sand. The scene is set in a simple, cartoon-like environment, with no additional objects or elements. The main subjects are three animated cats. The largest cat on the left is wearing a green hat and green sweater, the middle cat is wearing a red hat and red sweater, and the smallest cat on the right is wearing a blue hat and blue sweater. The cats are standing close to each other, with the middle cat slightly ahead of the other two. They are smiling and appear to be looking at each other, creating a sense of camaraderie and joy. The cats are mostly stationary, with slight movements such as turning their heads and shifting their positions slightly. The middle cat occasionally moves forward slightly, while the other two cats remain mostly stationary. The background remains static throughout the video, with no changes or movements. The camera is stationary, providing a fixed view of the cats and the background. The view is at eye level, capturing the cats from the front and slightly to the side, allowing for a clear and unobstructed view of their expressions and interactions.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=1.4569053361356957e-27
[OmniTrainingModule] training_step -> loss: 1.4569053361356957e-27
Epoch 0:   0%|          | 27/224174 [04:54<679:59:14,  0.09it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 27/224174 [04:54<679:59:26,  0.09it/s, v_num=42, train_loss_step=1.46e-27][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 27, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.393 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.013 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.406 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([873.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["a group of animated characters engaging in various activities in a lush, vibrant forest setting. The characters, who are young girls, are dressed in colorful outfits and interact with each other and their surroundings. The scene is filled with vivid colors and playful energy, showcasing the characters' joyful expressions and movements. The background is a lush, green forest filled with tall, leafy plants and trees. The forest is abundant with strawberries, adding a vibrant red color to the scene. The ground is covered with green grass, and the sunlight filters through the leaves, creating a warm and inviting atmosphere. The overall setting is bright and cheerful, suggesting a sunny day in a magical forest. The main subjects are four animated girls. The first girl, on the left, wears a white hat with a flower and a yellow polka-dotted dress. The second girl, in the middle, wears a yellow dress with polka dots and a flower in her hair. The third girl, on the right, wears a pink dress with a heart pattern and a flower in her hair. The fourth girl, in the background, is a small purple and white dog. The girls interact with each other, sometimes standing close together and other times walking or playing separately. The camera view is a mix of close-up and medium shots, focusing on the characters' faces and upper bodies. The camera movement is smooth and follows the characters' actions, providing a dynamic and engaging perspective of the scene."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 28/224174 [04:59<666:06:57,  0.09it/s, v_num=42, train_loss_step=1.46e-27]Epoch 0:   0%|          | 28/224174 [04:59<666:07:09,  0.09it/s, v_num=42, train_loss_step=0.000]   [OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 28, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.376 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.014 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.391 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([839.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["A man with a beard and headphones is sitting at a desk in a recording studio, engaging in a conversation. He is wearing a dark blue shirt and is positioned in front of a microphone, which is part of a professional recording setup. The background features a large monitor displaying a cartoon-like animation of a character, and various recording equipment and bottles are scattered around the desk. The man gestures with his hands while speaking, indicating an active and engaging conversation. The main subject is a man with a beard, wearing a dark blue shirt and headphones. He is seated at a desk, facing a microphone, and is actively gesturing with his hands as he speaks. The microphone is positioned in front of him, and he is surrounded by various recording equipment, including a laptop, a sound mixer, and a large monitor displaying a cartoon-like animation. The man's movements are primarily focused on his hands and upper body as he gestures while speaking. His gestures are varied, including pointing, waving, and moving his hands expressively. The background remains static, with no noticeable changes or movements. The camera is stationary, providing a fixed view of the man and his surroundings from a slightly elevated angle, capturing the entire scene in a clear and focused manner."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 29/224174 [05:04<652:51:06,  0.10it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 29/224174 [05:04<652:51:18,  0.10it/s, v_num=42, train_loss_step=0.000][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 29, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.383 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.014 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.397 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([682.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["A cartoon character, a small monkey, is seen walking along a sidewalk in front of a store. The monkey carries a large, green, and metallic object on its back, which appears to be a large, multi-layered box or container. The scene is set in a simple, cartoon-style urban environment with a focus on the monkey's journey along the sidewalk. The main subject is a small, cartoon monkey with a brown body, green pants, and a white belly. The monkey is carrying a large, green, and metallic object on its back, which is a multi-layered box or container. The monkey walks from the right side of the frame to the left, maintaining a steady pace. The monkey moves from the right side of the frame to the left, walking steadily along the sidewalk. The movement is smooth and consistent, with the monkey's legs and arms moving in a natural walking motion. The background remains static, with no changes or movements. The camera is stationary, providing a fixed view of the scene from a slightly elevated angle, capturing the entire sidewalk and the monkey's movement within the frame."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=8.8883392476526e-35
[OmniTrainingModule] training_step -> loss: 8.8883392476526e-35
Epoch 0:   0%|          | 30/224174 [05:08<640:29:29,  0.10it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 30/224174 [05:08<640:29:39,  0.10it/s, v_num=42, train_loss_step=8.89e-35][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 30, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.381 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.013 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.394 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([943.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["A cartoon monkey is engaged in a playful activity involving a red paint tray and a paint roller. The monkey, dressed in a red jumpsuit, is seen interacting with the paint roller, dipping it into the tray, and then using it to spread the paint on the floor. The scene is set against a plain white background, emphasizing the vibrant colors of the monkey and the paint. The monkey's movements are lively and animated, with a focus on the interaction between the paint roller and the tray. The monkey's movements are dynamic and playful, involving bending, reaching, and manipulating the paint roller. The monkey moves the paint roller in and out of the tray, dips it into the paint, and then spreads it on the floor. The movements are moderate in amplitude, with a medium speed, and are primarily directed downward and sideways. The background is a plain white surface, providing a clean and uncluttered backdrop that highlights the monkey and the paint tray. There are no additional objects or elements in the background, keeping the focus on the main subjects. The camera is stationary, providing a top-down view of the scene, capturing the monkey's actions and the paint tray in detail."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 31/224174 [05:13<629:23:54,  0.10it/s, v_num=42, train_loss_step=8.89e-35]Epoch 0:   0%|          | 31/224174 [05:13<629:24:07,  0.10it/s, v_num=42, train_loss_step=0.000]   [OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 31, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.371 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.021 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.393 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([971.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["A cartoon bear character is depicted in various poses and actions, including waving, adjusting a hat, and interacting with a painting set.The bear character is anthropomorphic with a friendly demeanor, characterized by large expressive eyes, a small black nose, and a pink bow tie. It wears a blue beret and a pink bow tie, and its fur is a rich brown color. The bear's movements are animated and lively, suggesting a cheerful and engaging personality.The setting appears to be a cozy, indoor space with a rustic charm, featuring a brick wall adorned with framed pictures and certificates, a wooden shelf with various items, and a painting easel with a canvas and paint palette. The overall ambiance suggests a creative and artistic environment."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 32/224174 [05:17<618:29:04,  0.10it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 32/224174 [05:17<618:29:15,  0.10it/s, v_num=42, train_loss_step=0.000][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 32, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.345 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.013 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.358 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([797.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["a character dressed in a blue uniform with a badge, holding and examining a piece of paper.The character is a three-dimensional animated figure with a round, plump face, large blue eyes, and a small, pointed nose. The character has a tuft of red hair on the top of its head. The uniform is detailed with a badge on the left chest area, and the character's hands are visible, holding a piece of paper.The setting appears to be an indoor environment, with the character standing in front of a framed picture featuring a fire truck and firefighters. The background is a plain wall with a wooden frame around the picture, suggesting a simple, uncluttered space."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 33/224174 [05:22<609:00:48,  0.10it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 33/224174 [05:22<609:01:00,  0.10it/s, v_num=42, train_loss_step=0.000][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 33, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.512 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.018 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.530 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([776.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["A cartoon wolf character with a red beanie and a white shirt is walking along a grassy bank near a body of water. The wolf has a fluffy white coat with a thick, bushy tail and is wearing blue shorts. The wolf is carrying a backpack with a mushroom and a flower. As the wolf walks, it encounters a group of mushrooms and flowers, which it interacts with playfully. The scene is set in a bright, cheerful environment with vibrant green grass and a blue water body. The main subject is a cartoon wolf character. The wolf has a red beanie, a white shirt, and blue shorts. It is carrying a backpack with a mushroom and a flower. The wolf walks along the grassy bank, interacting with the mushrooms and flowers. The wolf's movements are playful and animated, with its arms and legs moving dynamically as it walks and interacts with the mushrooms and flowers. The background consists of a bright, green grassy bank with a blue water body in the foreground. There are several mushrooms and flowers scattered around, adding to the playful and whimsical atmosphere. The scene is set in a cheerful, colorful environment with vibrant green grass and a blue water body. The camera follows the wolf from a side view, maintaining a consistent distance and angle as it walks and interacts with the mushrooms and flowers."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 34/224174 [05:27<600:06:26,  0.10it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 34/224174 [05:27<600:06:46,  0.10it/s, v_num=42, train_loss_step=0.000][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 34, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.487 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.016 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.503 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([649.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['two young men engaging in a playful and animated conversation against a backdrop of a cartoonish cityscape. They are positioned in front of a large, bold, and colorful heart-shaped sign that reads "NICOLE AMES." The men are dressed casually and are seen making various facial expressions and gestures, indicating a lighthearted and friendly interaction. The background consists of a cartoonish cityscape with buildings and a green park area. The buildings are stylized with a brown and tan color scheme, and the park area features a green tree and a small green house. The scene is set against a blue sky with fluffy white clouds, adding a whimsical and playful atmosphere to the video. The main subjects are two young men. The first man has curly hair and is wearing a brown t-shirt. The second man has short hair and is wearing a red and black checkered shirt. They are positioned side by side, facing the camera, and are engaged in a conversation. Their expressions and gestures change throughout the video, indicating a dynamic and lively interaction. The camera is stationary, capturing the scene from a frontal view with a medium shot that includes both subjects and the background.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=4.047365758214533e-31
[OmniTrainingModule] training_step -> loss: 4.047365758214533e-31
Epoch 0:   0%|          | 35/224174 [05:32<592:10:19,  0.11it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 35/224174 [05:32<592:10:29,  0.11it/s, v_num=42, train_loss_step=4.05e-31][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 35, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.578 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.015 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.594 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([994.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["a playful interaction between a cartoon cat and a young girl in a cozy, colorful room. The cat, with a striped pattern and a white face, is lying on a bed, while the girl, with brown hair and wearing a pink dress with white floral patterns, engages with the cat by petting and talking to it. The girl's expressions and gestures are animated and cheerful, indicating a friendly and affectionate relationship with the cat. The background is a warm, inviting room with a bed covered in colorful pillows and blankets. There are various decorations, including a picture frame with a heart-shaped design and a wall with a floral pattern. The room is well-lit, suggesting a comfortable and cheerful atmosphere. The main subjects are a cat and a girl. The cat has a striped pattern with a white face and is lying on its back on a bed. The girl, with brown hair and wearing a pink dress with white floral patterns, is standing next to the cat, petting it and talking to it. The cat's eyes are closed in a relaxed manner, and the girl's expressions and gestures are animated and affectionate. The cat remains mostly stationary, lying on its back, while the girl moves around, petting the cat and occasionally looking at the camera. The girl's movements are gentle and playful, with small, deliberate actions such as petting the cat and looking at the camera. The background remains static, with no significant changes or movements. The camera is stationary, providing a consistent view of the scene from a slightly elevated angle, capturing both the cat and the girl in a medium shot."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 36/224174 [05:37<584:30:13,  0.11it/s, v_num=42, train_loss_step=4.05e-31]Epoch 0:   0%|          | 36/224174 [05:37<584:30:49,  0.11it/s, v_num=42, train_loss_step=0.000]   [OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 36, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.363 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.015 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.378 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([769.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['Two children are playing with a toy Lego car on a road. The boy in the yellow shirt is sitting on the Lego car, while the girl in the purple shirt is standing behind him, holding onto the car. The boy in the blue shirt is lying on the ground, reaching out towards the Lego car. The scene is set in a bright, colorful, and simplistic environment with a road and green hills in the background. The children are engaged in a playful and imaginative activity, creating a lively and joyful atmosphere. The main subjects are two children and a Lego car. The boy in the yellow shirt is sitting on the Lego car, wearing a yellow shirt and purple pants. The girl in the purple shirt is standing behind him, wearing a purple shirt and purple pants. The boy in the blue shirt is lying on the ground, wearing a blue shirt and gray shorts, reaching out towards the Lego car. The Lego car is a bright yellow block with a red roof, positioned on a black road. The children are interacting with each other and the Lego car, creating a dynamic and engaging scene. The background features a simple, colorful, and cartoonish landscape with a road stretching into the distance, flanked by green hills. The sky is blue with a few scattered clouds, adding to the cheerful and playful atmosphere. The scene is devoid of any other objects or characters, focusing solely on the children and their imaginative play. The camera is stationary, providing a wide-angle view of the scene from a slightly elevated angle, capturing the entire interaction between the children and the Lego car.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=7.006492321624085e-45
[OmniTrainingModule] training_step -> loss: 7.006492321624085e-45
Epoch 0:   0%|          | 37/224174 [05:42<576:41:00,  0.11it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 37/224174 [05:42<576:41:11,  0.11it/s, v_num=42, train_loss_step=7.01e-45][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 37, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.408 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.016 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.425 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([891.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['A muscular man in a gray tank top and red cap is standing in a gym, engaging in a conversation with another person. He gestures with his hands while speaking, and at one point, they shake hands. The gym environment is characterized by a large, textured wall mural and various gym equipment, including weight racks and benches. The man appears to be in a focused and animated conversation, possibly discussing workout routines or gym activities. The main subjects are two men. The first man is wearing a gray tank top, a red cap, and has a muscular build. He is standing and gesturing with his hands while talking. The second man, partially visible, is wearing a black tank top and black shorts, and he is also muscular. They shake hands, indicating a friendly interaction. The man in the gray tank top is positioned centrally in the frame, while the second man is on the right side. The camera is stationary, capturing the scene from a medium shot that includes both men and part of the gym background. The view is at eye level, providing a clear and unobstructed view of the interaction between the two men.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 38/224174 [05:47<568:57:04,  0.11it/s, v_num=42, train_loss_step=7.01e-45]Epoch 0:   0%|          | 38/224174 [05:47<568:57:13,  0.11it/s, v_num=42, train_loss_step=0.000]   [OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 38, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.398 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.014 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.413 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([943.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["a playful and animated scene set in a snowy environment where two anthropomorphic dogs engage in a series of exaggerated and humorous poses and gestures. The main character, a pink dog with a large, expressive face and a muscular build, performs various dynamic poses, including flexing his muscles, throwing his arms up, and striking a fighting stance. Another dog, dressed in a bright yellow outfit, joins in, mimicking the pink dog's actions and adding to the comedic effect. The scene is filled with vibrant colors and exaggerated expressions, creating a whimsical and entertaining atmosphere. The background is a snowy landscape with a few snow-covered trees and a clear sky. The snow is pristine and untouched, adding to the serene and picturesque setting. The scene is brightly lit, suggesting it is daytime, and the overall atmosphere is calm and peaceful, contrasting with the animated actions of the dogs. The main subjects are two anthropomorphic dogs. The first dog, a pink dog with a muscular build, expressive eyes, and a large, friendly face, is the central character. He performs various dynamic poses, such as flexing his muscles, throwing his arms up, and striking a fighting stance. The second dog, dressed in a bright yellow outfit, mimics the pink dog's actions, adding to the comedic effect. The pink dog is positioned centrally, while the yellow dog is on the right side of the frame. The pink dog's movements are exaggerated and energetic, while the yellow dog's movements are more subdued but still playful. The camera remains mostly static, focusing on the two dogs with a medium shot that captures their full bodies and expressions. The view is slightly angled to the side, providing a clear view of their actions and interactions."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 39/224174 [05:52<562:23:11,  0.11it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 39/224174 [05:52<562:23:22,  0.11it/s, v_num=42, train_loss_step=0.000][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 39, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.374 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.013 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.388 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([891.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["a small, animated character, a purple-clad girl, standing on a wooden porch during a rainstorm. She is initially seen looking around, then she walks towards a door, opens it, and steps inside. The scene transitions to a cozy, warmly lit room filled with bookshelves and various decorations. The girl explores the room, looking at the books and other items on the shelves. The main subject is a small, animated girl wearing a purple dress and a pink hat. She has large, expressive eyes and a cheerful demeanor. She is initially standing on the porch, then she walks towards the door, opens it, and steps inside. Inside, she interacts with the bookshelves, examining the books and other items on the shelves. The girl's movements are slow and deliberate, indicating a sense of curiosity and exploration. She walks from the porch to the door, opens it, and steps inside. Inside, she moves slowly along the bookshelves, looking at the books and other items. The background remains static, with the rain continuing to fall outside, adding a dynamic element to the scene. The camera follows the girl's movements smoothly, maintaining a medium shot that captures both her and the surrounding environment."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 40/224174 [05:56<555:19:25,  0.11it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 40/224174 [05:56<555:19:34,  0.11it/s, v_num=42, train_loss_step=0.000][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 40, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.338 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.014 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.352 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([810.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["two animated characters, one blue and one orange, standing in a dark, rainy environment. The blue character, dressed in a black and white striped suit, holds a green bottle with a white label. The orange character, wearing a bright orange outfit, stands beside the blue character, occasionally making expressive facial expressions. The scene is set in a dark, rainy environment, with a large, glowing white and blue character floating in the background. The background is dark and rainy, with a large, glowing white and blue character floating in the distance. The scene is set indoors, with a dark, shadowy environment that contrasts with the bright, glowing character in the background. The main subjects are two animated characters. The blue character is dressed in a black and white striped suit and holds a green bottle with a white label. The orange character is dressed in a bright orange outfit and makes various facial expressions throughout the video. The blue character is positioned on the left side of the frame, while the orange character is on the right. The floating character in the background is large and glowing, adding a surreal element to the scene. The blue character remains mostly stationary, holding the green bottle and occasionally making small movements. The orange character makes expressive facial expressions, such as opening and closing its mouth, and occasionally shifts its position slightly. The floating character in the background remains stationary throughout the video. The overall movement is minimal and focused on the animated characters' facial expressions and slight body movements."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 41/224174 [06:01<548:37:53,  0.11it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 41/224174 [06:01<548:38:01,  0.11it/s, v_num=42, train_loss_step=0.000][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 41, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.373 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.013 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.386 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([849.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["a sequence of animated characters in a cave-like setting. The characters are anthropomorphic animals, with one character wearing a party hat and another wearing a tie. The characters interact with each other, displaying various expressions and movements. The scene is set in a dark, rocky cave with a sense of depth and shadow, creating a dramatic and somewhat eerie atmosphere. The characters move around the cave, interacting with each other. The character with the party hat moves forward and turns to face the other character, who remains relatively stationary but makes expressive gestures. The character with the tie also moves around, sometimes facing the other character and sometimes looking off to the side. The movements are moderate in amplitude, with a mix of walking and gesturing. The background remains static, emphasizing the characters' movements. The main subjects are two anthropomorphic animals. The first character is a purple creature with a party hat, wearing a red outfit. The second character is a green creature with a tie, wearing a blue outfit. The purple creature moves forward and turns to face the green creature, who remains relatively stationary but makes expressive gestures. The characters are positioned centrally in the frame, with the purple creature initially in the foreground and the green creature in the background. The camera remains stationary, providing a fixed view of the scene from a medium shot perspective, capturing the characters and their interactions within the cave setting."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 42/224174 [06:06<542:35:27,  0.11it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 42/224174 [06:06<542:35:35,  0.11it/s, v_num=42, train_loss_step=0.000][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 42, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.374 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.017 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.391 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([955.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['a cheerful scene set in a whimsical Halloween-themed town. A woman dressed in a purple dress and a black shirt with a Halloween-themed design dances joyfully in the middle of the street. She holds a pumpkin and a candy bar, smiling and laughing throughout her dance. A man in a black cape and a blue character with a pumpkin head also join her, adding to the festive atmosphere. The background is a colorful, cartoonish Halloween-themed town with various shops and decorations, including a "Cafe" sign, a "Candy" sign, and a large orange pumpkin. The scene is set against a backdrop of a clear blue sky with white stars, enhancing the festive and playful mood. The main subjects are a woman and a man. The woman is dressed in a purple dress with a black shirt featuring Halloween-themed designs, purple tights, and purple high heels. She holds a pumpkin in her left hand and a candy bar in her right hand. The man, dressed in a black cape, joins her midway through the video. A blue character with a pumpkin head also appears, adding to the festive scene. They are all dancing and interacting joyfully. The woman and the man dance energetically, moving their arms and legs in a lively manner. The woman\'s movements include spinning, twirling, and holding the pumpkin and candy bar. The man\'s movements are more subdued but still lively, adding to the festive atmosphere. The blue character moves in a playful manner, contributing to the overall dynamic of the scene. The background remains static, emphasizing the animated actions of the main subjects. The camera remains stationary, capturing the scene from a fixed, wide-angle view that encompasses all the main subjects and the festive background.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 43/224174 [06:10<536:27:51,  0.12it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 43/224174 [06:10<536:28:00,  0.12it/s, v_num=42, train_loss_step=0.000][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 43, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.469 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.029 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.498 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([540.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["two animated characters, a chef and a waiter, engaged in a friendly conversation and interaction. The chef, wearing a red shirt and a white apron, and the waiter, wearing a yellow shirt and a yellow cap, are standing close to each other, smiling and laughing. The chef holds a piece of paper, which they both examine and discuss, while the waiter gestures and points at the paper. The scene is set in a kitchen environment with a background of a fruit bowl and a blurred green tree, suggesting a casual and relaxed atmosphere. The background features a kitchen setting with a blurred green tree and a fruit bowl containing red fruits, possibly apples or cherries. The background is softly focused, creating a warm and inviting atmosphere. The colors are vibrant, with the green of the tree and the red of the fruit contrasting against the characters' clothing. The main subjects are the chef and the waiter. The chef is on the left side of the frame, wearing a red shirt and a white apron, and the waiter is on the right side, wearing a yellow shirt and a yellow cap. The chef holds a piece of paper, which they both examine and discuss. The waiter gestures with their hands, pointing at the paper and smiling. The characters are positioned close to each other, indicating a friendly and collaborative interaction. The camera is stationary, providing a medium close-up view of the characters, capturing their expressions and interactions clearly."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=9.612636817910325e-21
[OmniTrainingModule] training_step -> loss: 9.612636817910325e-21
Epoch 0:   0%|          | 44/224174 [06:15<531:32:33,  0.12it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 44/224174 [06:15<531:32:41,  0.12it/s, v_num=42, train_loss_step=9.61e-21][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 44, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.421 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.035 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.456 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([924.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['three young men sitting at a table, engaging in a lively conversation. They are wearing funny face masks and are animatedly gesturing and talking to each other. The setting appears to be a casual indoor environment, possibly a studio or a room with a red and white wall in the background. The atmosphere is light-hearted and playful, with the men frequently making expressive hand gestures and facial expressions. The main subjects are three young men. The man in the center is wearing a black shirt with the text "NAO FALA ME DO BRASIL" and a white face mask with a cartoon character. The man on the left is wearing a dark shirt and a face mask with a black and white pattern. The man on the right is wearing a green shirt and a face mask with a cartoon character. They are seated at a table, facing each other, and are actively talking and gesturing with their hands. The background consists of a red and white wall with a red circular logo or emblem. The setting appears to be indoors, possibly a studio or a room designed for recording or filming. The lighting is bright and even, highlighting the subjects and their expressions. The camera is stationary, capturing the scene from a medium shot that includes all three subjects and part of the background. The view is straight-on, providing a clear and unobstructed view of the subjects and their interactions.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 45/224174 [06:21<527:25:46,  0.12it/s, v_num=42, train_loss_step=9.61e-21]Epoch 0:   0%|          | 45/224174 [06:21<527:26:13,  0.12it/s, v_num=42, train_loss_step=0.000]   [OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 45, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.493 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.019 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.513 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([649.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["A woman with shoulder-length brown hair, wearing glasses and a blue top, stands in front of a microphone in a recording studio. She is smiling and appears to be speaking or singing. The background features a colorful, animated-style wall with various characters and objects, suggesting a fun and creative environment. The woman's expressions change subtly as she speaks, and her body remains mostly stationary, with slight movements of her head and shoulders. The background remains static throughout the video, with no visible changes or movements. The main subject is a woman with shoulder-length brown hair, wearing glasses and a blue top. She is positioned in front of a microphone, which is prominently placed in the foreground. Her facial expressions change slightly, indicating she is speaking or singing. She is wearing a light-colored cardigan over her blue top. The camera is stationary, capturing a medium close-up view of the woman in front of the microphone. The focus remains sharp on the woman, with the background slightly blurred to emphasize her presence."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=3.8647228532577797e-31
[OmniTrainingModule] training_step -> loss: 3.8647228532577797e-31
Epoch 0:   0%|          | 46/224174 [06:25<522:12:29,  0.12it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 46/224174 [06:25<522:12:40,  0.12it/s, v_num=42, train_loss_step=3.86e-31][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 46, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.351 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.019 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.371 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([966.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["a cute, animated character wearing a green, fluffy costume with a white face and large, expressive eyes. The character is lying on a blue couch, appearing to be asleep or in a deep state of relaxation. The character's facial expressions change subtly, showing signs of snoring and slight movements. The background is minimalistic, with a plain blue wall and a brown book placed on the couch, adding to the cozy and relaxed atmosphere. The main subject is a character wearing a green, fluffy costume with a white face and large, expressive eyes. The character is lying on its back on a blue couch, with its arms resting on its stomach and its legs bent at the knees. The character's facial expressions change slightly, indicating it is either sleeping or in a deep state of relaxation. The character's position remains mostly static, with only minor movements such as slight shifts in its head and arms. The background is a simple, minimalistic setting with a plain blue wall. There is a brown book placed on the couch next to the character, adding a touch of realism to the scene. The overall setting is cozy and relaxed, with no additional objects or elements in the background. The camera is stationary, providing a consistent, close-up view of the character from a slightly elevated angle, capturing the details of the character's facial expressions and the surrounding environment."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 47/224174 [06:30<516:55:34,  0.12it/s, v_num=42, train_loss_step=3.86e-31]Epoch 0:   0%|          | 47/224174 [06:30<516:55:44,  0.12it/s, v_num=42, train_loss_step=0.000]   [OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 47, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.434 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.014 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.448 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([133.8750], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['two animated snowman figurines, one larger and one smaller, positioned side by side on a reflective purple surface. The larger snowman is on the left and the smaller one is on the right. Both snowmen have blue eyes, carrot noses, and are dressed in white with black buttons. The larger snowman is waving its right arm while the smaller one is standing still. In the background, there are other animated characters, including a blonde-haired girl with blue eyes and a yellow dress, and a blue-haired girl with a purple dress. The scene is set against a purple background with a reflective surface, giving it a shiny and glossy appearance. The background includes other animated characters, such as a blonde-haired girl with blue eyes and a yellow dress, and a blue-haired girl with a purple dress. The setting appears to be a stylized, animated environment, possibly a toy store or a play area. The main subjects are two animated snowman figurines. The larger snowman is on the left, with a larger body and a bigger smile, while the smaller snowman is on the right, with a smaller body and a smaller smile. The larger snowman is waving its right arm, while the smaller snowman is standing still. The camera is stationary, providing a close-up view of the two snowman figurines. The view is slightly angled to capture both snowmen and the background characters.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=4.303255081176758
[OmniTrainingModule] training_step -> loss: 4.303255081176758
Epoch 0:   0%|          | 48/224174 [06:35<513:11:34,  0.12it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 48/224174 [06:35<513:11:44,  0.12it/s, v_num=42, train_loss_step=4.300][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 48, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.365 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.018 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.384 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([790.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['A woman with long brown hair, wearing a black hoodie, is seen in a room with a colorful wallpaper featuring various cartoon-like drawings. She is engaged in a conversation, using expressive hand gestures to emphasize her points. Her facial expressions range from thoughtful to animated, indicating a lively and engaging conversation. The background is a colorful wallpaper featuring various cartoon-like drawings of people and objects. The wallpaper is vibrant and adds a playful and artistic atmosphere to the scene. The room appears to be well-lit, with natural or artificial light illuminating the space. The main subject is a woman with long brown hair, wearing a black hoodie. She is positioned centrally in the frame, facing the camera. Throughout the video, she uses her hands to gesture, sometimes pointing or holding her chin, and her facial expressions change from thoughtful to animated. Her movements are deliberate and expressive, indicating an engaging conversation. The camera is stationary, capturing a medium close-up view of the woman. The focus remains steady on her face and upper body, allowing the viewer to clearly see her expressions and gestures.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 49/224174 [06:40<508:21:10,  0.12it/s, v_num=42, train_loss_step=4.300]Epoch 0:   0%|          | 49/224174 [06:40<508:21:20,  0.12it/s, v_num=42, train_loss_step=0.000][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 49, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.333 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.013 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.346 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([570.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['a whimsical scene set in a lush, green forest with a variety of colorful animated characters. The characters include a yellow robot with a round head and large eyes, a white rabbit with a blue helmet, a pink bunny with a pink bow, a blue bird with a yellow beak, and a purple bird with a blue beak. The characters interact with each other, engaging in playful and friendly conversations. The scene is filled with vibrant colors and a cheerful atmosphere, creating a delightful and engaging visual experience. The main subjects are the animated characters. The yellow robot has a round head with large eyes and a friendly expression. The white rabbit wears a blue helmet and blue overalls. The pink bunny has a pink bow in its hair and wears a pink dress. The blue bird has a yellow beak and wears a blue hat. The purple bird has a blue beak and wears a blue hood. These characters are positioned close to each other, interacting and conversing throughout the video. The characters move around the forest, engaging in various activities. The yellow robot walks forward and turns its head to look at the other characters. The white rabbit stands still and occasionally looks around. The pink bunny moves slightly, adjusting its position and looking at the other characters. The blue bird flaps its wings and moves around, while the purple bird stands still and occasionally looks around. The movements are smooth and moderate in speed, creating a lively and engaging scene. The camera remains mostly static, focusing on the characters and their interactions, with occasional slight pans to follow the movements of the characters. The view is at a medium distance, providing a clear and detailed look at the characters and their surroundings.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=1.744228574973195e-23
[OmniTrainingModule] training_step -> loss: 1.744228574973195e-23
Epoch 0:   0%|          | 50/224174 [06:44<503:37:18,  0.12it/s, v_num=42, train_loss_step=0.000]Epoch 0:   0%|          | 50/224174 [06:44<503:37:25,  0.12it/s, v_num=42, train_loss_step=1.74e-23][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 50, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.361 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.014 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.375 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([612.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['a humorous and exaggerated confrontation between two animated characters, a police officer and a civilian, in front of a building with a door. The civilian character, a rotund man with a large nose, repeatedly kicks the police officer, who is a smaller, more slender figure with a hat and a badge. The scene is set in a bright, cartoonish environment with a green lawn and a building with a door that glows with a green light. The background consists of a building with a door that glows with a green light. The door is flanked by two metal handrails. The scene is set on a bright green lawn, suggesting an outdoor setting. The building has a simple, square design with a flat roof and a small, decorative column on the right side. The main subjects are two animated characters. The first character is a police officer, depicted as a smaller, slender figure with a hat, badge, and a serious expression. The second character is a rotund civilian with a large nose, wearing a hat and a dark outfit. The civilian character repeatedly kicks the police officer, who is positioned in front of the door. The civilian character moves dynamically, kicking the police officer with force. The police officer remains relatively stationary, taking the kicks but not moving much. The background remains static throughout the video, with no changes or movements. The camera is stationary, providing a fixed view of the scene from a slightly elevated angle, capturing the entire interaction between the two characters.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=1.9151121884826072e-27
[OmniTrainingModule] training_step -> loss: 1.9151121884826072e-27
Epoch 0:   0%|          | 51/224174 [06:48<499:11:10,  0.12it/s, v_num=42, train_loss_step=1.74e-23]Epoch 0:   0%|          | 51/224174 [06:48<499:11:17,  0.12it/s, v_num=42, train_loss_step=1.92e-27][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 51, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.417 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.017 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.434 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([506.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["a pink, cartoonish character with a cheerful expression, floating in the air against a pink background adorned with various star and heart decorations. The character is initially seen in front of a large, colorful gift box with a bow. As the video progresses, the character moves away from the box and ascends into the air, leaving the box behind. The background remains static, filled with stars and hearts, creating a playful and festive atmosphere. The main subject is a pink, round character with a simple, happy face and a bow on its head. Initially, the character is positioned in front of a large gift box with a star pattern and a pink and white bow. The character then moves upwards, leaving the box behind. The character's movement is smooth and upward, with a moderate speed. The background remains static throughout the video, with no changes or movements. The camera view is fixed, capturing the character and the background in a steady, centered frame."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=3.902732842205322e-18
[OmniTrainingModule] training_step -> loss: 3.902732842205322e-18
Epoch 0:   0%|          | 52/224174 [06:54<495:57:01,  0.13it/s, v_num=42, train_loss_step=1.92e-27]Epoch 0:   0%|          | 52/224174 [06:54<495:57:29,  0.13it/s, v_num=42, train_loss_step=3.9e-18] [OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 52, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[Timer] VAE: 2.441 
[Check] audio_embeddings nan: False, inf: False, shape: torch.Size([1, 25, 10752])
[Timer] Wav2Vec: 0.022 
[Timer] : 0.000 
[Timer] forward_preprocess : 2.464 
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=False, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([980.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['A person is seated at a desk with various figurines and toys around them, engaging in a conversation or presentation.The individual is wearing a black t-shirt with a graphic design on the front. They have short, dark hair and are gesturing with their hands while speaking. The figurines and toys around them include a variety of characters from different franchises, such as Star Wars, Marvel, and DC Comics. The figurines are arranged on the desk, with some standing upright and others placed on the surface.The setting appears to be a room dedicated to collecting or displaying these figurines and toys. The desk is black, and the background is filled with shelves stocked with numerous books and other collectibles. There is a prominent display of figurines and toys, suggesting a space dedicated to hobbies or interests related to these items.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=False, inf=False, value=0.0
[OmniTrainingModule] training_step -> loss: 0.0
Epoch 0:   0%|          | 53/224174 [06:59<492:57:04,  0.13it/s, v_num=42, train_loss_step=3.9e-18]Epoch 0:   0%|          | 53/224174 [06:59<492:57:32,  0.13it/s, v_num=42, train_loss_step=0.000]  [OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 256 to 281
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 44 to 69
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 86 to 111
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 211 to 236
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 264 to 289
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 116 to 141
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 30 to 55
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 135 to 160
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 89 to 114
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 59 to 84
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 93 to 118
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 134 to 159
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 230 to 255
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 581 to 606
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 261 to 286
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 61 to 86
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 146 to 171
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 223 to 248
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 119 to 144
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 156 to 181
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 151 to 176
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 40 to 65
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 197 to 222
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 446 to 471
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 31 to 56
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 44 to 69
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 30 to 55
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 40 to 65
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 217 to 242
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 81 to 106
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 61 to 86
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 20 to 45
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 51 to 76
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 134 to 159
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 12 to 37
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 57 to 82
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 22 to 47
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 38 to 63
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 50 to 75
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 215 to 240
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 130 to 155
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 80 to 105
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 59 to 84
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 2 to 27
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 105 to 130
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 63 to 88
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 58 to 83
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 405 to 430
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 125 to 150
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 405 to 430
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 124 to 149
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 94 to 119
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 113 to 138
