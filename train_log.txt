| Reloading config from: pretrained_models/OmniAvatar-1.3B/config.json
{'config': 'configs/train_1.3B.yaml', 'exp_path': 'pretrained_models/OmniAvatar-1.3B', 'input_file': None, 'debug': True, 'infer': False, 'hparams': '', 'dtype': 'fp16', 'text_encoder_path': 'pretrained_models/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth', 'image_encoder_path': 'None', 'dit_path': 'pretrained_models/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors', 'vae_path': 'pretrained_models/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth', 'wav2vec_path': 'pretrained_models/wav2vec2-base-960h', 'num_persistent_param_in_dit': None, 'reload_cfg': True, 'sp_size': 1, 'seed': 42, 'image_sizes_720': [[400, 720], [720, 720], [720, 400]], 'image_sizes_1280': [[720, 720], [528, 960], [960, 528], [720, 1280], [1280, 720]], 'max_hw': 720, 'max_tokens': 30000, 'seq_len': 200, 'overlap_frame': 13, 'guidance_scale': 4.5, 'audio_scale': None, 'num_steps': 50, 'fps': 20, 'sample_rate': 16000, 'negative_prompt': 'Vivid color tones, background/camera moving quickly, screen switching, subtitles and special effects, mutation, overexposed, static, blurred details, subtitles, style, work, painting, image, still, overall grayish, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn face, deformed, disfigured, malformed limbs, fingers merging, motionless image, chaotic background, three legs, crowded background with many people, walking backward', 'silence_duration_s': 0.3, 'use_fsdp': False, 'tea_cache_l1_thresh': 0, 'dataset_base_path': '/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1', 'name': 'train_1.3B', 'savedir': '/mnt/hdd2/huanglingyu/vgg/OmniAvatar/outputs', 'batch_size': 1, 'nodes': 1, 'devices': 1, 'num_train_epochs': 1, 'mode': 'train', 'checkpoint_path': '', 'lr': '1e-4', 'max_frames': 120, 'debug_data_len': 5, 'rank': 0, 'world_size': 1, 'local_rank': 0, 'device': 'cuda:0', 'num_nodes': 1, 'i2v': True, 'use_audio': True, 'random_prefix_frames': True, 'model_config': {'in_dim': 33}, 'lora_target_modules': 'q,k,v,o,ffn.0,ffn.2', 'init_lora_weights': 'kaiming', 'lora_rank': 128, 'lora_alpha': 64.0, 'use_gradient_checkpointing': True, 'use_gradient_checkpointing_offload': False, 'train_architecture': 'lora'}
[2025-08-13 21:25:37,770] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-13 21:25:38,938] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[train_pl.py]-main-] config: {'dtype': 'fp16', 'text_encoder_path': 'pretrained_models/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth', 'image_encoder_path': 'None', 'dit_path': 'pretrained_models/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors', 'vae_path': 'pretrained_models/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth', 'wav2vec_path': 'pretrained_models/wav2vec2-base-960h', 'exp_path': 'pretrained_models/OmniAvatar-1.3B', 'num_persistent_param_in_dit': None, 'reload_cfg': True, 'sp_size': 1, 'seed': 42, 'image_sizes_720': [[400, 720], [720, 720], [720, 400]], 'image_sizes_1280': [[720, 720], [528, 960], [960, 528], [720, 1280], [1280, 720]], 'max_hw': 720, 'max_tokens': 30000, 'seq_len': 200, 'overlap_frame': 13, 'guidance_scale': 4.5, 'audio_scale': None, 'num_steps': 50, 'fps': 20, 'sample_rate': 16000, 'negative_prompt': 'Vivid color tones, background/camera moving quickly, screen switching, subtitles and special effects, mutation, overexposed, static, blurred details, subtitles, style, work, painting, image, still, overall grayish, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn face, deformed, disfigured, malformed limbs, fingers merging, motionless image, chaotic background, three legs, crowded background with many people, walking backward', 'silence_duration_s': 0.3, 'use_fsdp': False, 'tea_cache_l1_thresh': 0, 'dataset_base_path': '/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1', 'name': 'train_1.3B', 'savedir': '/mnt/hdd2/huanglingyu/vgg/OmniAvatar/outputs', 'batch_size': 1, 'nodes': 1, 'devices': 1, 'num_train_epochs': 1, 'mode': 'train', 'checkpoint_path': '', 'lr': 0.0001, 'max_frames': 120, 'debug': True, 'debug_data_len': 5}
[OmniTrainingModule] __init__
Loading models from: ['pretrained_models/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors']
    model_name: wan_video_dit model_class: WanModel
        This model is initialized with extra kwargs: {'has_image_input': False, 'patch_size': [1, 2, 2], 'in_dim': 33, 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'text_dim': 4096, 'out_dim': 16, 'num_heads': 12, 'num_layers': 30, 'eps': 1e-06}
Using WanModel with dim=1536, in_dim=33, ffn_dim=8960, out_dim=16, text_dim=4096, freq_dim=256, eps=1e-06, patch_size=[1, 2, 2], num_heads=12, num_layers=30, has_image_input=False, audio_hidden_size=32
[AudioPack] in_channels: 10752, t, h, w: 4, 1, 1
[AudioPack] patch_size: (4, 1, 1)
[AudioPack] proj: Linear(in_features=43008, out_features=32, bias=True)
[AudioPack] norm_out: LayerNorm((32,), eps=1e-05, elementwise_affine=True)

[WanModel] patch_embedding: Conv3d(33, 1536, kernel_size=(1, 2, 2), stride=(1, 2, 2))
[WanModel] text_embedding: Sequential(
  (0): Linear(in_features=4096, out_features=1536, bias=True)
  (1): GELU(approximate='tanh')
  (2): Linear(in_features=1536, out_features=1536, bias=True)
)
[WanModel] time_embedding: Sequential(
  (0): Linear(in_features=256, out_features=1536, bias=True)
  (1): SiLU()
  (2): Linear(in_features=1536, out_features=1536, bias=True)
)
[WanModel] time_projection: Sequential(
  (0): SiLU()
  (1): Linear(in_features=1536, out_features=9216, bias=True)
)
[WanModel] blocks (DiTBlock):
  Block 0: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 1: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 2: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 3: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 4: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 5: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 6: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 7: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 8: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 9: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 10: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 11: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 12: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 13: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 14: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 15: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 16: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 17: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 18: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 19: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 20: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 21: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 22: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 23: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 24: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 25: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 26: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 27: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 28: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 29: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
[WanModel] head: Head(
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (head): Linear(in_features=1536, out_features=64, bias=True)
)
[WanModel] RoPE freqs shape: [torch.Size([1024, 22]), torch.Size([1024, 21]), torch.Size([1024, 21])]
[WanModel] audio_proj: AudioPack(
  (proj): Linear(in_features=43008, out_features=32, bias=True)
  (norm_out): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
)
[WanModel] audio_cond_projs:
  Audio Cond Proj 0: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 1: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 2: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 3: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 4: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 5: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 6: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 7: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 8: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 9: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 10: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 11: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 12: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 13: Linear(in_features=32, out_features=1536, bias=True)
[Truncate] patch_embedding.weight: ckpt torch.Size([1536, 16, 1, 2, 2]) -> model torch.Size([1536, 33, 1, 2, 2])
    The following models are loaded: ['wan_video_dit'].
Loading models from: pretrained_models/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth
    model_name: wan_video_text_encoder model_class: WanTextEncoder
[WanTextEncoder] token_embedding: Embedding(256384, 4096)
[WanTextEncoder] dropout: Dropout(p=0.1, inplace=False)
[WanTextEncoder] blocks (T5SelfAttention):
  Block 0: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 1: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 2: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 3: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 4: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 5: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 6: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 7: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 8: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 9: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 10: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 11: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 12: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 13: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 14: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 15: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 16: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 17: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 18: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 19: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 20: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 21: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 22: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 23: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
[WanTextEncoder] norm: T5LayerNorm()
    The following models are loaded: ['wan_video_text_encoder'].
Loading models from: pretrained_models/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth
    model_name: wan_video_vae model_class: WanVideoVAE

[VideoVAE_] encoder: Encoder3d(
  (conv1): CausalConv3d(3, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
  (downsamples): Sequential(
    (0): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (1): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (2): Resample(
      (resample): Sequential(
        (0): ZeroPad2d((0, 1, 0, 1))
        (1): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2))
      )
    )
    (3): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(96, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): CausalConv3d(96, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    )
    (4): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (5): Resample(
      (resample): Sequential(
        (0): ZeroPad2d((0, 1, 0, 1))
        (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2))
      )
      (time_conv): CausalConv3d(192, 192, kernel_size=(3, 1, 1), stride=(2, 1, 1))
    )
    (6): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(192, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): CausalConv3d(192, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    )
    (7): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (8): Resample(
      (resample): Sequential(
        (0): ZeroPad2d((0, 1, 0, 1))
        (1): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2))
      )
      (time_conv): CausalConv3d(384, 384, kernel_size=(3, 1, 1), stride=(2, 1, 1))
    )
    (9): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (10): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
  )
  (middle): Sequential(
    (0): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (1): AttentionBlock(
      (norm): RMS_norm()
      (to_qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
      (proj): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
    )
    (2): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
  )
  (head): Sequential(
    (0): RMS_norm()
    (1): SiLU()
    (2): CausalConv3d(384, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))
  )
)
[VideoVAE_] conv1: CausalConv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))
[VideoVAE_] conv2: CausalConv3d(16, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))
[VideoVAE_] decoder: Decoder3d(
  (conv1): CausalConv3d(16, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
  (middle): Sequential(
    (0): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (1): AttentionBlock(
      (norm): RMS_norm()
      (to_qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
      (proj): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
    )
    (2): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
  )
  (upsamples): Sequential(
    (0): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (1): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (2): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (3): Resample(
      (resample): Sequential(
        (0): Upsample(scale_factor=(2.0, 2.0), mode='nearest-exact')
        (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (time_conv): CausalConv3d(384, 768, kernel_size=(3, 1, 1), stride=(1, 1, 1))
    )
    (4): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(192, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): CausalConv3d(192, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    )
    (5): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (6): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (7): Resample(
      (resample): Sequential(
        (0): Upsample(scale_factor=(2.0, 2.0), mode='nearest-exact')
        (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (time_conv): CausalConv3d(384, 768, kernel_size=(3, 1, 1), stride=(1, 1, 1))
    )
    (8): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (9): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (10): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (11): Resample(
      (resample): Sequential(
        (0): Upsample(scale_factor=(2.0, 2.0), mode='nearest-exact')
        (1): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (12): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (13): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (14): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
  )
  (head): Sequential(
    (0): RMS_norm()
    (1): SiLU()
    (2): CausalConv3d(96, 3, kernel_size=(3, 3, 3), stride=(1, 1, 1))
  )
)
    The following models are loaded: ['wan_video_vae'].
Using wan_video_text_encoder from pretrained_models/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth.
Using wan_video_dit from ['pretrained_models/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors'].
Using wan_video_vae from pretrained_models/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth.
No wan_video_image_encoder models available.
[OmniTrainingModule load_model] -> Use LoRA: lora rank: 128, lora alpha: 64.0, pretrained_lora_path: pretrained_models/OmniAvatar-1.3B/pytorch_model.pt
[OmniTrainingModule add_lora_to_model] -> lora_config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'o', 'ffn.0', 'k', 'ffn.2', 'v', 'q'}, exclude_modules=None, lora_alpha=64.0, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
[OmniTrainingModule add_lora_to_model] -> 634 parameters are loaded from pretrained_models/OmniAvatar-1.3B/pytorch_model.pt. 0 parameters are unexpected.
[OmniTrainingModule __init__]: Model loaded on cpu, dtype: torch.float32
===================[train_pl.py]-main-] model summary================================
[OmniTrainingModule] - name: pipe.text_encoder.token_embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.norm.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.patch_embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.patch_embedding.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.text_embedding.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.text_embedding.0.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.text_embedding.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.text_embedding.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.time_embedding.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.time_embedding.0.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.time_embedding.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.time_embedding.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.time_projection.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.time_projection.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.head.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.head.head.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.head.head.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_proj.proj.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_proj.proj.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_proj.norm_out.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_proj.norm_out.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.0.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.4.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.4.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.5.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.5.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.7.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.7.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.8.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.8.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.9.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.9.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.10.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.10.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.11.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.11.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.12.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.12.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.13.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.13.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.conv1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.conv1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.0.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.0.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.0.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.0.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.0.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.0.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.1.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.1.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.1.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.1.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.1.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.1.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.2.resample.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.2.resample.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.shortcut.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.shortcut.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.4.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.4.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.4.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.4.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.4.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.4.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.5.resample.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.5.resample.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.5.time_conv.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.5.time_conv.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.shortcut.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.shortcut.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.7.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.7.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.7.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.7.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.7.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.7.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.8.resample.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.8.resample.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.8.time_conv.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.8.time_conv.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.9.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.9.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.9.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.9.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.9.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.9.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.10.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.10.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.10.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.10.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.10.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.10.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.0.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.0.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.0.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.0.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.0.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.0.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.1.norm.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.1.to_qkv.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.1.to_qkv.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.1.proj.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.1.proj.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.2.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.2.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.2.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.2.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.2.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.2.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.head.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.head.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.head.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.conv1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.conv1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.conv2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.conv2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.conv1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.conv1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.0.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.0.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.0.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.0.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.0.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.0.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.1.norm.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.1.to_qkv.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.1.to_qkv.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.1.proj.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.1.proj.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.2.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.2.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.2.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.2.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.2.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.2.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.0.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.0.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.0.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.0.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.0.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.0.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.1.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.1.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.1.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.1.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.1.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.1.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.2.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.2.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.2.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.2.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.2.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.2.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.3.resample.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.3.resample.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.3.time_conv.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.3.time_conv.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.shortcut.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.shortcut.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.5.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.5.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.5.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.5.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.5.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.5.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.6.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.6.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.6.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.6.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.6.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.6.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.7.resample.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.7.resample.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.7.time_conv.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.7.time_conv.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.8.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.8.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.8.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.8.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.8.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.8.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.9.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.9.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.9.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.9.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.9.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.9.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.10.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.10.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.10.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.10.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.10.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.10.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.11.resample.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.11.resample.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.12.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.12.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.12.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.12.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.12.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.12.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.13.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.13.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.13.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.13.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.13.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.13.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.14.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.14.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.14.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.14.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.14.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.14.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.head.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.head.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.head.2.bias, requires_grad: False
[OmniTrainingModule] - name: audio_encoder.masked_spec_embed, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.0.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.0.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.0.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.1.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.2.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.3.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.4.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.5.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.6.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_projection.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_projection.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_projection.projection.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_projection.projection.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.pos_conv_embed.conv.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.pos_conv_embed.conv.parametrizations.weight.original0, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.pos_conv_embed.conv.parametrizations.weight.original1, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.final_layer_norm.bias, requires_grad: True
===================================================================================
[OmniTrainingModule]: start training with config: {'dtype': 'fp16', 'text_encoder_path': 'pretrained_models/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth', 'image_encoder_path': 'None', 'dit_path': 'pretrained_models/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors', 'vae_path': 'pretrained_models/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth', 'wav2vec_path': 'pretrained_models/wav2vec2-base-960h', 'exp_path': 'pretrained_models/OmniAvatar-1.3B', 'num_persistent_param_in_dit': None, 'reload_cfg': True, 'sp_size': 1, 'seed': 42, 'image_sizes_720': [[400, 720], [720, 720], [720, 400]], 'image_sizes_1280': [[720, 720], [528, 960], [960, 528], [720, 1280], [1280, 720]], 'max_hw': 720, 'max_tokens': 30000, 'seq_len': 200, 'overlap_frame': 13, 'guidance_scale': 4.5, 'audio_scale': None, 'num_steps': 50, 'fps': 20, 'sample_rate': 16000, 'negative_prompt': 'Vivid color tones, background/camera moving quickly, screen switching, subtitles and special effects, mutation, overexposed, static, blurred details, subtitles, style, work, painting, image, still, overall grayish, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn face, deformed, disfigured, malformed limbs, fingers merging, motionless image, chaotic background, three legs, crowded background with many people, walking backward', 'silence_duration_s': 0.3, 'use_fsdp': False, 'tea_cache_l1_thresh': 0, 'dataset_base_path': '/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1', 'name': 'train_1.3B', 'savedir': '/mnt/hdd2/huanglingyu/vgg/OmniAvatar/outputs/train_1.3B', 'batch_size': 1, 'nodes': 1, 'devices': 1, 'num_train_epochs': 1, 'mode': 'train', 'checkpoint_path': '', 'lr': 0.0001, 'max_frames': 120, 'debug': True, 'debug_data_len': 5}
[OmniTrainingModule] configure_optimizers
[OmniTrainingModule] on_fit_start -> device: cuda:0, param device: cuda:0
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/5 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/5 [00:00<?, ?it/s] [OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path']), batch_idx: 0, batch_size: 1
[OmniTrainingModule forward_preprocess] -> device: cuda:0
[OmniTrainingModule forward_preprocess] -> Loading video: /mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/wHRxsU26o8U_173/video.mp4
[OmniTrainingModule forward_preprocess] -> Original video fps: 25.0, rounded fps: 25
[OmniTrainingModule forward_preprocess] -> Raw video shape: torch.Size([115, 360, 640, 3])
[OmniTrainingModule forward_preprocess] -> Resized video shape: torch.Size([3, 115, 400, 640])
[OmniTrainingModule forward_preprocess] -> Loading audio: /mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/wHRxsU26o8U_173/audio.wav
[OmniTrainingModule forward_preprocess] -> Raw audio shape: (74304,), sr: 16000
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 51 to 76
[OmniTrainingModule forward_preprocess] -> L: 25, T: 7
[OmniTrainingModule forward_preprocess] -> Final video_clip shape: torch.Size([3, 25, 400, 640])
[OmniTrainingModule forward_preprocess] -> Final audio_clip shape: (16000,)
[OmniTrainingModule forward_preprocess] -> All videos stacked shape: torch.Size([1, 3, 25, 400, 640]), videos device: cuda:0
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[OmniTrainingModule forward_preprocess] -> Audio embedding 0 shape: torch.Size([1, 25, 10752])
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([886.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["a sequence of animated frames showing a character with large, expressive eyes and a simplistic design. The character appears to be a cartoonish creature with a pink and white color scheme. The character's eyes are large and black with white pupils, and it has a simple, friendly face. The character is initially seen peeking over a wooden fence, then it moves down a slide, and finally, it reaches the bottom of the slide. The background is a plain, light purple color, providing a clean and minimalistic setting for the character. The scene is set indoors, with no visible windows or external elements. The wooden fence at the top of the scene is the only object that adds texture and depth to the background. The main subject is a cartoonish character with large, expressive eyes and a simplistic design. The character has a pink and white color scheme, with a friendly and approachable appearance. The character's movements are smooth and deliberate. It starts by peeking over the wooden fence, then it moves down a slide, and finally, it reaches the bottom of the slide. The movement is slow and steady, with the character's head and body moving in a downward direction. The background remains static throughout the video, with no changes or movements."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
Epoch 0:  20%|        | 1/5 [00:16<01:07,  0.06it/s]Epoch 0:  20%|        | 1/5 [00:16<01:07,  0.06it/s, v_num=12][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path']), batch_idx: 1, batch_size: 1
[OmniTrainingModule forward_preprocess] -> device: cuda:0
[OmniTrainingModule forward_preprocess] -> Loading video: /mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/InGaVPreV7Q_16/video.mp4
[OmniTrainingModule forward_preprocess] -> Original video fps: 25.0, rounded fps: 25
[OmniTrainingModule forward_preprocess] -> Raw video shape: torch.Size([197, 360, 640, 3])
[OmniTrainingModule forward_preprocess] -> Resized video shape: torch.Size([3, 197, 400, 640])
[OmniTrainingModule forward_preprocess] -> Loading audio: /mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/InGaVPreV7Q_16/audio.wav
[OmniTrainingModule forward_preprocess] -> Raw audio shape: (126688,), sr: 16000
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 52 to 77
[OmniTrainingModule forward_preprocess] -> L: 25, T: 7
[OmniTrainingModule forward_preprocess] -> Final video_clip shape: torch.Size([3, 25, 400, 640])
[OmniTrainingModule forward_preprocess] -> Final audio_clip shape: (16000,)
[OmniTrainingModule forward_preprocess] -> All videos stacked shape: torch.Size([1, 3, 25, 400, 640]), videos device: cuda:0
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[OmniTrainingModule forward_preprocess] -> Audio embedding 0 shape: torch.Size([1, 25, 10752])
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([720.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["a cartoonish red character with large, expressive eyes and glasses, peeking out from behind a clock. The character is positioned on the left side of the clock face, which is white with black numbers and hands. The background is a solid blue, providing a contrasting backdrop to the vibrant red character. The character's eyes move slightly as it looks around, creating a playful and engaging scene. The main subject is a red character with large, round eyes and glasses. The character is positioned on the left side of the clock face, partially obscured by the clock's edge. The character's eyes move slightly, giving the impression of curiosity or surprise. The character remains stationary in terms of its position on the clock, but its eyes move slightly, indicating subtle movements. The background is a solid blue color, providing a clean and simple backdrop that contrasts with the vibrant red character. There are no additional objects or elements in the background, keeping the focus on the character and the clock. The camera is stationary, focusing on a close-up view of the clock and the character. The angle is slightly tilted, providing a clear view of the character's face and the clock's face."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
Epoch 0:  40%|      | 2/5 [00:32<00:48,  0.06it/s, v_num=12]Epoch 0:  40%|      | 2/5 [00:32<00:48,  0.06it/s, v_num=12][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path']), batch_idx: 2, batch_size: 1
[OmniTrainingModule forward_preprocess] -> device: cuda:0
[OmniTrainingModule forward_preprocess] -> Loading video: /mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/9qO7s-kXfEk_39/video.mp4
[OmniTrainingModule forward_preprocess] -> Original video fps: 24.0, rounded fps: 24
[OmniTrainingModule forward_preprocess] -> Raw video shape: torch.Size([729, 360, 640, 3])
[OmniTrainingModule forward_preprocess] -> Resized video shape: torch.Size([3, 729, 400, 640])
[OmniTrainingModule forward_preprocess] -> Loading audio: /mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/9qO7s-kXfEk_39/audio.wav
[OmniTrainingModule forward_preprocess] -> Raw audio shape: (485947,), sr: 16000
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 681 to 706
[OmniTrainingModule forward_preprocess] -> L: 25, T: 7
[OmniTrainingModule forward_preprocess] -> Final video_clip shape: torch.Size([3, 25, 400, 640])
[OmniTrainingModule forward_preprocess] -> Final audio_clip shape: (16650,)
[OmniTrainingModule forward_preprocess] -> All videos stacked shape: torch.Size([1, 3, 25, 400, 640]), videos device: cuda:0
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[OmniTrainingModule forward_preprocess] -> Audio embedding 0 shape: torch.Size([1, 25, 10752])
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([660.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["two animated characters, a green-shirted character and a blue-shirted character, standing next to each other. The green-shirted character is holding a bottle of red liquid, which appears to be a beverage, while the blue-shirted character is holding a bottle of green liquid, also a beverage. The green-shirted character is initially looking downward with a sad expression, while the blue-shirted character is looking at the green-shirted character with a neutral expression. The scene takes place in a dimly lit environment with a staircase in the background. The green-shirted character's expression changes from sad to a more neutral look as the video progresses. The blue-shirted character remains relatively still, holding the green bottle. The background consists of a dimly lit environment with a staircase leading upwards. The staircase has a wooden texture and is partially visible. The background is mostly dark, with a red-colored wall or curtain visible in the distance, adding a sense of depth to the scene. The main subjects are two animated characters. The green-shirted character is holding a bottle of red liquid, which appears to be a beverage, and is initially looking downward with a sad expression. The blue-shirted character is holding a bottle of green liquid, also a beverage, and is looking at the green-shirted character with a neutral expression. The green-shirted character's position remains relatively static, while the blue-shirted character's position shifts slightly as they hold the green bottle. The camera is stationary, providing a medium shot of the two characters from the waist up, capturing their expressions and interactions clearly."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
Epoch 0:  60%|    | 3/5 [01:19<00:52,  0.04it/s, v_num=12]Epoch 0:  60%|    | 3/5 [01:19<00:52,  0.04it/s, v_num=12][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path']), batch_idx: 3, batch_size: 1
[OmniTrainingModule forward_preprocess] -> device: cuda:0
[OmniTrainingModule forward_preprocess] -> Loading video: /mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/xUjy1uCHTyU_15/video.mp4
[OmniTrainingModule forward_preprocess] -> Original video fps: 30.0, rounded fps: 30
[OmniTrainingModule forward_preprocess] -> Raw video shape: torch.Size([199, 360, 640, 3])
[OmniTrainingModule forward_preprocess] -> Resized video shape: torch.Size([3, 199, 400, 640])
[OmniTrainingModule forward_preprocess] -> Loading audio: /mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/xUjy1uCHTyU_15/audio.wav
[OmniTrainingModule forward_preprocess] -> Raw audio shape: (106626,), sr: 16000
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 131 to 156
[OmniTrainingModule forward_preprocess] -> L: 25, T: 7
[OmniTrainingModule forward_preprocess] -> Final video_clip shape: torch.Size([3, 25, 400, 640])
[OmniTrainingModule forward_preprocess] -> Final audio_clip shape: (13325,)
[OmniTrainingModule forward_preprocess] -> All videos stacked shape: torch.Size([1, 3, 25, 400, 640]), videos device: cuda:0
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[OmniTrainingModule forward_preprocess] -> Audio embedding 0 shape: torch.Size([1, 25, 10752])
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([949.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["two animated characters, a woman and a man, dancing on a beach at sunset. The woman wears a red dress with a flowing skirt and black boots, while the man wears a white shirt with a black vest and white pants. They dance energetically, moving in sync with each other, and their movements are fluid and expressive. The background features a vibrant orange and pink sunset over a calm ocean, with palm trees visible in the distance. The main subjects are the two animated characters. The woman has pink hair with a red headband, wears a red dress with a flowing skirt, and black boots. The man has white hair, wears a white shirt with a black vest and white pants. They are positioned side by side, facing each other, and their movements are synchronized as they dance. The woman's dress flows with her movements, and her hair moves with the breeze. The man's movements are equally expressive, with his arms and legs moving in rhythm. The camera remains stationary, capturing the entire scene from a fixed viewpoint, focusing on the two characters as they dance."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
Epoch 0:  80%|  | 4/5 [01:35<00:23,  0.04it/s, v_num=12]Epoch 0:  80%|  | 4/5 [01:35<00:23,  0.04it/s, v_num=12][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path']), batch_idx: 4, batch_size: 1
[OmniTrainingModule forward_preprocess] -> device: cuda:0
[OmniTrainingModule forward_preprocess] -> Loading video: /mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/V8buho2fX8g_33/video.mp4
[OmniTrainingModule forward_preprocess] -> Original video fps: 23.976023976023978, rounded fps: 24
[OmniTrainingModule forward_preprocess] -> Raw video shape: torch.Size([1569, 360, 640, 3])
[OmniTrainingModule forward_preprocess] -> Resized video shape: torch.Size([3, 1569, 400, 640])
[OmniTrainingModule forward_preprocess] -> Loading audio: /mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/V8buho2fX8g_33/audio.wav
[OmniTrainingModule forward_preprocess] -> Raw audio shape: (1046941,), sr: 16000
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 205 to 230
[OmniTrainingModule forward_preprocess] -> L: 25, T: 7
[OmniTrainingModule forward_preprocess] -> Final video_clip shape: torch.Size([3, 25, 400, 640])
[OmniTrainingModule forward_preprocess] -> Final audio_clip shape: (16650,)
[OmniTrainingModule forward_preprocess] -> All videos stacked shape: torch.Size([1, 3, 25, 400, 640]), videos device: cuda:0
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[OmniTrainingModule forward_preprocess] -> Latent shape: torch.Size([1, 16, 7, 50, 80])
[OmniTrainingModule forward_preprocess] -> Audio embedding 0 shape: torch.Size([1, 25, 10752])
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([921.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["a cartoon character, a man with a mustache and a blue suit, sitting at a desk in front of a computer screen. He appears to be working or presenting, with various expressions and gestures indicating different emotions and actions. The background includes a large screen displaying a grid of blue and green lines, suggesting a digital or scientific theme. The character's movements are subtle and include shifting his gaze, changing his facial expressions, and occasionally gesturing with his hands. The background remains static, with the grid of lines on the screen providing a consistent visual element throughout the video. The movements are slow and deliberate, emphasizing the character's thoughtful and focused demeanor. The main subject is a cartoon man with a mustache, wearing a blue suit and a red tie. He is positioned at a desk, facing the camera, and interacts with a computer screen in front of him. His expressions change from neutral to concerned, and he occasionally gestures with his hands. The character remains seated throughout the video, with his movements limited to his facial expressions and hand gestures. The camera is stationary, providing a consistent frontal view of the character at the desk. The view is at eye level, focusing on the character's upper body and face."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
Epoch 0: 100%|| 5/5 [03:11<00:00,  0.03it/s, v_num=12]Epoch 0: 100%|| 5/5 [03:11<00:00,  0.03it/s, v_num=12]Epoch 0: 100%|| 5/5 [03:11<00:00,  0.03it/s, v_num=12]Epoch 0: 100%|| 5/5 [03:11<00:00,  0.03it/s, v_num=12]
