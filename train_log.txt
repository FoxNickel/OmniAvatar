| Reloading config from: pretrained_models/OmniAvatar-1.3B/config.json
{'config': 'configs/train_1.3B.yaml', 'exp_path': 'pretrained_models/OmniAvatar-1.3B', 'input_file': None, 'debug': True, 'infer': False, 'hparams': '', 'dtype': '16', 'text_encoder_path': 'pretrained_models/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth', 'image_encoder_path': 'None', 'dit_path': 'pretrained_models/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors', 'vae_path': 'pretrained_models/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth', 'wav2vec_path': 'pretrained_models/wav2vec2-base-960h', 'num_persistent_param_in_dit': None, 'reload_cfg': True, 'sp_size': 1, 'seed': 42, 'image_sizes_720': [[400, 720], [720, 720], [720, 400]], 'image_sizes_1280': [[720, 720], [528, 960], [960, 528], [720, 1280], [1280, 720]], 'max_hw': 720, 'max_tokens': 30000, 'seq_len': 200, 'overlap_frame': 13, 'guidance_scale': 4.5, 'audio_scale': None, 'num_steps': 50, 'fps': 20, 'sample_rate': 16000, 'negative_prompt': 'Vivid color tones, background/camera moving quickly, screen switching, subtitles and special effects, mutation, overexposed, static, blurred details, subtitles, style, work, painting, image, still, overall grayish, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn face, deformed, disfigured, malformed limbs, fingers merging, motionless image, chaotic background, three legs, crowded background with many people, walking backward', 'silence_duration_s': 0.3, 'use_fsdp': False, 'tea_cache_l1_thresh': 0, 'dataset_base_path': '/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1', 'name': 'train_1.3B', 'savedir': '/mnt/hdd2/huanglingyu/vgg/OmniAvatar/outputs', 'batch_size': 1, 'nodes': 1, 'devices': 1, 'num_train_epochs': 1, 'mode': 'train', 'checkpoint_path': '', 'lr': '1e-4', 'max_frames': 120, 'debug_data_len': 100, 'rank': 0, 'world_size': 1, 'local_rank': 0, 'device': 'cuda:0', 'num_nodes': 1, 'i2v': True, 'use_audio': True, 'random_prefix_frames': True, 'model_config': {'in_dim': 33}, 'lora_target_modules': 'q,k,v,o,ffn.0,ffn.2', 'init_lora_weights': 'kaiming', 'lora_rank': 128, 'lora_alpha': 64.0, 'use_gradient_checkpointing': True, 'use_gradient_checkpointing_offload': False, 'train_architecture': 'lora'}
[2025-08-14 22:51:52,438] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-14 22:51:53,617] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[train_pl.py]-main-] config: {'dtype': '16', 'text_encoder_path': 'pretrained_models/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth', 'image_encoder_path': 'None', 'dit_path': 'pretrained_models/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors', 'vae_path': 'pretrained_models/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth', 'wav2vec_path': 'pretrained_models/wav2vec2-base-960h', 'exp_path': 'pretrained_models/OmniAvatar-1.3B', 'num_persistent_param_in_dit': None, 'reload_cfg': True, 'sp_size': 1, 'seed': 42, 'image_sizes_720': [[400, 720], [720, 720], [720, 400]], 'image_sizes_1280': [[720, 720], [528, 960], [960, 528], [720, 1280], [1280, 720]], 'max_hw': 720, 'max_tokens': 30000, 'seq_len': 200, 'overlap_frame': 13, 'guidance_scale': 4.5, 'audio_scale': None, 'num_steps': 50, 'fps': 20, 'sample_rate': 16000, 'negative_prompt': 'Vivid color tones, background/camera moving quickly, screen switching, subtitles and special effects, mutation, overexposed, static, blurred details, subtitles, style, work, painting, image, still, overall grayish, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn face, deformed, disfigured, malformed limbs, fingers merging, motionless image, chaotic background, three legs, crowded background with many people, walking backward', 'silence_duration_s': 0.3, 'use_fsdp': False, 'tea_cache_l1_thresh': 0, 'dataset_base_path': '/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1', 'name': 'train_1.3B', 'savedir': '/mnt/hdd2/huanglingyu/vgg/OmniAvatar/outputs', 'batch_size': 1, 'nodes': 1, 'devices': 1, 'num_train_epochs': 1, 'mode': 'train', 'checkpoint_path': '', 'lr': 0.0001, 'max_frames': 120, 'debug': True, 'debug_data_len': 100}
[OmniTrainingModule] __init__
Loading models from: ['pretrained_models/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors']
    model_name: wan_video_dit model_class: WanModel
        This model is initialized with extra kwargs: {'has_image_input': False, 'patch_size': [1, 2, 2], 'in_dim': 33, 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'text_dim': 4096, 'out_dim': 16, 'num_heads': 12, 'num_layers': 30, 'eps': 1e-06}
Using WanModel with dim=1536, in_dim=33, ffn_dim=8960, out_dim=16, text_dim=4096, freq_dim=256, eps=1e-06, patch_size=[1, 2, 2], num_heads=12, num_layers=30, has_image_input=False, audio_hidden_size=32
[AudioPack] in_channels: 10752, t, h, w: 4, 1, 1
[AudioPack] patch_size: (4, 1, 1)
[AudioPack] proj: Linear(in_features=43008, out_features=32, bias=True)
[AudioPack] norm_out: LayerNorm((32,), eps=1e-05, elementwise_affine=True)

[WanModel] patch_embedding: Conv3d(33, 1536, kernel_size=(1, 2, 2), stride=(1, 2, 2))
[WanModel] text_embedding: Sequential(
  (0): Linear(in_features=4096, out_features=1536, bias=True)
  (1): GELU(approximate='tanh')
  (2): Linear(in_features=1536, out_features=1536, bias=True)
)
[WanModel] time_embedding: Sequential(
  (0): Linear(in_features=256, out_features=1536, bias=True)
  (1): SiLU()
  (2): Linear(in_features=1536, out_features=1536, bias=True)
)
[WanModel] time_projection: Sequential(
  (0): SiLU()
  (1): Linear(in_features=1536, out_features=9216, bias=True)
)
[WanModel] blocks (DiTBlock):
  Block 0: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 1: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 2: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 3: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 4: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 5: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 6: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 7: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 8: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 9: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 10: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 11: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 12: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 13: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 14: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 15: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 16: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 17: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 18: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 19: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 20: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 21: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 22: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 23: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 24: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 25: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 26: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 27: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 28: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
  Block 29: DiTBlock(
  (self_attn): SelfAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (cross_attn): CrossAttention(
    (q): Linear(in_features=1536, out_features=1536, bias=True)
    (k): Linear(in_features=1536, out_features=1536, bias=True)
    (v): Linear(in_features=1536, out_features=1536, bias=True)
    (o): Linear(in_features=1536, out_features=1536, bias=True)
    (norm_q): RMSNorm()
    (norm_k): RMSNorm()
    (attn): AttentionModule()
  )
  (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (norm3): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
  (ffn): Sequential(
    (0): Linear(in_features=1536, out_features=8960, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=8960, out_features=1536, bias=True)
  )
  (gate): GateModule()
)
[WanModel] head: Head(
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (head): Linear(in_features=1536, out_features=64, bias=True)
)
[WanModel] RoPE freqs shape: [torch.Size([1024, 22]), torch.Size([1024, 21]), torch.Size([1024, 21])]
[WanModel] audio_proj: AudioPack(
  (proj): Linear(in_features=43008, out_features=32, bias=True)
  (norm_out): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
)
[WanModel] audio_cond_projs:
  Audio Cond Proj 0: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 1: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 2: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 3: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 4: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 5: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 6: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 7: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 8: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 9: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 10: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 11: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 12: Linear(in_features=32, out_features=1536, bias=True)
  Audio Cond Proj 13: Linear(in_features=32, out_features=1536, bias=True)
[Truncate] patch_embedding.weight: ckpt torch.Size([1536, 16, 1, 2, 2]) -> model torch.Size([1536, 33, 1, 2, 2])
    The following models are loaded: ['wan_video_dit'].
Loading models from: pretrained_models/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth
    model_name: wan_video_text_encoder model_class: WanTextEncoder
[WanTextEncoder] token_embedding: Embedding(256384, 4096)
[WanTextEncoder] dropout: Dropout(p=0.1, inplace=False)
[WanTextEncoder] blocks (T5SelfAttention):
  Block 0: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 1: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 2: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 3: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 4: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 5: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 6: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 7: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 8: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 9: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 10: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 11: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 12: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 13: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 14: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 15: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 16: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 17: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 18: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 19: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 20: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 21: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 22: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
  Block 23: T5SelfAttention(
  (norm1): T5LayerNorm()
  (attn): T5Attention(
    (q): Linear(in_features=4096, out_features=4096, bias=False)
    (k): Linear(in_features=4096, out_features=4096, bias=False)
    (v): Linear(in_features=4096, out_features=4096, bias=False)
    (o): Linear(in_features=4096, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (norm2): T5LayerNorm()
  (ffn): T5FeedForward(
    (gate): Sequential(
      (0): Linear(in_features=4096, out_features=10240, bias=False)
      (1): GELU()
    )
    (fc1): Linear(in_features=4096, out_features=10240, bias=False)
    (fc2): Linear(in_features=10240, out_features=4096, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (pos_embedding): T5RelativeEmbedding(
    (embedding): Embedding(32, 64)
  )
)
[WanTextEncoder] norm: T5LayerNorm()
    The following models are loaded: ['wan_video_text_encoder'].
Loading models from: pretrained_models/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth
    model_name: wan_video_vae model_class: WanVideoVAE

[VideoVAE_] encoder: Encoder3d(
  (conv1): CausalConv3d(3, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
  (downsamples): Sequential(
    (0): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (1): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (2): Resample(
      (resample): Sequential(
        (0): ZeroPad2d((0, 1, 0, 1))
        (1): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2))
      )
    )
    (3): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(96, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): CausalConv3d(96, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    )
    (4): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (5): Resample(
      (resample): Sequential(
        (0): ZeroPad2d((0, 1, 0, 1))
        (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2))
      )
      (time_conv): CausalConv3d(192, 192, kernel_size=(3, 1, 1), stride=(2, 1, 1))
    )
    (6): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(192, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): CausalConv3d(192, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    )
    (7): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (8): Resample(
      (resample): Sequential(
        (0): ZeroPad2d((0, 1, 0, 1))
        (1): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2))
      )
      (time_conv): CausalConv3d(384, 384, kernel_size=(3, 1, 1), stride=(2, 1, 1))
    )
    (9): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (10): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
  )
  (middle): Sequential(
    (0): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (1): AttentionBlock(
      (norm): RMS_norm()
      (to_qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
      (proj): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
    )
    (2): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
  )
  (head): Sequential(
    (0): RMS_norm()
    (1): SiLU()
    (2): CausalConv3d(384, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))
  )
)
[VideoVAE_] conv1: CausalConv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))
[VideoVAE_] conv2: CausalConv3d(16, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))
[VideoVAE_] decoder: Decoder3d(
  (conv1): CausalConv3d(16, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
  (middle): Sequential(
    (0): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (1): AttentionBlock(
      (norm): RMS_norm()
      (to_qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
      (proj): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
    )
    (2): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
  )
  (upsamples): Sequential(
    (0): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (1): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (2): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (3): Resample(
      (resample): Sequential(
        (0): Upsample(scale_factor=(2.0, 2.0), mode='nearest-exact')
        (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (time_conv): CausalConv3d(384, 768, kernel_size=(3, 1, 1), stride=(1, 1, 1))
    )
    (4): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(192, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): CausalConv3d(192, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    )
    (5): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (6): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (7): Resample(
      (resample): Sequential(
        (0): Upsample(scale_factor=(2.0, 2.0), mode='nearest-exact')
        (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (time_conv): CausalConv3d(384, 768, kernel_size=(3, 1, 1), stride=(1, 1, 1))
    )
    (8): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (9): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (10): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (11): Resample(
      (resample): Sequential(
        (0): Upsample(scale_factor=(2.0, 2.0), mode='nearest-exact')
        (1): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (12): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (13): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
    (14): ResidualBlock(
      (residual): Sequential(
        (0): RMS_norm()
        (1): SiLU()
        (2): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        (3): RMS_norm()
        (4): SiLU()
        (5): Dropout(p=0.0, inplace=False)
        (6): CausalConv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): Identity()
    )
  )
  (head): Sequential(
    (0): RMS_norm()
    (1): SiLU()
    (2): CausalConv3d(96, 3, kernel_size=(3, 3, 3), stride=(1, 1, 1))
  )
)
    The following models are loaded: ['wan_video_vae'].
Using wan_video_text_encoder from pretrained_models/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth.
Using wan_video_dit from ['pretrained_models/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors'].
Using wan_video_vae from pretrained_models/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth.
No wan_video_image_encoder models available.
[OmniTrainingModule load_model] -> Use LoRA: lora rank: 128, lora alpha: 64.0, pretrained_lora_path: pretrained_models/OmniAvatar-1.3B/pytorch_model.pt
[OmniTrainingModule add_lora_to_model] -> lora_config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=128, target_modules={'k', 'q', 'v', 'ffn.2', 'ffn.0', 'o'}, exclude_modules=None, lora_alpha=64.0, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)
[OmniTrainingModule add_lora_to_model] -> 634 parameters are loaded from pretrained_models/OmniAvatar-1.3B/pytorch_model.pt. 0 parameters are unexpected.
[OmniTrainingModule __init__]: Model loaded on cpu, dtype: torch.float32
===================[train_pl.py]-main-] model summary================================
[OmniTrainingModule] - name: pipe.text_encoder.token_embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.0.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.1.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.2.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.3.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.4.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.5.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.6.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.7.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.8.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.9.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.10.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.11.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.12.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.13.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.14.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.15.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.16.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.17.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.18.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.19.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.20.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.21.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.22.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.norm1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.attn.q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.attn.k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.attn.v.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.attn.o.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.norm2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.ffn.gate.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.ffn.fc1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.ffn.fc2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.blocks.23.pos_embedding.embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.text_encoder.norm.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.patch_embedding.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.patch_embedding.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.text_embedding.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.text_embedding.0.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.text_embedding.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.text_embedding.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.time_embedding.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.time_embedding.0.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.time_embedding.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.time_embedding.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.time_projection.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.time_projection.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.0.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.1.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.2.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.3.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.4.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.5.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.6.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.7.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.8.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.9.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.10.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.11.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.12.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.13.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.14.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.15.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.16.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.17.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.18.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.19.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.20.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.21.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.22.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.23.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.24.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.25.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.26.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.27.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.28.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.self_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.q.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.q.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.q.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.q.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.k.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.k.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.k.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.k.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.v.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.v.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.v.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.v.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.o.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.o.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.o.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.o.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.norm_q.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.cross_attn.norm_k.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.norm3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.norm3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.0.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.0.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.0.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.0.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.2.base_layer.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.2.base_layer.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.2.lora_A.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.blocks.29.ffn.2.lora_B.default.weight, requires_grad: True
[OmniTrainingModule] - name: pipe.dit.head.modulation, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.head.head.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.head.head.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_proj.proj.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_proj.proj.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_proj.norm_out.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_proj.norm_out.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.0.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.0.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.3.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.3.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.4.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.4.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.5.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.5.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.7.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.7.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.8.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.8.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.9.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.9.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.10.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.10.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.11.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.11.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.12.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.12.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.13.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.dit.audio_cond_projs.13.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.conv1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.conv1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.0.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.0.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.0.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.0.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.0.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.0.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.1.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.1.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.1.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.1.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.1.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.1.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.2.resample.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.2.resample.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.shortcut.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.3.shortcut.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.4.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.4.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.4.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.4.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.4.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.4.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.5.resample.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.5.resample.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.5.time_conv.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.5.time_conv.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.shortcut.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.6.shortcut.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.7.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.7.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.7.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.7.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.7.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.7.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.8.resample.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.8.resample.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.8.time_conv.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.8.time_conv.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.9.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.9.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.9.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.9.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.9.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.9.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.10.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.10.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.10.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.10.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.10.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.downsamples.10.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.0.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.0.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.0.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.0.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.0.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.0.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.1.norm.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.1.to_qkv.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.1.to_qkv.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.1.proj.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.1.proj.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.2.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.2.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.2.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.2.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.2.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.middle.2.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.head.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.head.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.encoder.head.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.conv1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.conv1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.conv2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.conv2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.conv1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.conv1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.0.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.0.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.0.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.0.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.0.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.0.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.1.norm.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.1.to_qkv.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.1.to_qkv.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.1.proj.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.1.proj.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.2.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.2.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.2.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.2.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.2.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.middle.2.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.0.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.0.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.0.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.0.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.0.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.0.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.1.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.1.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.1.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.1.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.1.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.1.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.2.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.2.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.2.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.2.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.2.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.2.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.3.resample.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.3.resample.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.3.time_conv.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.3.time_conv.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.shortcut.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.4.shortcut.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.5.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.5.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.5.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.5.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.5.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.5.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.6.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.6.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.6.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.6.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.6.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.6.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.7.resample.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.7.resample.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.7.time_conv.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.7.time_conv.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.8.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.8.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.8.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.8.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.8.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.8.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.9.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.9.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.9.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.9.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.9.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.9.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.10.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.10.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.10.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.10.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.10.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.10.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.11.resample.1.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.11.resample.1.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.12.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.12.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.12.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.12.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.12.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.12.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.13.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.13.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.13.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.13.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.13.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.13.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.14.residual.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.14.residual.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.14.residual.2.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.14.residual.3.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.14.residual.6.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.upsamples.14.residual.6.bias, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.head.0.gamma, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.head.2.weight, requires_grad: False
[OmniTrainingModule] - name: pipe.vae.model.decoder.head.2.bias, requires_grad: False
[OmniTrainingModule] - name: audio_encoder.masked_spec_embed, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.0.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.0.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.0.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.1.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.2.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.3.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.4.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.5.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_extractor.conv_layers.6.conv.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_projection.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_projection.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_projection.projection.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.feature_projection.projection.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.pos_conv_embed.conv.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.pos_conv_embed.conv.parametrizations.weight.original0, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.pos_conv_embed.conv.parametrizations.weight.original1, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.0.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.1.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.2.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.3.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.4.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.5.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.6.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.7.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.8.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.9.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.10.final_layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.k_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.k_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.v_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.v_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.q_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.q_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.out_proj.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.attention.out_proj.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.layer_norm.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.feed_forward.intermediate_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.feed_forward.intermediate_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.feed_forward.output_dense.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.feed_forward.output_dense.bias, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.final_layer_norm.weight, requires_grad: True
[OmniTrainingModule] - name: audio_encoder.encoder.layers.11.final_layer_norm.bias, requires_grad: True
===================================================================================
[OmniTrainingModule]: start training with config: {'dtype': '16', 'text_encoder_path': 'pretrained_models/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth', 'image_encoder_path': 'None', 'dit_path': 'pretrained_models/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors', 'vae_path': 'pretrained_models/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth', 'wav2vec_path': 'pretrained_models/wav2vec2-base-960h', 'exp_path': 'pretrained_models/OmniAvatar-1.3B', 'num_persistent_param_in_dit': None, 'reload_cfg': True, 'sp_size': 1, 'seed': 42, 'image_sizes_720': [[400, 720], [720, 720], [720, 400]], 'image_sizes_1280': [[720, 720], [528, 960], [960, 528], [720, 1280], [1280, 720]], 'max_hw': 720, 'max_tokens': 30000, 'seq_len': 200, 'overlap_frame': 13, 'guidance_scale': 4.5, 'audio_scale': None, 'num_steps': 50, 'fps': 20, 'sample_rate': 16000, 'negative_prompt': 'Vivid color tones, background/camera moving quickly, screen switching, subtitles and special effects, mutation, overexposed, static, blurred details, subtitles, style, work, painting, image, still, overall grayish, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn face, deformed, disfigured, malformed limbs, fingers merging, motionless image, chaotic background, three legs, crowded background with many people, walking backward', 'silence_duration_s': 0.3, 'use_fsdp': False, 'tea_cache_l1_thresh': 0, 'dataset_base_path': '/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1', 'name': 'train_1.3B', 'savedir': '/mnt/hdd2/huanglingyu/vgg/OmniAvatar/outputs/train_1.3B', 'batch_size': 1, 'nodes': 1, 'devices': 1, 'num_train_epochs': 1, 'mode': 'train', 'checkpoint_path': '', 'lr': 0.0001, 'max_frames': 120, 'debug': True, 'debug_data_len': 100}
[OmniTrainingModule] configure_optimizers
[OmniTrainingModule] on_fit_start -> device: cuda:0, param device: cuda:0
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s][OmniTrainingModule] validation_step, args: ({'video_id': ['xUjy1uCHTyU_15'], 'prompt': ["two animated characters, a woman and a man, dancing on a beach at sunset. The woman wears a red dress with a flowing skirt and black boots, while the man wears a white shirt with a black vest and white pants. They dance energetically, moving in sync with each other, and their movements are fluid and expressive. The background features a vibrant orange and pink sunset over a calm ocean, with palm trees visible in the distance. The main subjects are the two animated characters. The woman has pink hair with a red headband, wears a red dress with a flowing skirt, and black boots. The man has white hair, wears a white shirt with a black vest and white pants. They are positioned side by side, facing each other, and their movements are synchronized as they dance. The woman's dress flows with her movements, and her hair moves with the breeze. The man's movements are equally expressive, with his arms and legs moving in rhythm. The camera remains stationary, capturing the entire scene from a fixed viewpoint, focusing on the two characters as they dance."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/xUjy1uCHTyU_15/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/xUjy1uCHTyU_15/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/xUjy1uCHTyU_15/first_frame.png']}, 0), kwargs keys: dict_keys([])
Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 468.69it/s][OmniTrainingModule] validation_step, args: ({'video_id': ['V8buho2fX8g_33'], 'prompt': ["a cartoon character, a man with a mustache and a blue suit, sitting at a desk in front of a computer screen. He appears to be working or presenting, with various expressions and gestures indicating different emotions and actions. The background includes a large screen displaying a grid of blue and green lines, suggesting a digital or scientific theme. The character's movements are subtle and include shifting his gaze, changing his facial expressions, and occasionally gesturing with his hands. The background remains static, with the grid of lines on the screen providing a consistent visual element throughout the video. The movements are slow and deliberate, emphasizing the character's thoughtful and focused demeanor. The main subject is a cartoon man with a mustache, wearing a blue suit and a red tie. He is positioned at a desk, facing the camera, and interacts with a computer screen in front of him. His expressions change from neutral to concerned, and he occasionally gestures with his hands. The character remains seated throughout the video, with his movements limited to his facial expressions and hand gestures. The camera is stationary, providing a consistent frontal view of the character at the desk. The view is at eye level, focusing on the character's upper body and face."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/V8buho2fX8g_33/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/V8buho2fX8g_33/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/V8buho2fX8g_33/first_frame.png']}, 1), kwargs keys: dict_keys([])
Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 616.40it/s]                                                                            Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/100 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/100 [00:00<?, ?it/s] [OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 0, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[Timer] VAE编码耗时: 8.329 秒
[Timer] forward_preprocess 总耗时: 8.782 秒
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=True, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([917.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['a conversation between two animated characters in a simple, minimalistic office setting. The character on the left, wearing glasses and a white lab coat, appears to be explaining something to the character on the right, who is dressed in a black shirt. The scene is set against a backdrop of a bookshelf filled with books, creating a professional and intellectual atmosphere. The main subjects are two animated characters. The character on the left has short black hair, wears glasses, a white lab coat, and a black shirt underneath. He is seated and appears to be speaking or explaining something. The character on the right has short hair, wears a black shirt, and is seated facing the left character. The two characters are positioned side by side, with the left character facing the camera and the right character facing the left character. The background consists of a simple office setting with a bookshelf filled with books. The bookshelf is made of wood and has a light brown color. The wall behind the bookshelf is a light yellow color, providing a clean and professional backdrop. The scene is well-lit, suggesting an indoor environment. The camera is stationary, providing a medium shot of the two characters from a frontal view. There is no camera movement, maintaining a steady and focused perspective on the conversation.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=True, inf=False, value=nan
[OmniTrainingModule] training_step -> loss: nan
Epoch 0:   1%|          | 1/100 [01:48<2:58:31,  0.01it/s]Epoch 0:   1%|          | 1/100 [01:48<2:58:32,  0.01it/s, v_num=44, train_loss_step=nan.0][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 1, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[Timer] VAE编码耗时: 2.652 秒
[Timer] forward_preprocess 总耗时: 2.982 秒
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=True, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([692.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['a whimsical scene with three animated characters drawn in a simple, cartoonish style. The characters are situated in a kitchen setting, with various kitchen items and decorations visible in the background. The characters interact with each other, engaging in playful and humorous actions. their movements and expressions, creating a light-hearted and entertaining atmosphere. The main subjects are three animated characters. The first character is a large, smiling creature with a round body and a large, cheerful smile. The second character is a smaller, more reserved creature with a round body and a simple, content expression. The third character is a small, round creature with a large, surprised expression. These characters are positioned close to each other, interacting and moving around. The background features a kitchen setting with various kitchen items and decorations. There are pots, pans, a stove, and other kitchen utensils. The scene is set against a light, beige background, giving it a warm and cozy feel. The kitchen items are arranged in a way that suggests a well-used and lived-in space. The camera is stationary, providing a fixed view of the scene. The perspective is slightly angled, capturing the characters and the background in a single, continuous shot.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=True, inf=False, value=nan
[OmniTrainingModule] training_step -> loss: nan
Epoch 0:   2%|▏         | 2/100 [01:54<1:33:22,  0.02it/s, v_num=44, train_loss_step=nan.0]Epoch 0:   2%|▏         | 2/100 [01:54<1:33:22,  0.02it/s, v_num=44, train_loss_step=nan.0][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 2, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[Timer] VAE编码耗时: 2.798 秒
[Timer] forward_preprocess 总耗时: 2.818 秒
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=True, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([448.7500], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['a playful and colorful scene of two animated characters, a red and yellow snail and a green and yellow fish, interacting with a black trash can filled with various items. The snail and fish are positioned on the ground next to the trash can, which is covered in a mix of trash and colorful items. The scene is set against a simple, industrial background with a gray wall and a concrete floor. The characters exhibit animated expressions and movements, creating a lively and engaging atmosphere. The background consists of a simple, industrial setting with a gray wall and a concrete floor. The trash can is the main object in the scene, filled with various items including a green and yellow fish, a red and yellow snail, a blue and yellow cup, a green and yellow spoon, and a black plastic bag. The scene is well-lit, with a focus on the characters and the trash can, creating a clear and detailed visual. The main subjects are a red and yellow snail and a green and yellow fish. The snail is positioned on the left side of the frame, while the fish is on the right. Both characters have expressive faces and are animated, with the snail appearing to be excited and the fish looking surprised. The trash can, which is filled with various items, is positioned behind the characters, with the items spilling out onto the ground. The characters interact with each other and the trash can, creating a dynamic scene. The camera is stationary, providing a fixed view of the scene from a slightly elevated angle, capturing the characters and the trash can in a clear and detailed manner.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=True, inf=False, value=nan
[OmniTrainingModule] training_step -> loss: nan
Epoch 0:   3%|▎         | 3/100 [02:00<1:04:50,  0.02it/s, v_num=44, train_loss_step=nan.0]Epoch 0:   3%|▎         | 3/100 [02:00<1:04:50,  0.02it/s, v_num=44, train_loss_step=nan.0][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 3, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[Timer] VAE编码耗时: 3.613 秒
[Timer] forward_preprocess 总耗时: 3.680 秒
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=True, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([886.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['A person is standing on a stage, presenting or speaking in front of an audience.The individual is wearing a purple hoodie with a white logo on the left chest area, dark pants, and black shoes. The person\'s hair is dark and appears to be shoulder-length. The attire suggests a casual yet possibly branded look, possibly indicating a gaming or entertainment context.The setting is a stage with a red backdrop, which is illuminated by bright stage lights. There is a large screen displaying a cartoon-style character with the text "HIBYMC" in the background. The stage has a patterned carpet, and the audience is seated in the dark, indicating that the event is likely indoors and possibly during the evening.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=True, inf=False, value=nan
[OmniTrainingModule] training_step -> loss: nan
Epoch 0:   4%|▍         | 4/100 [02:18<55:27,  0.03it/s, v_num=44, train_loss_step=nan.0]  Epoch 0:   4%|▍         | 4/100 [02:18<55:27,  0.03it/s, v_num=44, train_loss_step=nan.0][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 4, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[Timer] VAE编码耗时: 3.264 秒
[Timer] forward_preprocess 总耗时: 3.288 秒
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=True, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([943.5000], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["A man wearing a yellow face mask with a cartoon character design is sitting in the back seat of a car, engaging in a conversation with someone outside the frame. He is wearing a red shirt and gesturing with his hands, occasionally pointing towards the window. The car is parked in a sunny outdoor setting with trees and a bright sky visible through the window. The man's expressions and hand movements suggest he is explaining or discussing something with the person outside the car. The main subject is a man wearing a yellow face mask with a cartoon character design and a red shirt. He is seated in the back seat of a car, facing towards the front. His hands are frequently in motion, gesturing and pointing towards the window. The man's expressions and hand movements indicate he is actively communicating or explaining something. The background consists of the interior of a car, with the back seat visible. The car is parked outdoors, with trees and a bright sky visible through the window. The sunlight creates a bright and warm atmosphere inside the car. The man's movements are primarily hand gestures, including pointing and waving. His hands move in various directions, primarily towards the window. The background remains static, with no significant changes or movements. The camera is stationary, capturing a medium shot of the man from the back seat of the car. The view is slightly angled to include both the man and the window, providing a clear view of his gestures and expressions."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=True, inf=False, value=nan
[OmniTrainingModule] training_step -> loss: nan
Epoch 0:   5%|▌         | 5/100 [02:24<45:51,  0.03it/s, v_num=44, train_loss_step=nan.0]Epoch 0:   5%|▌         | 5/100 [02:24<45:51,  0.03it/s, v_num=44, train_loss_step=nan.0]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/100 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/100 [00:00<?, ?it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['xUjy1uCHTyU_15'], 'prompt': ["two animated characters, a woman and a man, dancing on a beach at sunset. The woman wears a red dress with a flowing skirt and black boots, while the man wears a white shirt with a black vest and white pants. They dance energetically, moving in sync with each other, and their movements are fluid and expressive. The background features a vibrant orange and pink sunset over a calm ocean, with palm trees visible in the distance. The main subjects are the two animated characters. The woman has pink hair with a red headband, wears a red dress with a flowing skirt, and black boots. The man has white hair, wears a white shirt with a black vest and white pants. They are positioned side by side, facing each other, and their movements are synchronized as they dance. The woman's dress flows with her movements, and her hair moves with the breeze. The man's movements are equally expressive, with his arms and legs moving in rhythm. The camera remains stationary, capturing the entire scene from a fixed viewpoint, focusing on the two characters as they dance."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/xUjy1uCHTyU_15/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/xUjy1uCHTyU_15/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/xUjy1uCHTyU_15/first_frame.png']}, 0), kwargs keys: dict_keys([])

Validation DataLoader 0:   1%|          | 1/100 [00:00<00:00, 300.43it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['V8buho2fX8g_33'], 'prompt': ["a cartoon character, a man with a mustache and a blue suit, sitting at a desk in front of a computer screen. He appears to be working or presenting, with various expressions and gestures indicating different emotions and actions. The background includes a large screen displaying a grid of blue and green lines, suggesting a digital or scientific theme. The character's movements are subtle and include shifting his gaze, changing his facial expressions, and occasionally gesturing with his hands. The background remains static, with the grid of lines on the screen providing a consistent visual element throughout the video. The movements are slow and deliberate, emphasizing the character's thoughtful and focused demeanor. The main subject is a cartoon man with a mustache, wearing a blue suit and a red tie. He is positioned at a desk, facing the camera, and interacts with a computer screen in front of him. His expressions change from neutral to concerned, and he occasionally gestures with his hands. The character remains seated throughout the video, with his movements limited to his facial expressions and hand gestures. The camera is stationary, providing a consistent frontal view of the character at the desk. The view is at eye level, focusing on the character's upper body and face."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/V8buho2fX8g_33/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/V8buho2fX8g_33/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/V8buho2fX8g_33/first_frame.png']}, 1), kwargs keys: dict_keys([])

Validation DataLoader 0:   2%|▏         | 2/100 [00:00<00:00, 179.12it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['wHRxsU26o8U_173'], 'prompt': ["a sequence of animated frames showing a character with large, expressive eyes and a simplistic design. The character appears to be a cartoonish creature with a pink and white color scheme. The character's eyes are large and black with white pupils, and it has a simple, friendly face. The character is initially seen peeking over a wooden fence, then it moves down a slide, and finally, it reaches the bottom of the slide. The background is a plain, light purple color, providing a clean and minimalistic setting for the character. The scene is set indoors, with no visible windows or external elements. The wooden fence at the top of the scene is the only object that adds texture and depth to the background. The main subject is a cartoonish character with large, expressive eyes and a simplistic design. The character has a pink and white color scheme, with a friendly and approachable appearance. The character's movements are smooth and deliberate. It starts by peeking over the wooden fence, then it moves down a slide, and finally, it reaches the bottom of the slide. The movement is slow and steady, with the character's head and body moving in a downward direction. The background remains static throughout the video, with no changes or movements."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/wHRxsU26o8U_173/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/wHRxsU26o8U_173/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/wHRxsU26o8U_173/first_frame.png']}, 2), kwargs keys: dict_keys([])

Validation DataLoader 0:   3%|▎         | 3/100 [00:00<00:00, 158.11it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['9qO7s-kXfEk_39'], 'prompt': ["two animated characters, a green-shirted character and a blue-shirted character, standing next to each other. The green-shirted character is holding a bottle of red liquid, which appears to be a beverage, while the blue-shirted character is holding a bottle of green liquid, also a beverage. The green-shirted character is initially looking downward with a sad expression, while the blue-shirted character is looking at the green-shirted character with a neutral expression. The scene takes place in a dimly lit environment with a staircase in the background. The green-shirted character's expression changes from sad to a more neutral look as the video progresses. The blue-shirted character remains relatively still, holding the green bottle. The background consists of a dimly lit environment with a staircase leading upwards. The staircase has a wooden texture and is partially visible. The background is mostly dark, with a red-colored wall or curtain visible in the distance, adding a sense of depth to the scene. The main subjects are two animated characters. The green-shirted character is holding a bottle of red liquid, which appears to be a beverage, and is initially looking downward with a sad expression. The blue-shirted character is holding a bottle of green liquid, also a beverage, and is looking at the green-shirted character with a neutral expression. The green-shirted character's position remains relatively static, while the blue-shirted character's position shifts slightly as they hold the green bottle. The camera is stationary, providing a medium shot of the two characters from the waist up, capturing their expressions and interactions clearly."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/9qO7s-kXfEk_39/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/9qO7s-kXfEk_39/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/9qO7s-kXfEk_39/first_frame.png']}, 3), kwargs keys: dict_keys([])

Validation DataLoader 0:   4%|▍         | 4/100 [00:00<00:00, 156.04it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['InGaVPreV7Q_16'], 'prompt': ["a cartoonish red character with large, expressive eyes and glasses, peeking out from behind a clock. The character is positioned on the left side of the clock face, which is white with black numbers and hands. The background is a solid blue, providing a contrasting backdrop to the vibrant red character. The character's eyes move slightly as it looks around, creating a playful and engaging scene. The main subject is a red character with large, round eyes and glasses. The character is positioned on the left side of the clock face, partially obscured by the clock's edge. The character's eyes move slightly, giving the impression of curiosity or surprise. The character remains stationary in terms of its position on the clock, but its eyes move slightly, indicating subtle movements. The background is a solid blue color, providing a clean and simple backdrop that contrasts with the vibrant red character. There are no additional objects or elements in the background, keeping the focus on the character and the clock. The camera is stationary, focusing on a close-up view of the clock and the character. The angle is slightly tilted, providing a clear view of the character's face and the clock's face."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/InGaVPreV7Q_16/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/InGaVPreV7Q_16/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/InGaVPreV7Q_16/first_frame.png']}, 4), kwargs keys: dict_keys([])

Validation DataLoader 0:   5%|▌         | 5/100 [00:00<00:00, 156.76it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['LNdhktKSj7w_20'], 'prompt': ["a close-up examination of a small toy figure, focusing on its details and features.The main subject is a small toy figure, which appears to be a character from a popular children's franchise, judging by the packaging and design. The toy is dressed in a blue outfit with a yellow and red headpiece, resembling a character from a popular animated series. The figure is held and rotated by a person with manicured nails painted in a shiny, dark color.The setting is a playful and colorful environment, likely a child's room or a toy display area. The background is filled with various toys and merchandise, including a toy house with a green roof, a toy figure resembling a character from a popular animated series, and other toys and packaging that suggest a theme of children's entertainment. The scene is well-lit, with a focus on the toy figure, which is the central point of interest."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/LNdhktKSj7w_20/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/LNdhktKSj7w_20/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/LNdhktKSj7w_20/first_frame.png']}, 5), kwargs keys: dict_keys([])

Validation DataLoader 0:   6%|▌         | 6/100 [00:00<00:00, 148.12it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['IgRUZToneHg_32'], 'prompt': ['three individuals seated on a couch, engaged in a lively discussion. The central figure, a man with a beard and glasses, is animatedly gesturing with his hands while speaking. He is dressed in a green shirt with a graphic design and a denim vest. The other two individuals, a man and a woman, are also seated on the couch, with the man holding a tablet and the woman looking on. The setting is casual and relaxed, with various pop culture items and snacks scattered around them. The main subject is a man with a beard and glasses, wearing a green shirt with a graphic design and a denim vest. He is seated on the couch, gesturing with his hands as he speaks. The other two individuals, a man and a woman, are also seated on the couch. The man on the left is holding a tablet and appears to be listening attentively. The woman on the right is looking at the man with the beard, possibly reacting to his gestures. The background consists of a red and white wall with various pop culture elements, including cartoon characters and comic book covers. There are also snacks and drinks placed on the couch, adding to the casual and relaxed atmosphere. The scene appears to be set indoors, possibly in a living room or a studio. The camera is stationary, providing a medium shot of the three individuals on the couch. The view is slightly angled to capture the expressions and gestures of the central figure.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/IgRUZToneHg_32/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/IgRUZToneHg_32/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/IgRUZToneHg_32/first_frame.png']}, 6), kwargs keys: dict_keys([])

Validation DataLoader 0:   7%|▋         | 7/100 [00:00<00:00, 156.55it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['kXo-GvedGLU_49'], 'prompt': ['a colorful animated scene inside a bus, where various characters are seated and interacting with each other. The main character, a large, friendly-looking creature with a furry blue and orange body, is seated in the front row, smiling and looking around. Other characters, including a young boy with blonde hair, a girl with dark hair, and a character dressed in a white astronaut suit, are also present, sitting and looking around. The background shows a cityscape with tall buildings and a clear sky, indicating the bus is traveling through an urban area. The main character, a large, friendly creature with a furry blue and orange body, is seated in the front row of the bus. It has large, expressive eyes and a cheerful smile. The other characters include a young boy with blonde hair, a girl with dark hair, and a character dressed in a white astronaut suit. The boy and girl are seated next to each other, while the astronaut character is seated across from them. The characters are mostly stationary, with subtle movements such as turning their heads and shifting their gaze. The background shows a cityscape with tall buildings and a clear blue sky, indicating the bus is traveling through an urban area. The buildings are colorful and varied, adding to the lively atmosphere of the scene. The cityscape is static, providing a consistent backdrop for the animated characters. The camera is stationary, providing a fixed view of the interior of the bus from a frontal perspective, capturing the expressions and interactions of the characters.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/kXo-GvedGLU_49/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/kXo-GvedGLU_49/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/kXo-GvedGLU_49/first_frame.png']}, 7), kwargs keys: dict_keys([])

Validation DataLoader 0:   8%|▊         | 8/100 [00:00<00:00, 169.44it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['KV5VhhCuu68_105'], 'prompt': ['a group of animated characters gathered around a table, engaging in a lively conversation. The characters are expressive and animated, with exaggerated facial expressions and body language. The setting is a bright, cheerful environment with a clear blue sky and a few floating orbs in the background. The characters are animated and expressive, with movements such as raising arms, leaning forward, and making animated gestures. The background remains static, with no significant changes or movements. The overall movement is moderate in amplitude and speed, typical of animated characters in a conversation. The main subjects are five animated characters. The first character is a girl with pink hair and glasses, wearing a red shirt. The second character is a boy with black hair and glasses, wearing a red shirt and a black vest. The third character is a robot with a silver body and red eyes, wearing a black and white suit. The fourth character is a green-skinned character with a hood, wearing a black robe. The fifth character is a purple-skinned character with a hood, wearing a black robe. They are all seated around a table, interacting with each other. The camera is stationary, providing a wide view of the characters and the table, capturing the entire scene in a single, steady shot.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/KV5VhhCuu68_105/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/KV5VhhCuu68_105/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/KV5VhhCuu68_105/first_frame.png']}, 8), kwargs keys: dict_keys([])

Validation DataLoader 0:   9%|▉         | 9/100 [00:00<00:00, 181.50it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['W1_l2Y_JiWM_28'], 'prompt': ["a whimsical scene with two animated characters, a green duck and an orange cat, interacting outside a red house. The duck, wearing a green hat and a purple scarf, approaches the house and engages with the cat, who is seen through a window. The cat, with large, expressive eyes and a surprised expression, looks out the window and interacts with the duck by waving and gesturing. The duck moves from the left side of the frame towards the house, stops in front of the window, and engages with the cat by waving and gesturing. The cat remains stationary inside the house, only moving its arms and facial expressions to interact with the duck. The movements are smooth and deliberate, with the duck moving at a moderate pace and the cat's movements being more expressive but limited to the window frame. The main subjects are a green duck and an orange cat. The duck has a green hat, a purple scarf, and a surprised expression. The cat has large, expressive eyes, a surprised expression, and is seen through a window. The duck moves towards the house and interacts with the cat by waving and gesturing. The cat remains inside the house, looking out the window and interacting with the duck. The background consists of a red house with a white window frame and a white wall. The scene is set outdoors with a grassy area in front of the house. The background is simple and uncluttered, focusing on the interaction between the duck and the cat. The camera is stationary, capturing a medium shot of the duck and the cat from a slightly low angle, providing a clear view of the interaction between the two characters."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/W1_l2Y_JiWM_28/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/W1_l2Y_JiWM_28/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/W1_l2Y_JiWM_28/first_frame.png']}, 9), kwargs keys: dict_keys([])

Validation DataLoader 0:  10%|█         | 10/100 [00:00<00:00, 192.13it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['JijKSE5OPwI_3315'], 'prompt': ["a whimsical scene set in a forest where a small, animated character, resembling a young girl, is perched on a tree branch. She is wearing a hat and a dress, and her facial expressions change from a smile to a look of concern. In the background, two animated characters, one resembling a bear and the other a rabbit, interact with each other. The scene is set in a forest with tall trees and a soft, dreamy atmosphere. The main character is a small, animated girl with a cheerful expression, wearing a hat and a dress. She is positioned on a tree branch, facing the camera. The bear character is positioned behind her, while the rabbit character is to the right of the frame. The girl's expressions change from a smile to a concerned look as she looks around. The bear character is mostly stationary, while the rabbit character moves around the tree. The camera is static, focusing on the girl on the tree branch, with occasional close-up shots of the bear and rabbit characters. The view is primarily from a medium distance, capturing the main character and the background characters clearly."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/JijKSE5OPwI_3315/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/JijKSE5OPwI_3315/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/JijKSE5OPwI_3315/first_frame.png']}, 10), kwargs keys: dict_keys([])

Validation DataLoader 0:  11%|█         | 11/100 [00:00<00:00, 201.90it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['XOtryKiK9ek_26'], 'prompt': ['a playful scene with two animated toy characters, a yellow chick and a blue bunny, sitting on a wooden bench. The chick has large, expressive eyes and a cheerful demeanor, while the bunny has a more reserved but friendly appearance. The setting includes a table with various toy furniture pieces, such as a white cake, a blue chair, and a purple tablecloth. the characters interacting with each other and the surrounding toys, creating a whimsical and engaging atmosphere. The main subjects are the yellow chick and the blue bunny. The chick has large, round eyes with black pupils and a cheerful expression, while the bunny has smaller, round eyes with black pupils and a more reserved but friendly demeanor. They are positioned on a wooden bench, with the chick on the left and the bunny on the right. The characters occasionally move closer to each other and interact with the toy furniture around them. The background consists of a table with various toy furniture pieces, including a white cake with purple icing, a blue chair, and a purple tablecloth. The scene is set on a white surface, likely a table or a play area. The toys are arranged in a way that suggests a cozy and inviting environment, possibly a playroom or a dollhouse setup. The camera is stationary, providing a stable and clear view of the scene from a slightly elevated angle, capturing the entire setup of the toy characters and their surroundings.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/XOtryKiK9ek_26/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/XOtryKiK9ek_26/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/XOtryKiK9ek_26/first_frame.png']}, 11), kwargs keys: dict_keys([])

Validation DataLoader 0:  12%|█▏        | 12/100 [00:00<00:00, 204.89it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['9A4qN6PfFfE_28'], 'prompt': ["a humorous scene with a character dressed in a blue suit and red tie, wearing large, round glasses, and holding a colorful, oversized bow tie. The character is animated and expressive, making various facial expressions and gestures while holding the bow tie. The background includes a blurred space setting with rockets and other space-themed elements, adding a playful and imaginative atmosphere to the scene. The character's movements are animated and expressive, involving various facial expressions and hand gestures. The character holds the bow tie up to their face, adjusts it, and occasionally looks around. The movements are moderate in amplitude, with a medium speed, and are primarily focused on the upper body and face. The background remains static throughout the video. The main subject is a character with a blue suit, red tie, and large round glasses. The character is holding a bright blue bow tie with red polka dots. The character's expressions and hand movements suggest a playful and humorous demeanor. The character is positioned centrally in the frame and remains the focal point throughout the video. The camera is stationary, focusing on a medium close-up view of the character, capturing their upper body and facial expressions clearly."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/9A4qN6PfFfE_28/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/9A4qN6PfFfE_28/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/9A4qN6PfFfE_28/first_frame.png']}, 12), kwargs keys: dict_keys([])

Validation DataLoader 0:  13%|█▎        | 13/100 [00:00<00:00, 209.08it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['ClvOpkBJUwI_43'], 'prompt': ['a sequence of animated characters engaged in various actions against a dynamic background.The main characters include a white tiger with a yellow mane and a large, expressive face, displaying a range of emotions from curiosity to excitement. Another character is a large, anthropomorphic creature with a bear-like body, wearing a red and white striped shirt, a red vest, and a green tail. This character has a cheerful demeanor, with a wide smile and a friendly expression. There is also a white bird with a red beak and a red band around its neck, which appears to be in motion, possibly flying or gliding.The setting is a vibrant blue background with a starburst effect, giving the impression of rapid movement or travel through space. The characters are set against this backdrop, which enhances the sense of motion and adventure.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/ClvOpkBJUwI_43/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/ClvOpkBJUwI_43/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/ClvOpkBJUwI_43/first_frame.png']}, 13), kwargs keys: dict_keys([])

Validation DataLoader 0:  14%|█▍        | 14/100 [00:00<00:00, 211.14it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['K9MNRUPJaqY_10'], 'prompt': ['a sequence of animated characters engaged in a conversation.The characters are animated with distinct features. One character has blonde hair, wears a white shirt with a blue tie, and has a red scarf around the neck. The other character has brown hair, wears a green shirt, and has glasses. Both characters have expressive faces, with the blonde character often seen with a surprised or shocked expression, while the brown-haired character displays a range of emotions from confusion to contemplation.The setting appears to be an indoor corridor with a dark background, possibly a hallway or a passageway. The lighting is dim, with some illumination coming from the ceiling, casting shadows on the walls and the characters. The environment suggests a serious or tense atmosphere, possibly indicating a significant event or dialogue taking place between the characters.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/K9MNRUPJaqY_10/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/K9MNRUPJaqY_10/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/K9MNRUPJaqY_10/first_frame.png']}, 14), kwargs keys: dict_keys([])

Validation DataLoader 0:  15%|█▌        | 15/100 [00:00<00:00, 214.10it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['pL3ZO1XNG6Q_3'], 'prompt': ['a group of animated characters in a futuristic setting. A muscular robot stands in the center, holding a large, futuristic gun. To the left, a young boy with glasses and a red jacket looks on with concern. To the right, a green-haired character with a surprised expression sits on a chair. The background includes a futuristic control panel and a purple light effect, suggesting a high-tech environment. The robot aims and fires the gun, causing the green-haired character to scream and fall back in the chair. The boy on the left reacts with shock and horror. The scene ends with the robot walking away, leaving the characters in a state of distress. The main subjects are the robot, the boy, and the green-haired character. The robot is muscular, wearing a metallic suit with a visor, and holds a large futuristic gun. The boy is wearing glasses and a red jacket, standing to the left with a concerned expression. The green-haired character has spiky hair and is sitting on a chair, initially looking surprised and then screaming and falling back in the chair. The robot aims and fires the gun, causing the green-haired character to fall back and the boy to react with shock. The robot then walks away, leaving the characters in distress. The camera is static, providing a wide view of the scene, capturing the interactions and reactions of the characters without any movement or zooming.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/pL3ZO1XNG6Q_3/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/pL3ZO1XNG6Q_3/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/pL3ZO1XNG6Q_3/first_frame.png']}, 15), kwargs keys: dict_keys([])

Validation DataLoader 0:  16%|█▌        | 16/100 [00:00<00:00, 216.92it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['mAajfve2ZE4_32'], 'prompt': ["a playful and animated sequence where a small, blue, cartoonish character with large eyes and a cheerful expression interacts with a large, blue, cartoonish character with a menacing expression. The small character is initially seen standing on a wooden floor, looking up at the large character, which is initially seen in the background. The large character then appears to be floating or levitating towards the small character, causing the small character to become startled and scared. The large character then proceeds to engage in a playful and friendly interaction with the small character, who remains cautious but eventually smiles and laughs. The background consists of a simple, two-tone blue and orange striped wallpaper. The floor is wooden with visible grain and some scattered pieces of wood. The scene is set indoors, with no additional objects or elements visible, creating a clean and minimalistic backdrop that focuses attention on the characters. The main subjects are two cartoonish characters. The small character is blue with large, expressive eyes and a cheerful smile. The large character is also blue with a menacing expression, large eyes, and a somewhat intimidating appearance. The small character is positioned on the floor, while the large character is initially in the background and then moves closer to the small character. The large character's arms are outstretched, and it appears to be floating or levitating. The camera is static, with a fixed view that captures the entire scene from a slightly elevated angle, providing a clear and unobstructed view of the characters and their interactions."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/mAajfve2ZE4_32/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/mAajfve2ZE4_32/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/mAajfve2ZE4_32/first_frame.png']}, 16), kwargs keys: dict_keys([])

Validation DataLoader 0:  17%|█▋        | 17/100 [00:00<00:00, 218.80it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['do8WF43itUc_67'], 'prompt': ["a cartoon character, a blue figure with a simple face, performing various actions and expressions on a white paper background. The character starts with a neutral expression, then transitions to a surprised look, followed by a happy and excited expression, and finally, a content and satisfied expression. The background remains static throughout the video, emphasizing the character's movements and expressions. The main subject is a blue cartoon character with a simple face, two eyes, a nose, and a mouth. The character has a small body with two arms and legs, and it is depicted in various poses and expressions. The character starts with a neutral expression, then shows surprise, happiness, excitement, and finally, contentment. The character is positioned centrally on the white paper background and remains the focal point throughout the video. The background is a plain white paper with a subtle texture, giving a clean and minimalist look. There are no additional objects or elements in the background, ensuring that the focus remains on the character. The scene is well-lit, with even lighting that highlights the character's features and movements. The camera is static, with a fixed view that centers the character in the frame, ensuring that the focus remains on the character's expressions and movements."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/do8WF43itUc_67/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/do8WF43itUc_67/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/do8WF43itUc_67/first_frame.png']}, 17), kwargs keys: dict_keys([])

Validation DataLoader 0:  18%|█▊        | 18/100 [00:00<00:00, 217.23it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['3wLF8oDA3ZM_7'], 'prompt': ["a sequence where a young girl interacts with a large, expressive animated monkey. The girl, with short brown hair and wearing an orange shirt, gently pats and comforts the monkey, which has large, expressive green eyes and a soft, furry appearance. The scene takes place in a dark, mysterious environment with a backdrop of floating rocks and a bright, glowing light source. The interaction between the girl and the monkey is tender and affectionate, highlighting a bond between the two characters. The main subjects are a young girl and a large animated monkey. The girl has short brown hair and wears an orange shirt. She is positioned to the left of the monkey, often seen patting or touching the monkey's head and back. The monkey has large, expressive green eyes, a soft fur texture, and a gentle demeanor. The girl and monkey are closely positioned, with the girl often leaning in to comfort the monkey. The background is a dark, mysterious environment with floating rocks and a bright, glowing light source. The rocks are scattered and appear to be suspended in the air, creating a sense of depth and mystery. The light source is intense and central, casting a warm glow that illuminates the scene. The overall atmosphere is somewhat ethereal and otherworldly. The camera remains mostly static, focusing on a close-up view of the girl and monkey, capturing their expressions and interactions in detail."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/3wLF8oDA3ZM_7/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/3wLF8oDA3ZM_7/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/3wLF8oDA3ZM_7/first_frame.png']}, 18), kwargs keys: dict_keys([])

Validation DataLoader 0:  19%|█▉        | 19/100 [00:00<00:00, 220.26it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['lUXsK27lbro_60'], 'prompt': ['a person drawing a cartoon character on a piece of paper. The character is a muscular, anthropomorphic creature with a large head and a muscular build. The drawing process involves adding various elements to the character, such as a heart, a hammer, and a shield, as well as a background of greenery. The drawing is done with a white marker, and the character is depicted with a determined and strong expression. The main subject is a cartoon character, which is a muscular, anthropomorphic creature with a large head, muscular build, and a determined expression. The character is drawn with a white marker, and various elements are added to it, including a heart, a hammer, a shield, and a background of greenery. The character is positioned centrally on the paper, and the drawing progresses with each element being added around it. The background consists of a green, leafy environment, which is drawn around the character. The greenery is detailed and adds depth to the scene. The background is static and does not change throughout the video. The camera is stationary, providing a close-up view of the drawing process. The focus remains on the character and the elements being added to it, with no camera movement.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/lUXsK27lbro_60/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/lUXsK27lbro_60/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/lUXsK27lbro_60/first_frame.png']}, 19), kwargs keys: dict_keys([])

Validation DataLoader 0:  20%|██        | 20/100 [00:00<00:00, 223.11it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['kUgUD1_3Sqw_56'], 'prompt': ["a whimsical scene set in a mechanical environment, where a group of animated characters interacts with a large gear mechanism. The characters include a pink-haired girl with a top hat, a blue-haired girl with a blue bow, and a brown-haired boy with a red bow. They are positioned on a wooden platform that spans across a large gear, with the gear's teeth and the background machinery providing a detailed and intricate backdrop. The characters engage in playful and animated movements, interacting with each other and the gear mechanism. The main subjects are three animated characters. The first character is a pink-haired girl with a top hat, wearing a pink dress and holding a wand. The second character is a blue-haired girl with a blue bow, wearing a blue dress and holding a blue wand. The third character is a brown-haired boy with a red bow, wearing a red dress and holding a red wand. They are positioned on a wooden platform that spans across a large gear mechanism. The characters interact with each other and the gear, with the girl in the top hat often pointing or gesturing with her wand. The background consists of a detailed mechanical environment, featuring large gears, metal structures, and intricate machinery. The scene is set in a dark, industrial-like setting with a focus on the gear mechanism and its components. The lighting is dim, highlighting the mechanical elements and creating a dramatic and somewhat mysterious atmosphere. The camera is stationary, providing a fixed view of the scene from a slightly elevated angle, capturing the entire platform and the gear mechanism in a single, continuous shot."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/kUgUD1_3Sqw_56/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/kUgUD1_3Sqw_56/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/kUgUD1_3Sqw_56/first_frame.png']}, 20), kwargs keys: dict_keys([])

Validation DataLoader 0:  21%|██        | 21/100 [00:00<00:00, 221.79it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['pIoo2YBOID8_62'], 'prompt': ["a group of animated characters interacting with a map. The characters include a green alien with large eyes and a purple shirt, a pink-haired character with a white shirt and a pink skirt, and a green character with a yellow shirt and a red backpack. The characters are engaged in various actions such as pointing at the map, holding it, and gesturing towards each other. The scene is set against a plain background, emphasizing the characters' movements and interactions. The main subjects are three animated characters. The green alien character has large, expressive eyes and wears a purple shirt and a purple skirt. The pink-haired character has a white shirt and a pink skirt, and the green character has a yellow shirt and a red backpack. The characters are positioned close to each other, with the green alien holding the map and the pink-haired character pointing at it. The green character is standing behind the others, occasionally interacting with them. The background is a plain, light-colored paper with a slightly textured surface, providing a neutral backdrop that keeps the focus on the characters. There are no additional objects or elements in the background, creating a simple and uncluttered scene. The camera is static, providing a consistent view of the characters and their interactions with the map. The view is a medium shot, capturing the upper bodies of the characters and the map they are holding."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/pIoo2YBOID8_62/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/pIoo2YBOID8_62/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/pIoo2YBOID8_62/first_frame.png']}, 21), kwargs keys: dict_keys([])

Validation DataLoader 0:  22%|██▏       | 22/100 [00:00<00:00, 223.24it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['lCm4_UX0MW8_32'], 'prompt': ['a sequence of animated drawings on a white paper background. The drawings depict a green plant sprouting from a grave, a cartoonish character with a sad expression, and a green creature with a surprised expression. The drawings are simple and cartoonish, with bold lines and minimal detailing. The sequence progresses with the plant growing, the character becoming more animated, and the creature appearing and interacting with the other characters. The main objects in the video are a green plant, a sad-looking cartoon character, and a green creature. The plant is initially small and green, sprouting from a grave with the word "RIP" written above it. The sad-looking character has a round body, large eyes, and a frowning expression. The green creature has a surprised expression, with large eyes and a small body. The plant grows taller and more detailed as the video progresses, while the character becomes more animated and expressive, and the creature appears and interacts with the other characters. The background is a plain white paper with a slightly textured surface, giving it a slightly aged look. There are no additional objects or elements in the background, keeping the focus on the animated drawings. The camera view is static, focusing on the drawings without any movement or zooming. The perspective is slightly angled, providing a clear view of the characters and their interactions.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/lCm4_UX0MW8_32/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/lCm4_UX0MW8_32/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/lCm4_UX0MW8_32/first_frame.png']}, 22), kwargs keys: dict_keys([])

Validation DataLoader 0:  23%|██▎       | 23/100 [00:00<00:00, 225.43it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['eYdJmWwxLVc_3'], 'prompt': ["A young woman with long blonde hair and blue nail polish is holding a bright orange plastic cup with a cartoon chicken design. She is sitting indoors, in front of a wooden playpen with a string of lights in the background. The woman is smiling and talking animatedly, occasionally looking at the cup and then at the camera. She appears to be demonstrating or explaining something related to the cup. The main subject is a young woman with long blonde hair and blue nail polish. She is holding a bright orange plastic cup with a cartoon chicken design. The cup has a small white sticker on it. The woman is smiling and talking, occasionally looking at the cup and then at the camera. Her expressions and gestures suggest she is explaining or demonstrating something. The woman's movements are primarily focused on her upper body and hands. She moves the cup around, looking at it and then at the camera. Her gestures are expressive, with her hands moving slightly as she talks. The background remains static, with no noticeable changes or movements. The camera is stationary, focusing on a medium close-up view of the woman. The camera angle is slightly tilted upwards, capturing the woman's upper body and the cup she is holding."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/eYdJmWwxLVc_3/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/eYdJmWwxLVc_3/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/eYdJmWwxLVc_3/first_frame.png']}, 23), kwargs keys: dict_keys([])

Validation DataLoader 0:  24%|██▍       | 24/100 [00:00<00:00, 228.36it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['EMdbWt6ut5g_53'], 'prompt': ['A man and a woman are seated at a dining table, enjoying a meal together. The man, wearing a black t-shirt with a cartoon character on it, is holding a box of Cinnamon Toast Crunch cereal and a spoon. The woman, dressed in a plaid shirt, is also holding a spoon and a piece of cereal. They are engaged in conversation and eating, with the man occasionally gesturing with the cereal box. The scene is set in a cozy, well-lit room with large windows covered by white curtains, creating a warm and inviting atmosphere. The main subjects are a man and a woman. The man is wearing glasses, a black t-shirt with a cartoon character on it, and has short hair. He is holding a box of Cinnamon Toast Crunch cereal and a spoon. The woman has long hair, wears glasses, and is dressed in a plaid shirt. She is holding a spoon and a piece of cereal. They are seated across from each other at a dining table, engaging in conversation and eating. The man occasionally gestures with the cereal box. The background features a cozy, well-lit room with large windows covered by white curtains. The windows allow natural light to flood the room, creating a warm and inviting atmosphere. The windowsill has some decorative items, including a small figurine and a plant. The table is covered with various dishes, including a large tray of food and a box of Cinnamon Toast Crunch cereal. The camera is stationary, providing a medium shot of the man and woman at the table, capturing their interactions and the food on the table.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/EMdbWt6ut5g_53/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/EMdbWt6ut5g_53/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/EMdbWt6ut5g_53/first_frame.png']}, 24), kwargs keys: dict_keys([])

Validation DataLoader 0:  25%|██▌       | 25/100 [00:00<00:00, 228.38it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['C6FZwkF-kFI_94'], 'prompt': ["a cartoon character, a green creature with a large head and a small body, sitting on a chair in front of a television. The creature appears to be angry and is seen throwing a green object at the television. The scene is set against a light blue background with a rough, textured paper texture. The creature's facial expressions change from angry to more aggressive as it throws the object. The main subject is a green creature with a large head, small body, and large eyes. It is sitting on a chair with its legs crossed. The creature is wearing glasses and has a frowning expression. It is holding a green object in its right hand, which it throws at the television. The television is positioned to the right of the creature, and it has a black screen with a green and red circular logo. The creature's movements are dynamic, with its body leaning forward and its arm extending to throw the green object. The object is thrown with force, moving from the creature's hand towards the television. The movement is quick and directed, with the object traveling at a moderate speed towards the television. The background remains static throughout the video. The camera is stationary, providing a fixed view of the scene from a side angle, capturing the entire action without any movement or zoom."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/C6FZwkF-kFI_94/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/C6FZwkF-kFI_94/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/C6FZwkF-kFI_94/first_frame.png']}, 25), kwargs keys: dict_keys([])

Validation DataLoader 0:  26%|██▌       | 26/100 [00:00<00:00, 232.22it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['6_77CGw9yLE_68'], 'prompt': ["a cartoon dog character interacting with a cardboard box. The dog, wearing a checkered shirt, initially holds the box in its mouth and then places it on the ground. The dog then looks inside the box, seemingly curious about its contents. The scene takes place in front of a white door with a window, and the background includes a green lawn and a residential area. The dog's movements are deliberate and slow. It initially holds the box in its mouth, then places it on the ground, and finally looks inside the box. The dog's head moves slightly as it inspects the box, and its body remains mostly stationary. The background remains static throughout the video, with no noticeable changes or movements. The main subject is a cartoon dog character. The dog has a light brown fur coat, large expressive eyes, and a small black nose. It wears a checkered shirt with pink and white stripes. The dog holds a cardboard box in its mouth and then places it on the ground. The dog then looks inside the box, showing curiosity. The dog is positioned in front of a white door with a window, and it remains the focal point of the video. The camera is stationary, providing a fixed view of the scene from a slightly low angle, focusing on the dog and the door."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/6_77CGw9yLE_68/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/6_77CGw9yLE_68/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/6_77CGw9yLE_68/first_frame.png']}, 26), kwargs keys: dict_keys([])

Validation DataLoader 0:  27%|██▋       | 27/100 [00:00<00:00, 235.72it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['vH6fgxat5Eg_171'], 'prompt': ['two animated characters, a girl with long, curly brown hair and a girl with long, curly red hair, standing in a vibrant, colorful garden. The girl with brown hair is wearing an orange dress with a floral pattern and a white headband with a yellow flower. The girl with red hair is wearing a pink dress with white polka dots and a white hat with a green leaf. They are standing in front of a pink, whimsical gazebo adorned with strawberries and greenery. The girl with red hair is initially looking down and then turns to face the girl with brown hair, smiling and waving. The scene is bright and cheerful, with a focus on the interaction between the two characters. The main subjects are the two animated girls. The girl with brown hair is positioned on the left side of the frame, while the girl with red hair is on the right. The girl with brown hair is looking down initially but then turns to face the girl with red hair, who is smiling and waving. They are standing close to each other, with the girl with red hair initially looking down and then turning to face the girl with brown hair. The camera is stationary, capturing a medium shot of the two girls from a slightly low angle, focusing on their upper bodies and the surrounding garden.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/vH6fgxat5Eg_171/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/vH6fgxat5Eg_171/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/vH6fgxat5Eg_171/first_frame.png']}, 27), kwargs keys: dict_keys([])

Validation DataLoader 0:  28%|██▊       | 28/100 [00:00<00:00, 238.89it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['VWEX6G9Gj2g_38'], 'prompt': ['two animated characters, a large blue-skinned creature with a greenish-blue face and a smaller pink-skinned creature with a red face. The blue creature is wearing a green shirt and brown shorts, while the pink creature is wearing a pink dress with a blue flower in her hair. The blue creature is initially seen running away from the pink creature, who is chasing after him. The blue creature then stops and looks back, appearing surprised and slightly scared. The pink creature continues to chase him, and the blue creature covers his face with his hands, looking frightened. The scene is set in a lush, green environment with a blurred background, suggesting a forest or a grassy area. The main subjects are the two animated characters. The blue creature has large, expressive eyes and a surprised expression, while the pink creature has a cheerful and determined expression. The blue creature is initially running away from the pink creature, who is chasing him. The blue creature then stops and looks back, covering his face with his hands, indicating fear. The pink creature continues to chase him, and the blue creature remains in a defensive posture. The background is a blurred, green environment, suggesting a forest or a grassy area. The scene is set outdoors with a natural, lush green color palette. There are no other objects or characters in the background, keeping the focus on the two main characters. The camera is static, focusing on the two characters from a medium close-up view, capturing their expressions and movements clearly.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/VWEX6G9Gj2g_38/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/VWEX6G9Gj2g_38/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/VWEX6G9Gj2g_38/first_frame.png']}, 28), kwargs keys: dict_keys([])

Validation DataLoader 0:  29%|██▉       | 29/100 [00:00<00:00, 241.94it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['MrZ9ByqPs_c_6'], 'prompt': ["A man and a woman are seated next to each other in a casual indoor setting, engaging in a lively conversation. The man, wearing a gray cap, gray t-shirt with a Marvel comic design, and a wristwatch, is animatedly gesturing with his hands while speaking. The woman, dressed in a black top, is attentively listening and occasionally smiling. In the background, an elderly man is seated at a table, partially visible and seemingly engaged in a separate activity. The main subjects are a man and a woman. The man is wearing a gray cap, gray t-shirt with a Marvel comic design, and a wristwatch. He is gesturing with his hands while speaking, indicating an animated conversation. The woman is dressed in a black top and is seated next to him, attentively listening and occasionally smiling. The elderly man in the background is seated at a table, partially visible, and appears to be engaged in a separate activity. The man's gestures are dynamic and varied, moving his hands in different directions as he speaks. The woman remains relatively still, occasionally smiling and looking at the man. The elderly man in the background is stationary, focused on his activity. The overall movement is moderate, with the man's gestures being the most prominent. The camera is stationary, capturing a medium shot of the man and woman from the side, with a slight focus on the man's gestures and expressions."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/MrZ9ByqPs_c_6/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/MrZ9ByqPs_c_6/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/MrZ9ByqPs_c_6/first_frame.png']}, 29), kwargs keys: dict_keys([])

Validation DataLoader 0:  30%|███       | 30/100 [00:00<00:00, 235.43it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['40A95zIfaR0_6'], 'prompt': ['two animated blue characters, one wearing a police uniform and the other in a military uniform, engaged in a conversation inside a vehicle. The scene is set in a vehicle with a visible steering wheel and a window. The characters are animated in a realistic manner, with detailed facial expressions and body movements. The background includes a cloudy sky with a large, glowing statue of a person, adding a surreal and artistic element to the scene. The background consists of the interior of a vehicle, including a steering wheel and a window. The scene is set against a cloudy sky with a large, glowing statue of a person, which adds a surreal and artistic touch to the video. The statue is positioned behind the characters, creating a sense of depth and perspective. The main subjects are two animated blue characters. The first character is a police officer, wearing a blue uniform with a badge, a gun holster, and a walkie-talkie. The second character is a military officer, also in a blue uniform with a badge, a gun holster, and a walkie-talkie. They are positioned side by side, facing each other, and engaging in a conversation. The police officer is holding a walkie-talkie to his ear, while the military officer is gesturing with his hand. The camera is stationary, providing a fixed view of the characters and the background. The view is a medium shot, capturing the upper bodies of the characters and the background elements.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/40A95zIfaR0_6/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/40A95zIfaR0_6/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/40A95zIfaR0_6/first_frame.png']}, 30), kwargs keys: dict_keys([])

Validation DataLoader 0:  31%|███       | 31/100 [00:00<00:00, 233.27it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['9AUfv6kR2xo_30'], 'prompt': ['a whimsical scene with animated Lego-style characters in a miniature town setting. The main focus is on a brown Lego horse with a stern expression and a yellow Lego truck with a cheerful face. The horse stands on the left side of the frame, while the truck is positioned on the right, both facing each other. The background consists of a charming Lego town with various buildings, trees, and a road. The scene is set in a Lego-style miniature town with a variety of buildings, including houses with red roofs and blue windows, a green tree, and a road with a white line. The background is detailed and colorful, adding to the whimsical and playful atmosphere of the video. The camera is stationary, providing a fixed view of the scene with a medium shot that captures both the horse and the truck in the frame.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/9AUfv6kR2xo_30/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/9AUfv6kR2xo_30/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/9AUfv6kR2xo_30/first_frame.png']}, 31), kwargs keys: dict_keys([])

Validation DataLoader 0:  32%|███▏      | 32/100 [00:00<00:00, 230.66it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['rA1UA6CvyM4_6'], 'prompt': ['a sequence of animated characters engaging in various activities, primarily involving food and drink.The characters are animated with exaggerated facial features and expressive eyes, giving them a cartoonish appearance. One character has a large, round head with a red headscarf and a yellow outfit, while another character has a slim build with a black and white striped shirt and a red headscarf. The characters exhibit a range of emotions, from curiosity to contentment, as they interact with each other and their surroundings.The setting appears to be a domestic interior, likely a kitchen or dining area, with a wooden floor and a white wall. There is a table with a vase of flowers, a fan, and a box of cookies visible in the background. The lighting is warm and soft, suggesting an indoor environment during the daytime.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/rA1UA6CvyM4_6/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/rA1UA6CvyM4_6/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/rA1UA6CvyM4_6/first_frame.png']}, 32), kwargs keys: dict_keys([])

Validation DataLoader 0:  33%|███▎      | 33/100 [00:00<00:00, 235.82it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['2iwm0Cvb14I_8'], 'prompt': ["A plush toy of Kirby, a popular video game character, is positioned on top of a Nintendo Wii game controller box. The toy is dressed in a yellow and black outfit with a black collar and red mouth. The toy is animated, moving its arms and head in a playful manner, as if it is interacting with the camera or the viewer. The background consists of a white door and a white wall, creating a simple and clean setting. The main subject is a plush toy of Kirby, a yellow character with a red mouth and black and yellow striped collar. Kirby is positioned on top of a Nintendo Wii game controller box, which is placed on a white surface. Kirby's arms and head move dynamically, simulating a playful and animated interaction. The toy's movements are lively and engaging, with its arms and head moving in various directions, simulating a playful and animated interaction. The movements are moderate in amplitude, with a medium speed, creating a sense of liveliness and engagement. The background remains static throughout the video. The camera is stationary, providing a consistent view of the scene from a slightly elevated angle, capturing the entirety of Kirby and the game controller box."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/2iwm0Cvb14I_8/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/2iwm0Cvb14I_8/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/2iwm0Cvb14I_8/first_frame.png']}, 33), kwargs keys: dict_keys([])

Validation DataLoader 0:  34%|███▍      | 34/100 [00:00<00:00, 240.50it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['jc-MkTbDUFQ_10'], 'prompt': ["A young boy is standing in a kitchen, holding a bottle of Tabasco sauce. He is wearing a gray t-shirt with a Star Wars design and appears to be excitedly showing off the bottle. The boy is animatedly talking and gesturing with the bottle, which is prominently displayed in his hands. The kitchen is cluttered with various items, including a stove, sink, and utensils, creating a lively and busy atmosphere. The main subject is a young boy, approximately 6-8 years old, wearing a gray t-shirt with a Star Wars design. He is holding a bottle of Tabasco sauce, which is bright red with a green and yellow label featuring a cartoonish green pepper. The boy is actively moving the bottle around, showing it off to the camera. His expressions and gestures indicate excitement and enthusiasm. The boy is moving the bottle of Tabasco sauce around in various directions, including up, down, and sideways. His movements are quick and enthusiastic, indicating excitement. The background remains static, with no significant changes or movements. The camera is stationary, focusing on the boy and the bottle of Tabasco sauce. The view is a medium close-up, capturing the boy's upper body and the bottle clearly."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/jc-MkTbDUFQ_10/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/jc-MkTbDUFQ_10/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/jc-MkTbDUFQ_10/first_frame.png']}, 34), kwargs keys: dict_keys([])

Validation DataLoader 0:  35%|███▌      | 35/100 [00:00<00:00, 245.57it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['SQONLdb1gow_21'], 'prompt': ["A man is giving a presentation or a talk in a well-lit room with a colorful background featuring educational posters and various objects. He is wearing a blue and white checkered shirt and is standing behind a wooden desk with a laptop, a notebook, a pen, and a small robotic arm. The man uses expressive hand gestures to emphasize his points, occasionally looking at the camera and speaking directly to the audience. The background consists of a colorful educational poster with various scientific and educational illustrations, including charts, diagrams, and cartoon-like characters. The poster is mounted on a wooden shelf with additional educational items such as books, a globe, and a small robotic arm. The setting appears to be a classroom or a study area, with a warm and inviting atmosphere. The main subject is a man with a beard, wearing a blue and white checkered shirt. He is positioned behind a wooden desk, facing the camera. His hands are often raised, gesturing as he speaks, and he occasionally looks at the camera. On the desk, there is a laptop, a notebook, a pen, and a small robotic arm. The man's movements are dynamic, involving hand gestures and occasional head movements to engage with the audience. The camera is stationary, capturing a medium shot of the man from the waist up, ensuring that both the subject and the background are clearly visible."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/SQONLdb1gow_21/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/SQONLdb1gow_21/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/SQONLdb1gow_21/first_frame.png']}, 35), kwargs keys: dict_keys([])

Validation DataLoader 0:  36%|███▌      | 36/100 [00:00<00:00, 250.62it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['M5Mof0JAUI0_387'], 'prompt': ['two animated characters, a boy and a girl, interacting in a cluttered room filled with various items. The boy, wearing a green shirt and black shorts, and the girl, wearing a yellow shirt and red shorts, engage in a playful conversation and gesture towards each other. The scene is lively and filled with expressions of joy and curiosity, as the characters move around and interact with the objects around them. The main subjects are two animated characters, a boy and a girl. The boy has a round head with a tuft of hair on top, wears a green shirt, black shorts, and has a cheerful expression. The girl has a similar round head with a tuft of hair, wears a yellow shirt, red shorts, and also has a cheerful expression. They are positioned centrally in the frame, facing each other, and engage in a playful interaction, with the boy often pointing and the girl responding with gestures and expressions. The background is a cluttered room filled with various items, including boxes, toys, and other miscellaneous objects. The room appears to be a storage or play area, with a mix of organized and disorganized items. The lighting is bright and even, suggesting an indoor setting. The overall scene is colorful and lively, with a focus on the characters and their interactions. The camera is static, providing a medium shot that captures both characters and the background in a single frame, allowing for a clear view of their interactions and expressions.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/M5Mof0JAUI0_387/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/M5Mof0JAUI0_387/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/M5Mof0JAUI0_387/first_frame.png']}, 36), kwargs keys: dict_keys([])

Validation DataLoader 0:  37%|███▋      | 37/100 [00:00<00:00, 255.61it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['KTzUdU-XkBE_90'], 'prompt': ['a paleoanthropologist in a white lab coat, initially seen in a contemplative pose with his hand on his chin, then transitioning to holding a large, detailed skull model. The scene is set in a modern laboratory with a blue background and a grid-like window. The paleoanthropologist is then joined by a 3D animated caveman character, who holds the skull model and gazes at it with curiosity. The caveman character is depicted with a muscular build, furry hair, and a neutral expression. The background is a modern laboratory setting with a blue backdrop and a grid-like window. The lighting is bright and even, highlighting the subjects and the skull model. The scene is clean and minimalistic, focusing on the subjects without any additional distractions. The camera remains stationary with a medium shot, capturing both the paleoanthropologist and the caveman character in the frame. The view is consistent, maintaining a clear focus on the subjects and their interaction with the skull model.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/KTzUdU-XkBE_90/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/KTzUdU-XkBE_90/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/KTzUdU-XkBE_90/first_frame.png']}, 37), kwargs keys: dict_keys([])

Validation DataLoader 0:  38%|███▊      | 38/100 [00:00<00:00, 260.52it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['HJVRvU7YnnM_25'], 'prompt': ["a group of animated characters engaged in a playful interaction by a pool of water. The characters are anthropomorphic, resembling Lego-like figures with exaggerated facial expressions and dynamic poses. They are dressed in various costumes, including armor and robes, and are positioned around the pool, creating a lively and engaging scene. The characters interact with each other, sometimes embracing, sometimes playfully fighting, and at times, looking at the camera with expressions of surprise or excitement. The background features a stone-walled pool area with a waterfall in the distance, adding to the fantasy setting. The characters exhibit a range of movements, including walking, embracing, playfully fighting, and looking at the camera. The movements are dynamic and varied, with medium to large amplitude, and moderate speed. The background remains static, with the waterfall providing a consistent element of motion. The camera remains mostly static, focusing on the characters and their interactions, with occasional slight pans to follow the characters' movements."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/HJVRvU7YnnM_25/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/HJVRvU7YnnM_25/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/HJVRvU7YnnM_25/first_frame.png']}, 38), kwargs keys: dict_keys([])

Validation DataLoader 0:  39%|███▉      | 39/100 [00:00<00:00, 265.34it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['RMu4Oc8ZxPw_1'], 'prompt': ["two men engaging in a lively discussion about a fantasy-themed game, specifically Game of Thrones. They are seated in a room adorned with various Game of Thrones memorabilia, including posters, figurines, and books. The men are animated and expressive, frequently gesturing and laughing as they discuss the game's plot and characters. The setting is casual and relaxed, with a focus on the enthusiasm and camaraderie shared by the two friends. The main subjects are two men, one with a beard and glasses, and the other with a beard and a purple shirt. They are seated side by side, facing the camera, and are engaged in a conversation about Game of Thrones. The man on the left frequently gestures with his hands, while the man on the right plays a guitar and occasionally speaks. The figurines and books on the table in front of them are also significant objects, adding to the immersive atmosphere of the scene. The background is a room decorated with Game of Thrones memorabilia, including posters, figurines, and books. The walls are adorned with various Game of Thrones-related items, creating a themed environment. The room is well-lit, with a warm and inviting ambiance. The camera is stationary, providing a clear and stable view of the two men and their surroundings. The view is a medium shot, capturing both subjects and their immediate environment."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/RMu4Oc8ZxPw_1/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/RMu4Oc8ZxPw_1/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/RMu4Oc8ZxPw_1/first_frame.png']}, 39), kwargs keys: dict_keys([])

Validation DataLoader 0:  40%|████      | 40/100 [00:00<00:00, 267.46it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['_JM6pLZVT5c_46'], 'prompt': ["A young girl is playing with a pink plastic toy, which appears to be a small, cute, cartoonish character with a smiling face and a star on its head. She is holding the toy in her hands and interacting with it, showing it off to the camera. The girl is wearing a light-colored, patterned long-sleeve shirt and has long blonde hair. The background consists of a red and green striped wall, suggesting an indoor setting, possibly a playroom or a classroom. The main subject is a young girl with long blonde hair, wearing a light-colored, patterned long-sleeve shirt. She is holding a pink plastic toy, which is a small, cartoonish character with a smiling face and a star on its head. The girl is actively engaging with the toy, holding it up to the camera, and moving it around. The toy is the focal point of her interaction, and she is positioned centrally in the frame. The girl's movements are gentle and deliberate as she holds and manipulates the toy. She moves the toy around in various directions, showing it from different angles. Her hands move smoothly and slowly, indicating a calm and playful demeanor. The background remains static throughout the video, with no changes or movements. The camera is stationary, focusing on a medium close-up view of the girl and the toy. The camera angle is slightly tilted upwards, capturing the girl's upper body and the toy in detail."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/_JM6pLZVT5c_46/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/_JM6pLZVT5c_46/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/_JM6pLZVT5c_46/first_frame.png']}, 40), kwargs keys: dict_keys([])

Validation DataLoader 0:  41%|████      | 41/100 [00:00<00:00, 269.76it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['KNKKjoE7oUw_59'], 'prompt': ["a cartoon character performing a dance routine. The character is a stylized, animated figure with a large, exaggerated build, wearing a red jacket, white shirt, and grey pants. He is also wearing a red cap and orange sneakers. The character dances energetically, moving his arms and legs in a rhythmic manner. The background is a plain, light blue gradient, which keeps the focus on the dancing character. The main subject is a cartoon character, characterized by a large, exaggerated build, wearing a red jacket, white shirt, grey pants, a red cap, and orange sneakers. The character is positioned centrally in the frame and performs a dance routine, moving his arms and legs rhythmically. The character's movements are dynamic and energetic, involving various dance poses and gestures. The background is a plain, light blue gradient, providing a clean and uncluttered backdrop that emphasizes the dancing character. There are no additional objects or elements in the background, keeping the focus solely on the character. The camera is static, with a fixed view that centers the dancing character in the frame. There is no camera movement, ensuring that the focus remains on the character's dance routine."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/KNKKjoE7oUw_59/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/KNKKjoE7oUw_59/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/KNKKjoE7oUw_59/first_frame.png']}, 41), kwargs keys: dict_keys([])

Validation DataLoader 0:  42%|████▏     | 42/100 [00:00<00:00, 270.83it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['EbzxajP-a04_13'], 'prompt': ['a conversation between two animated characters in a simple, minimalistic office setting. The character on the left, wearing glasses and a white lab coat, appears to be explaining something to the character on the right, who is dressed in a black shirt. The scene is set against a backdrop of a bookshelf filled with books, creating a professional and intellectual atmosphere. The main subjects are two animated characters. The character on the left has short black hair, wears glasses, a white lab coat, and a black shirt underneath. He is seated and appears to be speaking or explaining something. The character on the right has short hair, wears a black shirt, and is seated facing the left character. The two characters are positioned side by side, with the left character facing the camera and the right character facing the left character. The background consists of a simple office setting with a bookshelf filled with books. The bookshelf is made of wood and has a light brown color. The wall behind the bookshelf is a light yellow color, providing a clean and professional backdrop. The scene is well-lit, suggesting an indoor environment. The camera is stationary, providing a medium shot of the two characters from a frontal view. There is no camera movement, maintaining a steady and focused perspective on the conversation.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/EbzxajP-a04_13/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/EbzxajP-a04_13/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/EbzxajP-a04_13/first_frame.png']}, 42), kwargs keys: dict_keys([])

Validation DataLoader 0:  43%|████▎     | 43/100 [00:00<00:00, 271.41it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['ZnAzwHVM_YY_86'], 'prompt': ["a cheerful animated train character, designed with bright colors and expressive eyes, moving along a track in a lush, green environment. The train, adorned with a colorful hat and a friendly smile, travels smoothly on the tracks, passing by various trees and a red signpost. The scene is vibrant and playful, with the train's movement creating a sense of joy and adventure. The background consists of a lush, green landscape with rolling hills and scattered trees. The scene is bright and colorful, with a clear blue sky and a few fluffy white clouds. The greenery is vibrant, with trees and bushes adding depth and texture to the environment. The train moves smoothly and steadily along the tracks from left to right. The movement is consistent and moderate in speed, creating a sense of calm and enjoyment. The background remains static, with no significant changes or movements, emphasizing the train's journey. The main subject is a cheerful train character with large, expressive eyes and a friendly smile. The train is blue with a yellow and red hat, and it moves along a track. The train's wheels are visible, and it maintains a consistent speed and direction throughout the video. The train is positioned centrally in the frame, moving from left to right. The camera remains stationary, providing a consistent view of the train's journey along the track, capturing the entire scene in a wide-angle shot."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/ZnAzwHVM_YY_86/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/ZnAzwHVM_YY_86/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/ZnAzwHVM_YY_86/first_frame.png']}, 43), kwargs keys: dict_keys([])

Validation DataLoader 0:  44%|████▍     | 44/100 [00:00<00:00, 272.69it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['jbSjadzxgWA_2'], 'prompt': ['A group of animated characters, including a cat in a suit and tie, a dog in a suit and tie, and various other animals dressed in human clothing, are seen walking through a lobby area. They are carrying luggage and walking towards a counter where a bartender is standing. The scene is set in a bright, spacious lobby with large windows, a clock on the wall, and a counter with a menu board. The animals are dressed in human clothing, with the cat wearing a suit and tie, and the dog wearing a suit and tie as well. The other animals are dressed in various human-like outfits, such as a pig in a hat and a purple dress, a bear in a suit, and a dog in a hat and a red dress. They are carrying luggage, including suitcases and backpacks, and walking in a line towards the counter. The background features a bright, spacious lobby with large windows that allow natural light to flood the area. There is a clock on the wall, a counter with a menu board, and a few chairs. The overall setting is clean and modern, with a minimalist design. The camera is stationary, providing a wide-angle view of the scene from a slightly elevated perspective, capturing the entire lobby and the movement of the characters within it.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/jbSjadzxgWA_2/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/jbSjadzxgWA_2/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/jbSjadzxgWA_2/first_frame.png']}, 44), kwargs keys: dict_keys([])

Validation DataLoader 0:  45%|████▌     | 45/100 [00:00<00:00, 275.73it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['uA1OOFODQGE_225'], 'prompt': ["a whimsical scene featuring a friendly interaction between a small, cute, animated character and a large, cheerful dragon. The dragon, with vibrant red scales and expressive eyes, engages in playful gestures and facial expressions, while the smaller character, with large, round eyes and a cute, smiling face, responds with similar expressions of joy and curiosity. The background is a simple, flat surface with a few scattered objects, creating a minimalist and playful setting that emphasizes the characters' interaction. The main subjects are a small, cute, animated character and a large, cheerful dragon. The small character has large, round eyes with heart-shaped pupils, a small nose, and a smiling face. It wears a pink outfit with a white bow on its head. The dragon has red, scaly skin, large, expressive eyes with purple pupils, and a friendly smile. The dragon's large, expressive eyes and open mouth convey a sense of joy and playfulness. The dragon's arms are raised and slightly outstretched, and it occasionally opens its mouth as if speaking or singing. The small character stands close to the dragon, often looking up at it with a curious and happy expression. The background is a simple, flat surface with a few scattered objects, including a fish and some rocks. The scene is set against a plain, light-colored backdrop, which helps to keep the focus on the main characters. The objects are positioned in a way that suggests a natural, outdoor setting, with the fish floating in the water and the rocks scattered around. The camera is static, providing a consistent, eye-level view of the characters, ensuring that the focus remains on their interaction and expressions."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/uA1OOFODQGE_225/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/uA1OOFODQGE_225/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/uA1OOFODQGE_225/first_frame.png']}, 45), kwargs keys: dict_keys([])

Validation DataLoader 0:  46%|████▌     | 46/100 [00:00<00:00, 277.81it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['XL6HXRm8etA_13'], 'prompt': ["A man wearing a yellow face mask with a cartoon character design is sitting in the back seat of a car, engaging in a conversation with someone outside the frame. He is wearing a red shirt and gesturing with his hands, occasionally pointing towards the window. The car is parked in a sunny outdoor setting with trees and a bright sky visible through the window. The man's expressions and hand movements suggest he is explaining or discussing something with the person outside the car. The main subject is a man wearing a yellow face mask with a cartoon character design and a red shirt. He is seated in the back seat of a car, facing towards the front. His hands are frequently in motion, gesturing and pointing towards the window. The man's expressions and hand movements indicate he is actively communicating or explaining something. The background consists of the interior of a car, with the back seat visible. The car is parked outdoors, with trees and a bright sky visible through the window. The sunlight creates a bright and warm atmosphere inside the car. The man's movements are primarily hand gestures, including pointing and waving. His hands move in various directions, primarily towards the window. The background remains static, with no significant changes or movements. The camera is stationary, capturing a medium shot of the man from the back seat of the car. The view is slightly angled to include both the man and the window, providing a clear view of his gestures and expressions."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/XL6HXRm8etA_13/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/XL6HXRm8etA_13/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/XL6HXRm8etA_13/first_frame.png']}, 46), kwargs keys: dict_keys([])

Validation DataLoader 0:  47%|████▋     | 47/100 [00:00<00:00, 279.08it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['awlQ64AU0U8_8'], 'prompt': ["a cartoon bear character with a friendly and expressive face. The bear wears round glasses and has a cheerful demeanor, often smiling and making eye contact with the viewer. The background is a lush, green field with tall grass, suggesting a natural, outdoor setting. The video has a playful and engaging style, with the bear character being the central focus and the background providing a serene, natural backdrop. The main subject is a cartoon bear character with a light brown fur coat, round glasses, and a cheerful expression. The bear is positioned centrally in the frame and remains the focal point throughout the video. The bear's movements are subtle, mainly involving slight head tilts and changes in facial expressions, such as smiling and blinking. The background consists of a green, grassy field with tall, blurred grass, indicating a natural, outdoor setting. The scene is bright and vibrant, with the greenery providing a serene and inviting atmosphere. There are no other objects or characters in the background, keeping the focus on the bear. The camera is static, with a fixed view that centers on the bear character, providing a clear and consistent perspective throughout the video."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/awlQ64AU0U8_8/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/awlQ64AU0U8_8/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/awlQ64AU0U8_8/first_frame.png']}, 47), kwargs keys: dict_keys([])

Validation DataLoader 0:  48%|████▊     | 48/100 [00:00<00:00, 279.37it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['6TBcCQCG_ZY_19'], 'prompt': ["a person's hand drawing a character on a computer screen using a stylus. The character being drawn is a female with a confident expression, wearing a futuristic outfit and holding a futuristic gun. The drawing process is captured in a close-up view, focusing on the hand movements and the evolving character design. The background is a plain white canvas, emphasizing the drawing process. The main subject is a female character being drawn on a computer screen. She has a confident expression, wearing a futuristic outfit consisting of a gray jacket, a white fur-lined hood, and a futuristic gun holstered on her hip. Her hair is styled in a ponytail, and she wears futuristic glasses. The drawing hand, which is the only visible part of the person, uses a stylus to draw the character. The hand moves with precision, adding details to the character's face and outfit. The drawing hand moves with deliberate and precise strokes, adding details to the character's face and outfit. The movements are smooth and controlled, with the stylus moving horizontally and vertically to create different elements of the character. The background remains static throughout the video, focusing solely on the drawing process. The camera is stationary, providing a close-up view of the drawing process. The focus remains on the hand and the computer screen, with no additional camera movements."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/6TBcCQCG_ZY_19/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/6TBcCQCG_ZY_19/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/6TBcCQCG_ZY_19/first_frame.png']}, 48), kwargs keys: dict_keys([])

Validation DataLoader 0:  49%|████▉     | 49/100 [00:00<00:00, 281.19it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['XRYFzN9jM8Y_9'], 'prompt': ['a humorous scene with three animated characters interacting with a large, round, and slightly burnt cake. The characters are anthropomorphic, with exaggerated facial expressions and movements. The main character, an orange creature with a large, round body and a wide smile, holds the cake and offers it to the other two characters. The other two characters, a green creature with a wide-open mouth and a purple creature with a large, round body and a wide smile, eagerly reach out towards the cake. The scene takes place in a simple, cartoonish setting with a purple door and a wooden floor. The background is a simple, cartoonish setting with a purple door and a wooden floor. The door has a round, silver knob and a small, round window. The floor is made of wooden planks with a warm, brown color. The scene is well-lit with a soft, even light that highlights the characters and the cake. The main subjects are three animated characters. The first character is an orange creature with a large, round body, a wide smile, and a large, round head. It holds a large, round, slightly burnt cake in its hands. The second character is a green creature with a wide-open mouth, large eyes, and a small, round body. The third character is a purple creature with a large, round body, a wide smile, and a large, round head. The characters are positioned in front of a purple door, with the orange character holding the cake in the center and the green and purple characters reaching out towards it. The camera is stationary, providing a medium shot that captures the characters and the door in the frame.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/XRYFzN9jM8Y_9/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/XRYFzN9jM8Y_9/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/XRYFzN9jM8Y_9/first_frame.png']}, 49), kwargs keys: dict_keys([])

Validation DataLoader 0:  50%|█████     | 50/100 [00:00<00:00, 281.44it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['IO9GVbyQtP4_5'], 'prompt': ["A person is drawing a cute, cartoon-style character with a bunny ear and a flower on its head. The character has large, expressive eyes and a cheerful expression. The drawing process is captured in a close-up view, focusing on the hand holding a pink marker and the character being drawn. The background is plain white, emphasizing the drawing and the vibrant colors used. The main subject is a cartoon character with a bunny ear, a flower on its head, and large, expressive eyes. The character is drawn with a pink marker by a hand holding the marker. The character is positioned centrally in the frame, and the drawing progresses with the marker moving across the character's face and body. The background is a plain white surface, providing a clean and uncluttered backdrop that highlights the drawing. There are no additional objects or elements in the background, ensuring that the focus remains on the drawing process. The camera is stationary, providing a close-up view of the drawing process. The angle is slightly above the character, capturing the hand, marker, and character in a clear and focused manner."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/IO9GVbyQtP4_5/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/IO9GVbyQtP4_5/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/IO9GVbyQtP4_5/first_frame.png']}, 50), kwargs keys: dict_keys([])

Validation DataLoader 0:  51%|█████     | 51/100 [00:00<00:00, 282.55it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['TPsK47PAmWs_59'], 'prompt': ["A small, animated character, dressed in a purple hooded outfit with a white undershirt and pink shoes, is seen interacting with a stack of books. The character stands on a pile of books, reaching out and touching the books, and then bending down to pick up a book. The scene is set in a cozy, dimly lit room with a window in the background, adding a soft, warm light to the setting. The character's expressions and movements are animated and lively, creating a playful and engaging atmosphere. The main subject is a small, animated character with blonde hair, wearing a purple hooded outfit with a white undershirt and pink shoes. The character is positioned on top of a stack of books, standing and interacting with the books. The character's movements include reaching out to touch the books, bending down to pick up a book, and standing up again. The character's expressions are animated and lively, adding a sense of playfulness and curiosity to the scene. The background features a cozy, dimly lit room with a large window that allows soft, warm light to filter in. The window is framed by wooden panels, and there are several books stacked on a wooden shelf. The room has a rustic and homely feel, with a few framed pictures and a small potted plant visible. The overall atmosphere is warm and inviting. The camera is stationary, providing a fixed view of the scene from a slightly low angle, capturing the character and the books in a clear and focused manner."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/TPsK47PAmWs_59/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/TPsK47PAmWs_59/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/TPsK47PAmWs_59/first_frame.png']}, 51), kwargs keys: dict_keys([])

Validation DataLoader 0:  52%|█████▏    | 52/100 [00:00<00:00, 284.16it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['nk4a-fMlOQE_10'], 'prompt': ["a cartoonish scene featuring two animated characters, a boy and a girl, engaging in skateboarding activities. The boy, wearing a brown shirt and a hat, is seen performing various skateboard tricks, including grinding on a ramp and jumping over obstacles. The girl, dressed in a red shirt and a hat, is also seen skateboarding and attempting tricks. The background is a simple, flat landscape with a warm, orange hue, giving the scene a cozy and inviting atmosphere. The main subjects are two animated characters, a boy and a girl. The boy is wearing a brown shirt and a hat, and he is actively skateboarding, performing tricks such as grinding on a ramp and jumping over obstacles. The girl is wearing a red shirt and a hat, and she is also skateboarding, attempting tricks, and occasionally falling. The boy and girl are positioned in the foreground, with the boy often in the center of the frame and the girl to his right. The boy's movements are dynamic and energetic, involving skateboarding tricks such as grinding on a ramp and jumping over obstacles. His actions are quick and fluid, with a focus on performing the tricks. The girl's movements are more varied, including skateboarding, attempting tricks, and occasionally falling. The background remains static, with no changes or movements. The camera is stationary, providing a fixed view of the scene, focusing on the characters and their activities."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/nk4a-fMlOQE_10/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/nk4a-fMlOQE_10/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/nk4a-fMlOQE_10/first_frame.png']}, 52), kwargs keys: dict_keys([])

Validation DataLoader 0:  53%|█████▎    | 53/100 [00:00<00:00, 284.57it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['u-MHbxxI1P0_35'], 'prompt': ['A person is holding a framed artwork in front of them, displaying it to the camera.The individual has blonde hair and is wearing a gray sweater with a cartoonish design featuring various animals. Their nails are painted in a turquoise shade. The framed artwork features a stylized illustration of two characters, one resembling a bear and the other a human-like figure, set against a snowy backdrop. The characters appear to be engaged in a conversation or interaction.The setting is an indoor space with a colorful wall in the background adorned with various illustrations and posters. The lighting is bright, suggesting an indoor environment with ample natural or artificial light.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/u-MHbxxI1P0_35/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/u-MHbxxI1P0_35/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/u-MHbxxI1P0_35/first_frame.png']}, 53), kwargs keys: dict_keys([])

Validation DataLoader 0:  54%|█████▍    | 54/100 [00:00<00:00, 281.70it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['ITXU3SzUq6s_74'], 'prompt': ["a whimsical scene featuring two animated bunny characters, one purple and one green, sitting on a colorful blanket in a garden. The purple bunny is initially sleeping, while the green bunny looks around curiously. As the video progresses, the green bunny becomes more animated, eventually waking up and interacting with the purple bunny. The scene is set in a lush, vibrant garden with blooming trees and colorful decorations, creating a cheerful and playful atmosphere. The main subjects are two animated bunny characters. The purple bunny is initially sleeping, lying on its side with its eyes closed, and is positioned on the left side of the blanket. The green bunny is initially stationary but becomes more animated, eventually waking up and sitting up on the blanket. The green bunny is positioned on the right side of the blanket. The purple bunny remains mostly stationary, while the green bunny exhibits more movement, including waking up and interacting with the purple bunny. The green bunny's movements are moderate in amplitude, primarily involving its head and upper body, with a slow to moderate speed. The background remains static throughout the video. The camera is stationary, providing a fixed view of the scene from a slightly elevated angle, capturing the entirety of the blanket and the bunnies within it."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/ITXU3SzUq6s_74/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/ITXU3SzUq6s_74/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/ITXU3SzUq6s_74/first_frame.png']}, 54), kwargs keys: dict_keys([])

Validation DataLoader 0:  55%|█████▌    | 55/100 [00:00<00:00, 278.67it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['BJga7tYsMQM_12'], 'prompt': ["a person interacting with a tablet, specifically navigating through a game or application.The individual in the video is not fully visible, with only their hand and part of their arm being shown. The hand appears to be that of an adult, with fair skin and no distinctive jewelry or markings. The person's attire is not visible, but the hand is wearing a dark-colored sleeve, suggesting a long-sleeved garment.The setting is an indoor environment, likely a home or office, with a wooden desk or table in the background. The tablet is the main focus, with its screen displaying a colorful, cartoonish game or application. The background of the screen shows a bright, yellowish sky with a blue horizon, and a greenish-yellow ground with a cartoonish character riding a motorcycle. The character is wearing a red helmet and a blue shirt, and the motorcycle has a red frame and yellow wheels. The screen also shows various buttons and icons, including a lock symbol, a gear icon, and a red cross, indicating different functions or options within the game or application."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/BJga7tYsMQM_12/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/BJga7tYsMQM_12/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/BJga7tYsMQM_12/first_frame.png']}, 55), kwargs keys: dict_keys([])

Validation DataLoader 0:  56%|█████▌    | 56/100 [00:00<00:00, 278.02it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['3VEqIEn6F1s_12'], 'prompt': ["A young woman with long blonde hair and makeup is sitting on a white couch in a cozy room. She is wearing a black sleeveless top and a necklace with a green stone. The woman is animatedly talking and gesturing with her hands, occasionally pointing at something off-camera. In the background, there are two cartoon mice, one on each side of the frame, which appear to be interacting with her. The woman's expressions and gestures suggest she is engaged in a lively conversation or storytelling. The main subject is a young woman with long blonde hair, wearing a black sleeveless top and a necklace with a green stone. She is seated on a white couch and is actively talking and gesturing with her hands. The two cartoon mice, one on each side of the frame, are positioned behind her and appear to be interacting with her, possibly responding to her gestures. The woman's movements are dynamic and expressive, involving her hands and facial expressions. She points, gestures, and occasionally looks off-camera, indicating she is engaged in a conversation or storytelling. The mice remain relatively stationary, only moving slightly in response to her gestures. The background remains static throughout the video. The camera is stationary, providing a medium close-up view of the woman and the mice. The focus remains on the woman, capturing her expressions and gestures clearly."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/3VEqIEn6F1s_12/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/3VEqIEn6F1s_12/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/3VEqIEn6F1s_12/first_frame.png']}, 56), kwargs keys: dict_keys([])

Validation DataLoader 0:  57%|█████▋    | 57/100 [00:00<00:00, 278.03it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['VaExQJCCG7I_39'], 'prompt': ['two animated characters, a blue pill and a yellow pill, engaging in a playful interaction on a sandy desert landscape. The blue pill, wearing a top hat and a mustache, holds a green bottle and pours its contents into the yellow pill, which lies on its side. The yellow pill reacts with a surprised and happy expression, waving its arms and smiling broadly. The scene is set against a bright yellow sky with fluffy white clouds, adding a cheerful and whimsical atmosphere to the video. The background is a simple, bright yellow sky with scattered white clouds, giving a cheerful and sunny desert atmosphere. The sandy ground beneath the characters is light brown, with a few small cacti visible in the distance, enhancing the desert setting. The overall scene is devoid of any other objects or characters, focusing solely on the two main subjects. The main subjects are the blue pill and the yellow pill. The blue pill is anthropomorphized with a top hat, mustache, and a green bottle in its hand. It is positioned on the left side of the frame, pouring the liquid from the bottle into the yellow pill. The yellow pill is lying on its side, with a surprised and happy expression, waving its arms and smiling. The blue pill and yellow pill interact closely, with the blue pill pouring the liquid into the yellow pill. The camera is static, providing a fixed view of the scene from a slightly elevated angle, capturing the entire interaction between the two pills without any movement or zooming.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/VaExQJCCG7I_39/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/VaExQJCCG7I_39/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/VaExQJCCG7I_39/first_frame.png']}, 57), kwargs keys: dict_keys([])

Validation DataLoader 0:  58%|█████▊    | 58/100 [00:00<00:00, 279.80it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['-kegemta74A_441'], 'prompt': ["an animated character, a brown monkey with a wide-eyed expression, suspended in mid-air while holding onto a large, metallic, spiral object. The monkey is also holding a green crab with a yellow shell in its other hand. The scene takes place against a bright, blue sky with a lush green forest in the background. The monkey appears to be in a playful or adventurous situation, possibly engaging in a comedic or whimsical activity. The background consists of a bright blue sky with no visible clouds, suggesting a clear and sunny day. Below the sky, there is a lush green forest with various types of trees and foliage. The scene is set in a tropical or jungle environment, adding to the playful and adventurous atmosphere of the video. The main subject is a brown monkey with large, expressive eyes and a wide-open mouth, indicating surprise or excitement. The monkey is holding onto a large, metallic, spiral object with its right hand and a green crab with a yellow shell in its left hand. The monkey's body is positioned horizontally, and it appears to be suspended in mid-air. The monkey's facial expressions and body language convey a sense of playfulness and surprise. The camera is static, providing a clear and focused view of the monkey and its actions. The camera angle is slightly above the monkey, giving a bird's-eye view of the scene."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/-kegemta74A_441/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/-kegemta74A_441/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/-kegemta74A_441/first_frame.png']}, 58), kwargs keys: dict_keys([])

Validation DataLoader 0:  59%|█████▉    | 59/100 [00:00<00:00, 283.34it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['kbc8KyRjsRQ_91'], 'prompt': ['Two men are engaged in a lively conversation on a brightly lit stage. One man, wearing a brown jacket and a maroon shirt, gestures animatedly with his hands while speaking. The other man, dressed in a pink shirt and a brown blazer, listens attentively and occasionally responds with hand gestures. The background features a colorful, animated scene with various characters and objects, suggesting a festive or celebratory atmosphere. The main subjects are two men. The man on the left has dark hair, wears glasses, and is dressed in a brown jacket over a maroon shirt. He is actively gesturing with his hands, indicating he is speaking or explaining something. The man on the right has short hair, wears glasses, and is dressed in a pink shirt and a brown blazer. He is listening attentively and occasionally gestures with his hands. They are positioned close to each other, facing the camera, and appear to be engaged in a friendly conversation. The background features a vibrant, animated scene with various characters and objects. The characters are depicted in a cartoonish style, with bright colors and exaggerated features. The objects include balloons, a kite, and other festive elements, contributing to a cheerful and celebratory atmosphere. The background is set against a yellow and orange gradient, enhancing the lively and energetic feel of the scene. The camera is stationary, capturing the two men from a medium shot, allowing both subjects and the background to be clearly visible.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/kbc8KyRjsRQ_91/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/kbc8KyRjsRQ_91/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/kbc8KyRjsRQ_91/first_frame.png']}, 59), kwargs keys: dict_keys([])

Validation DataLoader 0:  60%|██████    | 60/100 [00:00<00:00, 286.83it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['4HJj5vW6Vak_74'], 'prompt': ["A colorful, animated vehicle, resembling a vintage car, drives down a road lined with trees and scattered with variously colored eggs.The vehicle is brightly colored with a yellow body, a red roof, and a yellow and red striped pattern on the side. It has large, round, colorful wheels and a whimsical design with a face on the front, featuring a large, round, yellow eye and a small, red mouth. The car's windows are adorned with colorful, striped patterns.The setting is a whimsical, cartoonish landscape with a road that appears to be made of wooden planks. The road is lined with trees that have a festive appearance, decorated with colorful ornaments and wrapped in ribbons. Scattered along the road are variously colored eggs, adding to the festive atmosphere. The sky is a bright blue with fluffy white clouds, suggesting a cheerful, sunny day."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/4HJj5vW6Vak_74/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/4HJj5vW6Vak_74/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/4HJj5vW6Vak_74/first_frame.png']}, 60), kwargs keys: dict_keys([])

Validation DataLoader 0:  61%|██████    | 61/100 [00:00<00:00, 290.37it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['RmT6xEx_bh8_13'], 'prompt': ["a whimsical scene featuring two animated characters, a pink cat and a blue dog, in a rustic outdoor setting. The cat, with a large, expressive face and a yellow bow on its head, is seated on a green chair, diligently washing its paws in a bucket filled with green leaves. The dog, with a similar large, expressive face and a red nose, is standing nearby, observing the cat's actions with a curious expression. The setting is a wooden structure with a stone wall, giving a rustic and charming atmosphere. The background consists of a rustic wooden structure with a stone wall, suggesting an outdoor or farm-like setting. The ground is covered with dirt and scattered leaves, adding to the natural and earthy ambiance. There are no other significant objects or characters in the background, keeping the focus on the two main characters. The main subjects are a pink cat and a blue dog. The cat is seated on a green chair, holding a bucket of green leaves and washing its paws. The cat's facial expressions are animated and expressive, showing concentration and slight discomfort. The dog stands nearby, watching the cat with a curious and slightly amused expression. The dog's posture is relaxed, with its body slightly leaning forward. The camera is stationary, providing a medium shot that captures both characters and the surrounding environment. The view is at eye level, giving a clear and engaging perspective of the scene."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/RmT6xEx_bh8_13/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/RmT6xEx_bh8_13/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/RmT6xEx_bh8_13/first_frame.png']}, 61), kwargs keys: dict_keys([])

Validation DataLoader 0:  62%|██████▏   | 62/100 [00:00<00:00, 293.84it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['6sAIaI2H0wM_176'], 'prompt': ['a playful and colorful scene of two animated characters, a red and yellow snail and a green and yellow fish, interacting with a black trash can filled with various items. The snail and fish are positioned on the ground next to the trash can, which is covered in a mix of trash and colorful items. The scene is set against a simple, industrial background with a gray wall and a concrete floor. The characters exhibit animated expressions and movements, creating a lively and engaging atmosphere. The background consists of a simple, industrial setting with a gray wall and a concrete floor. The trash can is the main object in the scene, filled with various items including a green and yellow fish, a red and yellow snail, a blue and yellow cup, a green and yellow spoon, and a black plastic bag. The scene is well-lit, with a focus on the characters and the trash can, creating a clear and detailed visual. The main subjects are a red and yellow snail and a green and yellow fish. The snail is positioned on the left side of the frame, while the fish is on the right. Both characters have expressive faces and are animated, with the snail appearing to be excited and the fish looking surprised. The trash can, which is filled with various items, is positioned behind the characters, with the items spilling out onto the ground. The characters interact with each other and the trash can, creating a dynamic scene. The camera is stationary, providing a fixed view of the scene from a slightly elevated angle, capturing the characters and the trash can in a clear and detailed manner.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/6sAIaI2H0wM_176/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/6sAIaI2H0wM_176/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/6sAIaI2H0wM_176/first_frame.png']}, 62), kwargs keys: dict_keys([])

Validation DataLoader 0:  63%|██████▎   | 63/100 [00:00<00:00, 297.22it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['wHDLuEl81jw_104'], 'prompt': ["a cartoonish animated character with exaggerated facial features and a menacing expression. The character, dressed in a black suit with a red tie, is depicted in various dynamic poses and expressions, suggesting a playful and slightly mischievous demeanor. The character interacts with a large, round, white object, which appears to be a part of the character's body, and at times, the character seems to be playfully engaging with an unseen person or object. The background is a dark, nighttime setting with a few illuminated windows and a silhouette of a building, adding to the mysterious and slightly eerie atmosphere. The main subject is a cartoonish animated character with exaggerated facial features, including large eyes, a wide-open mouth, and a menacing expression. The character is dressed in a black suit with a red tie and has spiky black hair. The character is positioned centrally in the frame and interacts with a large, round, white object, which appears to be part of the character's body. The character's movements are playful and mischievous, often involving reaching out towards the object or the unseen person or object. The camera is static, focusing on the character from a medium close-up view, capturing the character's expressions and movements clearly."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/wHDLuEl81jw_104/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/wHDLuEl81jw_104/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/wHDLuEl81jw_104/first_frame.png']}, 63), kwargs keys: dict_keys([])

Validation DataLoader 0:  64%|██████▍   | 64/100 [00:00<00:00, 300.60it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['Z2g7fS7kdpc_497'], 'prompt': ['two animated characters, a cheerful orange character with a smiling face and a green character with large, expressive eyes, sitting at a desk in a classroom. The orange character is holding a pencil and appears to be drawing or writing, while the green character looks on with a mix of curiosity and amusement. The green character occasionally points at the orange character, adding to the playful interaction between them. The background is a classroom setting with a wooden desk, chairs, and a bulletin board filled with various pictures and papers. The scene is well-lit, suggesting it is daytime. The background includes a wooden desk with a few chairs, a bulletin board with various pictures and papers, and a window with light streaming in. The overall setting is a classroom, suggesting a learning environment. The main subjects are the two animated characters. The orange character has a smiling face with large eyes and a small, pointed nose, and is holding a pencil. The green character has large, expressive eyes with a surprised or amused expression, and is wearing a white shirt with a green collar. The green character points at the orange character, indicating a playful interaction. They are positioned side by side at a desk, with the orange character on the left and the green character on the right. The camera is stationary, providing a medium close-up view of the two characters at the desk, capturing their expressions and interactions clearly.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/Z2g7fS7kdpc_497/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/Z2g7fS7kdpc_497/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/Z2g7fS7kdpc_497/first_frame.png']}, 64), kwargs keys: dict_keys([])

Validation DataLoader 0:  65%|██████▌   | 65/100 [00:00<00:00, 303.98it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['PdzKq9P_EEg_443'], 'prompt': ['A cheerful animated character with curly hair and large expressive eyes is seated at a table, interacting with a butterfly. The character is wearing a purple hoodie with a star on the front and is holding a red spoon. The butterfly lands on a red plate with a white star pattern, and the character seems to be feeding it. The scene is set in a cozy room with a window in the background, providing a soft, natural light. The main subject is an animated character with curly hair, large expressive eyes, and a purple hoodie with a star on the front. The character is holding a red spoon and is seated at a table. A butterfly is the secondary subject, landing on a red plate with a white star pattern. The character and the butterfly interact closely, with the butterfly perched on the plate while the character looks at it and occasionally moves the spoon towards it. The background features a cozy room with a large window that allows natural light to fill the scene. The window shows a blurred view of greenery outside, suggesting a peaceful, outdoor setting. The room has a warm, inviting ambiance with soft, pastel colors. The camera is stationary, providing a clear, frontal view of the character and the table. The view is at eye level, focusing on the interaction between the character and the butterfly.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/PdzKq9P_EEg_443/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/PdzKq9P_EEg_443/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/PdzKq9P_EEg_443/first_frame.png']}, 65), kwargs keys: dict_keys([])

Validation DataLoader 0:  66%|██████▌   | 66/100 [00:00<00:00, 307.29it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['k3bYUA30nNY_31'], 'prompt': ['a family of five animated characters sitting together on a blue couch. The characters are positioned in a row, with the father on the left, the mother in the middle, and three children on the right. The family members are dressed in casual, comfortable clothing, and they all have blonde hair. The background is a simple, dark brown wall with a subtle texture. The main subjects are the five animated characters. The father is on the left, wearing a blue shirt and light blue pants with a pattern. The mother is in the middle, wearing a purple shirt and light blue pants with a pattern. The three children are on the right, each wearing a light blue shirt and light blue pants with a pattern. The mother and father are holding the children, creating a close-knit family scene. The characters are stationary, sitting on the couch. The couch is blue, and there is a small, dark brown table in front of the couch. The scene is set indoors, likely in a living room. The camera is stationary, providing a frontal view of the family sitting on the couch. The view is at eye level, capturing the entire family in a single frame.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/k3bYUA30nNY_31/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/k3bYUA30nNY_31/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/k3bYUA30nNY_31/first_frame.png']}, 66), kwargs keys: dict_keys([])

Validation DataLoader 0:  67%|██████▋   | 67/100 [00:00<00:00, 310.68it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['gJjCaClGUFU_16'], 'prompt': ["a person engaging in the activity of opening a small toy package and removing a toy from it.The individual's hands are the main focus, with visible fingers and nails, suggesting they are likely an adult. The hands are fair-skinned and appear to be of a light complexion. The toy being handled is a small, colorful figurine with a cartoonish design, featuring a character with a large head, large eyes, and a simplistic, cheerful expression. The figurine is wearing a green top with a yellow skirt and has a pink bow in its hair. The toy is held and manipulated with care, indicating it might be a collectible or a cherished item.The setting is minimalistic, with a plain white background that provides a neutral setting that does not distract from the action taking place. The focus is solely on the hands and the toy, with no other objects or elements in the immediate vicinity. The lighting is bright and even, suggesting an indoor environment with controlled lighting conditions."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/gJjCaClGUFU_16/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/gJjCaClGUFU_16/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/gJjCaClGUFU_16/first_frame.png']}, 67), kwargs keys: dict_keys([])

Validation DataLoader 0:  68%|██████▊   | 68/100 [00:00<00:00, 314.00it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['cXUdkJPsGhU_110'], 'prompt': ['a whimsical scene set in a kitchen with a playful twist. Two cartoonish characters, a small creature and a large jar, interact in a lighthearted manner. The small creature, resembling a mouse or a small animal, is initially seen standing near the jar, which is initially empty. As the video progresses, the jar begins to fill up with a liquid, and the small creature reacts with surprise and excitement. The scene is set in a kitchen with various kitchen items and decorations visible in the background. The kitchen items include a stove, a sink, and various kitchen utensils. The background is a light, warm color palette, giving the scene a cozy and inviting atmosphere. The small creature and the jar are the main subjects of the video. The small creature is positioned near the jar initially, and as the video progresses, the jar begins to fill up with a liquid. The small creature reacts with wide eyes and a surprised expression. The jar is initially empty but gradually fills up with a yellow liquid, which the small creature seems to be reacting to. The camera is stationary, providing a fixed view of the scene. The perspective is slightly angled, giving a clear view of both the small creature and the jar.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/cXUdkJPsGhU_110/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/cXUdkJPsGhU_110/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/cXUdkJPsGhU_110/first_frame.png']}, 68), kwargs keys: dict_keys([])

Validation DataLoader 0:  69%|██████▉   | 69/100 [00:00<00:00, 317.29it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['QpTmSVK6-qM_97'], 'prompt': ["a series of animated scenes depicting a group of flying saucers hovering over a mountainous landscape. The saucers are blue with glowing lights and appear to be in motion, flying in a line across the sky. The background consists of a vibrant, colorful sky with fluffy white clouds and a mountain range with green and purple hues. The video transitions to a scene inside a car, where a person is seen giving a thumbs-up gesture, indicating approval or satisfaction. The person is dressed in a dark suit and is seated in the driver's seat of the car. The video has a playful and whimsical style, with vibrant colors and a cartoonish animation style."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/QpTmSVK6-qM_97/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/QpTmSVK6-qM_97/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/QpTmSVK6-qM_97/first_frame.png']}, 69), kwargs keys: dict_keys([])

Validation DataLoader 0:  70%|███████   | 70/100 [00:00<00:00, 320.44it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['qoWS3UmHL6Q_3'], 'prompt': ["a whimsical animated character, a pirate with a large, curly orange beard and a black pirate hat, standing against a plain blue background. The pirate is dressed in a blue and yellow striped shirt, red pants, and a blue bow tie. He is animatedly waving his right hand and smiling, giving the impression of welcoming or greeting someone. The background remains static throughout the video, focusing entirely on the animated pirate. The main subject is a pirate character with a large, curly orange beard and a black pirate hat. He is wearing a blue and yellow striped shirt, red pants, and a blue bow tie. The pirate is animatedly waving his right hand and smiling, indicating a friendly and welcoming demeanor. He is positioned centrally in the frame, with his body facing the viewer and his arm extended outward. The pirate character exhibits smooth, animated movements. He waves his right hand in a friendly gesture, and his facial expressions change to convey a welcoming smile. The background remains static throughout the video, with no changes or movements. The pirate's movements are moderate in amplitude, primarily directed outward from his body, and executed at a steady, moderate speed. The camera is static, maintaining a fixed view of the animated pirate character from a frontal perspective. There is no camera movement, and the focus remains consistently on the pirate throughout the video."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/qoWS3UmHL6Q_3/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/qoWS3UmHL6Q_3/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/qoWS3UmHL6Q_3/first_frame.png']}, 70), kwargs keys: dict_keys([])

Validation DataLoader 0:  71%|███████   | 71/100 [00:00<00:00, 323.73it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['FdAmtd1tL5w_26'], 'prompt': ['a whimsical animated scene set on a cartoonish map with a large "X" in the center. Various characters, including a squirrel, a raccoon, a pirate, and a family, are positioned around the "X" and interact with each other. The characters are colorful and animated, with the pirate and family members dressed in costumes, and the squirrel and raccoon are depicted in a playful manner. The scene is set against a blue background with a compass and a rainbow-colored object in the foreground, adding to the cartoonish and playful atmosphere. The main subjects are the characters positioned around the large "X" on the map. The pirate, dressed in a hat and a red bandana, stands near the top of the "X." The family, consisting of a mother, father, and two children, are positioned near the bottom of the "X." The squirrel and raccoon are located near the top of the "X," with the squirrel wearing a red bandana and the raccoon wearing a red hat. The characters interact with each other, creating a lively and engaging scene. The camera is static, providing a clear and stable view of the entire scene. The camera angle is slightly elevated, offering a bird\'s-eye view of the map and the characters.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/FdAmtd1tL5w_26/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/FdAmtd1tL5w_26/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/FdAmtd1tL5w_26/first_frame.png']}, 71), kwargs keys: dict_keys([])

Validation DataLoader 0:  72%|███████▏  | 72/100 [00:00<00:00, 326.91it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['3tvNPOH1FaE_36'], 'prompt': ["A person is decorating a large, round, bright red cake with a white frosting base. The individual is wearing a black t-shirt with a cartoon character design and a Santa hat. They are seen sprinkling a bag of orange-colored, crispy snacks on top of the cake, adding a festive touch. The setting appears to be a modern kitchen with white cabinets and a countertop. The person's movements are deliberate and careful as they sprinkle the snacks onto the cake. The background remains static throughout the video, with no significant changes or movements. The main subject is a large, round cake with a white frosting base, placed on a black baking tray. The cake is being decorated by a person wearing a black t-shirt with a cartoon character design and a Santa hat. The person's hands are actively involved in sprinkling a bag of orange-colored, crispy snacks onto the cake. The snacks are being held and sprinkled in a controlled manner. The camera is stationary, providing a clear and steady view of the cake and the person's actions. The view is at a medium shot, capturing both the cake and the person's upper body and hands."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/3tvNPOH1FaE_36/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/3tvNPOH1FaE_36/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/3tvNPOH1FaE_36/first_frame.png']}, 72), kwargs keys: dict_keys([])

Validation DataLoader 0:  73%|███████▎  | 73/100 [00:00<00:00, 330.09it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['3d6CmVsTCNk_183'], 'prompt': ["A cartoon dog with a red body and a pink face, wearing a blue shirt, is seen holding a tennis racket and engaging in a playful and animated manner. The dog appears to be in a joyful mood, smiling and making various facial expressions. The background consists of a simple, orange-brick wall and a yellow trash can. The dog's movements are dynamic and lively, involving various poses and actions such as standing, jumping, and interacting with other animated characters. The dog's movements are energetic and varied, including standing still, jumping, and interacting with other animated characters. The dog's actions are quick and playful, with movements such as swinging the tennis racket, making facial expressions, and engaging with the other characters. The background remains static throughout the video. The main subject is a cartoon dog with a red body, pink face, and blue shirt. The dog is holding a tennis racket and is positioned centrally in the frame. The dog's expressions change frequently, showing joy and playfulness. Other animated characters, including a mouse and a cat, interact with the dog, adding to the dynamic nature of the scene. The camera is stationary, providing a consistent view of the scene from a medium shot angle, focusing on the main character and the immediate surroundings."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/3d6CmVsTCNk_183/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/3d6CmVsTCNk_183/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/3d6CmVsTCNk_183/first_frame.png']}, 73), kwargs keys: dict_keys([])

Validation DataLoader 0:  74%|███████▍  | 74/100 [00:00<00:00, 333.25it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['YopPcrYGeYI_29'], 'prompt': ["A young girl is seated at a table, engaging in a playful activity with a colorful drink. She is wearing a white t-shirt with a cartoon character design and has long dark hair. The girl is seen interacting with a pink drink with a cartoon pig face, using a straw and a small toy to create a playful scene. The setting is simple, with a blue background and a white table, and the girl's expressions and movements are lively and cheerful. The main subject is a young girl with long dark hair, wearing a white t-shirt with a cartoon character design. She is seated at a white table with three colorful drinks in front of her. The drinks have cartoon faces, including a pig, a cow, and a unicorn. The girl uses a straw and a small toy to interact with the drinks, creating a playful and engaging scene. The girl's movements are dynamic and playful. She uses her hands to manipulate the straw and toy, moving them around the drinks. Her facial expressions change frequently, showing excitement and enjoyment. The movements are moderate in amplitude, mostly confined to the table area, and the speed is lively and engaging. The background is a simple blue wall, providing a clean and uncluttered setting. The table is white, and there are a few small items on it, including a pink drink, a straw, and a small toy. The scene is well-lit, suggesting an indoor setting with good lighting."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/YopPcrYGeYI_29/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/YopPcrYGeYI_29/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/YopPcrYGeYI_29/first_frame.png']}, 74), kwargs keys: dict_keys([])

Validation DataLoader 0:  75%|███████▌  | 75/100 [00:00<00:00, 336.40it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['5xI0qX5ZOrc_340'], 'prompt': ['Two young women are engaged in a lively conversation and playful interaction outside a house. One woman, dressed in a yellow sweater with a cartoon character design and purple leggings, is animatedly gesturing and expressing herself with enthusiasm. The other woman, wearing a floral dress and black sneakers, is attentively listening and responding with smiles and laughter. The scene is set on a paved area in front of a house with a wooden door and decorative windows. A bicycle is parked to the left of the frame, and there are some potted plants and a blue hose visible in the background. The main subjects are two young women. The woman on the left is wearing a yellow sweater with a cartoon character design and purple leggings. She is gesturing with her hands and appears to be speaking animatedly. The woman on the right is wearing a floral dress and black sneakers. She is listening attentively, smiling, and occasionally laughing. They are standing close to each other, facing each other, and their interaction is filled with gestures and expressions of joy. The camera is stationary, capturing the scene from a medium shot that includes both women and part of the house in the background.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/5xI0qX5ZOrc_340/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/5xI0qX5ZOrc_340/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/5xI0qX5ZOrc_340/first_frame.png']}, 75), kwargs keys: dict_keys([])

Validation DataLoader 0:  76%|███████▌  | 76/100 [00:00<00:00, 339.22it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['7_HDt3dwEns_29'], 'prompt': ["a series of animated drawings featuring two characters, a male and a female, engaged in various playful and affectionate interactions. The male character, dressed in a checkered shirt and a hat, is depicted in different poses, sometimes sitting and sometimes standing, while the female character, wearing a dress and a hat, is also shown in various poses, including sitting and standing. The drawings are done in a black and white style, with the characters' expressions and body language conveying a sense of joy and camaraderie. The background is minimalistic and plain, focusing entirely on the characters and their interactions. The setting is not detailed, allowing the viewer to focus on the characters and their actions. The main subjects are two animated characters, a male and a female. The male character is dressed in a checkered shirt, a hat, and has curly hair. The female character is dressed in a dress with a bow, a hat, and has long, wavy hair. They are positioned close to each other, often interacting through gestures and expressions. The male character is often depicted in a seated position, while the female character is shown in various poses, including sitting and standing. The camera view is static, focusing on the characters from a frontal perspective. There is no camera movement, and the view remains consistent throughout the video."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/7_HDt3dwEns_29/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/7_HDt3dwEns_29/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/7_HDt3dwEns_29/first_frame.png']}, 76), kwargs keys: dict_keys([])

Validation DataLoader 0:  77%|███████▋  | 77/100 [00:00<00:00, 342.34it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['HEob5ChKXlo_9'], 'prompt': ["a cartoonish, animated scene of a large, colorful building with a blue and red exterior. The building has a unique design with a large, round window and a door that opens to reveal a small, dark interior. A small, green character with a round head and a large, round body appears and opens the door, revealing a small, cluttered room inside. The character then closes the door and walks away, leaving the building empty. The main object is a large, cartoonish building with a blue and red exterior. The building has a large, round window and a red door with a white frame. A small, green character with a round head and a large, round body appears and opens the door. The character then closes the door and walks away. The building remains stationary throughout the video. The background is a simple, bright green field with a few scattered green objects. The scene is set outdoors, with a clear sky in the background. The overall setting is simple and cartoonish, with no additional objects or elements. The camera is stationary, providing a fixed view of the building and the character's actions. The view is a medium shot, capturing the building and the character clearly."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/HEob5ChKXlo_9/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/HEob5ChKXlo_9/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/HEob5ChKXlo_9/first_frame.png']}, 77), kwargs keys: dict_keys([])

Validation DataLoader 0:  78%|███████▊  | 78/100 [00:00<00:00, 345.34it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['uCYxnOTG6t0_38'], 'prompt': ["a cartoon dog character engaged in a phone conversation while seated at a table. The dog, with a pink body and a black nose, is animated with expressive facial features and large, round eyes. The dog holds a black phone to its ear with its right paw and uses its left paw to manipulate a yellow iron. The background is a simple, plain yellow wall, emphasizing the dog's actions and expressions. The dog's movements are primarily focused on its facial expressions and hand movements. The dog's head tilts and turns slightly as it talks on the phone, and its eyes widen and narrow in response to the conversation. The dog's right paw remains steady on the phone, while its left paw moves to adjust the iron. The iron is moved slightly up and down, indicating the dog's interaction with it. The movements are moderate in amplitude and speed, with a clear focus on the dog's actions and expressions. The main subject is a cartoon dog character with a pink body, black nose, and large, expressive eyes. The dog is seated at a table with a yellow iron and a green and purple piece of clothing. The dog holds a black phone to its ear with its right paw and uses its left paw to manipulate the iron. The dog's facial expressions change throughout the video, indicating a conversation. The dog remains seated at the table throughout the video. The camera is stationary, providing a medium close-up view of the dog from the side, capturing the dog's facial expressions and hand movements clearly."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/uCYxnOTG6t0_38/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/uCYxnOTG6t0_38/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/uCYxnOTG6t0_38/first_frame.png']}, 78), kwargs keys: dict_keys([])

Validation DataLoader 0:  79%|███████▉  | 79/100 [00:00<00:00, 348.36it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['uHwGBw7gOmU_176'], 'prompt': ["A cute animated panda bear wearing a chef's hat and apron is seen in a kitchen setting, preparing dough. The panda is focused and diligent, rolling out the dough on a wooden table. The scene is filled with a warm, cozy atmosphere, highlighting the panda's endearing expressions and actions as it works. The background consists of a wooden kitchen table and a warm, cozy kitchen setting with a wooden chair. The background features a patterned wall with a mix of warm colors, adding to the homely and inviting atmosphere. The scene is well-lit, suggesting a bright and cheerful environment. The main subject is a panda bear character, characterized by its black and white fur, large round eyes, and a chef's hat with a blue and white striped band. The panda is wearing a blue apron with white buttons. It is positioned centrally in the frame, rolling out dough on a wooden table. The panda's expressions and actions are the focal point, showing its concentration and effort in the task. The panda's movements are deliberate and focused, involving rolling out the dough with its paws. The panda occasionally pauses to look at the dough, then continues rolling. The dough moves slightly as the panda rolls it out, and there is a slight dusting of flour visible on the table. The movements are moderate in amplitude and speed, emphasizing the panda's careful and precise actions. The camera is stationary, providing a medium close-up view of the panda bear, capturing its upper body and the table. The view is steady, focusing on the panda's actions and expressions without any noticeable movement or zoom."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/uHwGBw7gOmU_176/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/uHwGBw7gOmU_176/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/uHwGBw7gOmU_176/first_frame.png']}, 79), kwargs keys: dict_keys([])

Validation DataLoader 0:  80%|████████  | 80/100 [00:00<00:00, 351.42it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['OPYgZUxon_o_115'], 'prompt': ["a series of animated frames showcasing a character with blonde hair and blue eyes. The character's hair is long and flowing, and the character is wearing a white shirt. The frames transition smoothly, showing the character from various angles and expressions. The character's expressions range from neutral to smiling, and the background remains consistent throughout the video. The background is a plain, light blue color, providing a clean and simple backdrop that does not distract from the main character. There are no additional objects or elements in the background, keeping the focus on the character. The main subject is a character with blonde hair and blue eyes. The character is wearing a white shirt and has a friendly and approachable demeanor. The character's expressions change from neutral to smiling, and the character's hair flows naturally. The character is positioned centrally in the frame, and the camera angle shifts slightly to capture different perspectives of the character. The camera used appears to be a static camera with slight panning and tilting to capture different angles of the character. The view is a medium close-up, focusing on the character's upper body and face."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/OPYgZUxon_o_115/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/OPYgZUxon_o_115/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/OPYgZUxon_o_115/first_frame.png']}, 80), kwargs keys: dict_keys([])

Validation DataLoader 0:  81%|████████  | 81/100 [00:00<00:00, 354.40it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['kfQ1DsVyc-I_0'], 'prompt': ['a person holding a DVD cover of the animated film "Ice Age: A Mammoth Christmas" and rotating it to display different angles.The individual in the video is not fully visible, but their hands are the main focus. The hands appear to be of an adult, with fair skin and neatly trimmed nails. The person is holding the DVD cover with both hands, and their fingers are spread out to show the cover from various angles. The DVD cover itself is vibrant with a predominantly blue background and features the main characters of the film, including a saber-toothed tiger, a sloth, and a possum, all dressed in festive attire. The title "Ice Age: A Mammoth Christmas" is prominently displayed in white text with a red and white striped Santa hat graphic. Additional text on the cover includes "Blu-ray" and "DVD," indicating the format of the media.The setting is minimalistic, with a plain wooden surface that provides a neutral background, ensuring the focus remains on the DVD cover and the person\'s hands. The lighting is bright and even, casting soft shadows and highlighting the texture of the DVD cover and the person\'s skin.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/kfQ1DsVyc-I_0/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/kfQ1DsVyc-I_0/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/kfQ1DsVyc-I_0/first_frame.png']}, 81), kwargs keys: dict_keys([])

Validation DataLoader 0:  82%|████████▏ | 82/100 [00:00<00:00, 354.90it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['Q7HY73X4HdE_11'], 'prompt': ["A person is seen pouring a thick, yellow liquid from a large cylindrical container into a smaller container. The scene takes place in a workshop or garage with various tools and equipment visible in the background. The person is wearing a gray t-shirt with a cartoon character design and glasses, and they are seen handling the containers with care. The liquid appears to be a type of lubricant or oil, and the person is focused on ensuring an even distribution of the liquid into the smaller container. The main subjects are two cylindrical containers, one large and one small, filled with a thick, yellow liquid. The person, whose upper body is visible, is wearing a gray t-shirt with a cartoon character design and glasses. They are holding the large container and pouring the liquid into the smaller container. The person's hands and arms are the primary focus, showing the careful and deliberate motion of pouring the liquid. The background features a workshop or garage setting with various tools and equipment. There are shelves with various items, including bottles, a tire, and other tools. The floor is concrete, and there are some scattered droplets of the yellow liquid. The scene is well-lit, suggesting it is daytime. The camera is stationary, providing a medium close-up view of the person and the containers. The focus remains on the pouring action, capturing the details of the liquid and the containers."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/Q7HY73X4HdE_11/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/Q7HY73X4HdE_11/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/Q7HY73X4HdE_11/first_frame.png']}, 82), kwargs keys: dict_keys([])

Validation DataLoader 0:  83%|████████▎ | 83/100 [00:00<00:00, 354.53it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['MXZebGdVctY_62'], 'prompt': ['a playful and colorful scene featuring a large, animated red dragon and a small, animated pink bunny. The dragon, with its fierce expression and sharp teeth, is depicted in various dynamic poses, sometimes appearing to roar or move towards the bunny. The bunny, with a cheerful expression and a pink bow in its hair, is often seen standing or walking near the dragon. The background is a simple, bright green setting with a few scattered rocks and a tree, adding to the whimsical and cartoonish atmosphere of the scene. The main subjects are a large red dragon and a small pink bunny. The dragon is characterized by its red, scaly body, sharp teeth, and expressive eyes. It is often depicted with its mouth open, showing its sharp teeth, and sometimes with its arms raised or moving towards the bunny. The bunny is small, with a pink body, pink ears, and a pink bow in its hair. It is positioned near the dragon, sometimes standing or walking, and appears to interact with the dragon. The dragon and bunny are the focal points of the video, with the dragon being the larger and more dominant figure. The dragon exhibits various movements, including opening its mouth wide, raising its arms, and moving towards the bunny. These movements are dynamic and exaggerated, emphasizing the playful and animated nature of the scene. The bunny remains relatively stationary, with slight movements such as walking or standing, and occasionally interacting with the dragon. The background remains static, with no significant changes or movements. The camera remains mostly static, focusing on the dragon and bunny from a slightly elevated, side-on view, capturing their interactions and movements clearly.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/MXZebGdVctY_62/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/MXZebGdVctY_62/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/MXZebGdVctY_62/first_frame.png']}, 83), kwargs keys: dict_keys([])

Validation DataLoader 0:  84%|████████▍ | 84/100 [00:00<00:00, 354.28it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['4V9wUo4tvXc_62'], 'prompt': ["a dramatic scene featuring two animated characters engaged in a struggle. One character, a muscular man with a serious expression, is lying on the ground, while the other character, a green-skinned, masked figure, is standing over him, holding him down. The man appears to be in distress, struggling against the figure's grip. The setting is dark and gritty, suggesting a tense and intense atmosphere. The main subjects are two animated characters. The first character is a muscular man with a serious expression, wearing a white tank top and jeans. He is lying on his back on the ground, struggling against the second character. The second character is a green-skinned, masked figure with a menacing expression, wearing a green suit with armor. The figure is standing over the man, holding him down with both hands. The man's arms are raised, trying to break free, while the figure's grip is firm and unyielding. The background is a dark, gritty urban environment, likely a street or alley. The ground is rough and uneven, with scattered debris and small rocks. The lighting is dim, with a few bright spots that suggest streetlights or other sources of illumination. The scene is set at night, adding to the overall tense and dramatic atmosphere. The camera is stationary, providing a close-up view of the characters and their interaction. The angle is slightly elevated, giving a clear view of the characters' expressions and the intensity of the struggle."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/4V9wUo4tvXc_62/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/4V9wUo4tvXc_62/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/4V9wUo4tvXc_62/first_frame.png']}, 84), kwargs keys: dict_keys([])

Validation DataLoader 0:  85%|████████▌ | 85/100 [00:00<00:00, 352.29it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['0YD0zfDQOOM_18'], 'prompt': ["a person drawing a character on a piece of paper using a black pen. The character being drawn is a young girl with long hair, wearing a dress with a circular pattern and a cape. The artist's hand is steady and focused, moving the pen with precision to create the character's features and attire. The scene is intimate and detailed, capturing the artist's hand movements and the gradual development of the character's design. The main subject is a person's hand holding a black pen, drawing a character on a piece of paper. The artist's hand is positioned above the paper, moving the pen to draw the character's features and attire. The character's pose is dynamic, with the girl standing and holding her arm out. The background is a piece of white paper with a grid of black lines, likely a sketchbook or drawing pad. The paper is placed on a wooden surface, possibly a table or desk. The scene is well-lit, with natural light illuminating the drawing area. There are no additional objects or elements in the background, keeping the focus on the drawing process. The camera is stationary, providing a close-up view of the drawing process. The angle is slightly above the paper, capturing the artist's hand and the character being drawn in detail."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/0YD0zfDQOOM_18/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/0YD0zfDQOOM_18/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/0YD0zfDQOOM_18/first_frame.png']}, 85), kwargs keys: dict_keys([])

Validation DataLoader 0:  86%|████████▌ | 86/100 [00:00<00:00, 351.62it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['hUpkFe7DckY_31'], 'prompt': ['A young man stands on a red carpeted stage, delivering a presentation. He is dressed in a casual outfit consisting of a blue shirt, brown pants, and glasses. He holds a small, whiteboard-like sign with a cartoon drawing of a yellow character and text. The stage is adorned with two large red letters spelling "UB" on either side, and the background is dark, emphasizing the subject. The young man moves minimally, primarily shifting his weight and adjusting his posture slightly as he speaks. His movements are slow and deliberate, focusing on his speech. The background remains static throughout the video, with no changes or movements. The main subject is a young man wearing a blue shirt, brown pants, and glasses. He holds a small whiteboard sign with a cartoon drawing of a yellow character and text. He stands on a red carpeted stage, positioned centrally in the frame. His movements are minimal, primarily involving slight shifts in his posture and gestures as he speaks. The camera is stationary, capturing the subject from a frontal view at a medium distance, maintaining a steady and clear focus on the young man throughout the video.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/hUpkFe7DckY_31/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/hUpkFe7DckY_31/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/hUpkFe7DckY_31/first_frame.png']}, 86), kwargs keys: dict_keys([])

Validation DataLoader 0:  87%|████████▋ | 87/100 [00:00<00:00, 351.43it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['bz4v5oCsfRs_46'], 'prompt': ["A man is seen interacting with a large snake inside a white bathtub. The man, wearing a brown shirt and beige shorts, is blindfolded and appears to be trying to handle the snake. The snake, which is coiled and partially out of the tub, moves around the man as he adjusts his position. The setting is colorful and playful, with a large, smiling cartoon character painted on the wall behind them. The man's movements are slow and deliberate as he adjusts his position in the tub and interacts with the snake. The snake moves in a coiled, undulating manner, occasionally uncoiling and moving around the man. The background remains static, with no significant changes or movements. The main subjects are a man and a large snake. The man is wearing a brown shirt and beige shorts, and he is blindfolded. He is seated in a white bathtub, trying to handle the snake, which is coiled and partially out of the tub. The snake is large, with a patterned skin, and it moves around the man as he adjusts his position. The man's movements are slow and careful, while the snake's movements are more dynamic and fluid. The camera is stationary, providing a clear and stable view of the scene from a slightly elevated angle, capturing the entire interaction between the man and the snake."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/bz4v5oCsfRs_46/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/bz4v5oCsfRs_46/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/bz4v5oCsfRs_46/first_frame.png']}, 87), kwargs keys: dict_keys([])

Validation DataLoader 0:  88%|████████▊ | 88/100 [00:00<00:00, 351.71it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['rF6pkBQs3tQ_157'], 'prompt': ["a character with pink hair and futuristic armor, holding a spherical object with a green and black pattern. The character is engaged in a conversation with two other characters, one with red hair and another with green hair. The setting appears to be a kitchen with a stove and various kitchen appliances in the background. The character with pink hair is animated and expressive, gesturing with the spherical object and speaking to the other characters. The background consists of a kitchen setting with a stove, a microwave, and various kitchen appliances. The scene is set in a dimly lit environment, giving it a somewhat mysterious or futuristic feel. The lighting is warm, with a focus on the characters and the objects they are interacting with. The main character is animated and expressive, holding the spherical object with both hands and moving it slightly while speaking. The character with red hair is mostly stationary, occasionally moving slightly to the side. The character with green hair is also mostly stationary, observing the interaction. The movements are moderate in amplitude, with the main character's gestures being the most dynamic. The camera is stationary, providing a medium shot that captures the main character and the two other characters in the frame. The view is slightly angled, focusing on the interaction between the characters."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/rF6pkBQs3tQ_157/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/rF6pkBQs3tQ_157/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/rF6pkBQs3tQ_157/first_frame.png']}, 88), kwargs keys: dict_keys([])

Validation DataLoader 0:  89%|████████▉ | 89/100 [00:00<00:00, 352.52it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['raGPZPdt-HI_25'], 'prompt': ["a cartoon character with a distinctive appearance, characterized by a large, expressive face, a thick beard, and a colorful outfit. The character is animated and appears to be in a lively and animated state, possibly engaging in a conversation or expressing emotions. The character's movements are dynamic, with frequent changes in facial expressions and hand gestures, suggesting a lively and engaging interaction. The background is a simple, out-of-focus green and brown landscape, suggesting an outdoor setting. There are no distinct objects or structures in the background, which keeps the focus on the animated character. The scene appears to be set in a natural environment, possibly a forest or a park. The main subject is a cartoon character with a large, expressive face, a thick beard, and a colorful outfit. The character is wearing a blue shirt with a floral pattern and a yellow necklace with a large pendant. The character's hands are often raised or gesturing, and the character's facial expressions change frequently, indicating a range of emotions. The character is positioned centrally in the frame and occupies most of the visual space. The camera is static, focusing on a medium close-up view of the character, capturing the upper body and face in detail. There is no noticeable camera movement, maintaining a steady and consistent view of the animated character."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/raGPZPdt-HI_25/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/raGPZPdt-HI_25/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/raGPZPdt-HI_25/first_frame.png']}, 89), kwargs keys: dict_keys([])

Validation DataLoader 0:  90%|█████████ | 90/100 [00:00<00:00, 351.72it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['3Xa0YD9VhIE_11'], 'prompt': ["an animated character, resembling Finn from the animated series Adventure Time, navigating through a forest filled with trees and scattered debris. The character is seen pushing and pulling wooden logs, creating a makeshift bridge or pathway. The scene is set in a whimsical, cartoonish forest with vibrant pink trees and scattered objects, including a cake and a small animal. The character's movements are deliberate and careful as they maneuver the logs, showcasing a sense of determination and resourcefulness. The background consists of a whimsical forest with tall, pink-colored trees and scattered debris. The ground is covered with grass and small rocks, and there are additional trees and objects scattered throughout the scene. The forest appears to be in a cartoonish, stylized setting, with vibrant colors and a playful atmosphere. The main subject is Finn, an animated character with blue shorts, a green backpack, and a light blue shirt. He is interacting with wooden logs, pushing and pulling them to create a pathway. The character is positioned centrally in the frame, moving from left to right, and is focused on his task. The logs are positioned horizontally across the ground, and Finn is seen pushing and pulling them to create a makeshift bridge. The camera is stationary, providing a wide-angle view of the scene, capturing the entirety of Finn's actions and the surrounding forest environment."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/3Xa0YD9VhIE_11/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/3Xa0YD9VhIE_11/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/3Xa0YD9VhIE_11/first_frame.png']}, 90), kwargs keys: dict_keys([])

Validation DataLoader 0:  91%|█████████ | 91/100 [00:00<00:00, 351.05it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['CNPcFgo8StY_26'], 'prompt': ["A young woman with long blonde hair is featured in a series of video frames, showcasing various poses and expressions. She is dressed in a white blouse with a playful pattern of cartoon characters and is seen smiling, making heart shapes with her hands, and adjusting her hair. The background is minimalistic, featuring a softly lit, white wall with string lights that add a cozy and warm ambiance to the scene. The woman's movements are smooth and deliberate, involving slight head tilts, hand gestures, and hair adjustments. She transitions from a neutral expression to smiling and making heart shapes with her hands, then adjusting her hair. The movements are slow and graceful, emphasizing her relaxed and confident demeanor. The background remains static throughout the video, with no changes or movements. The main subject is a young woman with long blonde hair, wearing a white blouse with cartoon character prints. She is positioned centrally in the frame and is the focal point of the video. Her expressions range from neutral to smiling, and she engages in various poses, including making heart shapes with her hands and adjusting her hair. The camera is stationary, maintaining a medium close-up view of the woman, capturing her upper body and face clearly."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/CNPcFgo8StY_26/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/CNPcFgo8StY_26/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/CNPcFgo8StY_26/first_frame.png']}, 91), kwargs keys: dict_keys([])

Validation DataLoader 0:  92%|█████████▏| 92/100 [00:00<00:00, 349.47it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['V_NJAJGCKD8_52'], 'prompt': ["A man is speaking animatedly in front of a map backdrop. He is wearing a grey cardigan over a blue t-shirt and has a microphone attached to his shirt. His hands are expressive, moving in various gestures as he speaks, indicating emphasis and engagement with his topic. The background features a map with various illustrated figures, suggesting a theme related to travel or exploration. The man's movements are primarily focused on his hands and upper body. He uses expressive hand gestures, moving them up and down, side to side, and occasionally pointing. His head and facial expressions also change slightly as he speaks, indicating different points in his speech. The background remains static throughout the video. The main subject is a man with short hair, a beard, and a friendly demeanor. His hands are actively moving, indicating he is speaking and emphasizing his points. He is positioned centrally in the frame, facing the camera directly. The camera is stationary, capturing the man from a medium shot that focuses on his upper body and face. The view is direct and centered, allowing the viewer to clearly see the man's expressions and gestures."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/V_NJAJGCKD8_52/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/V_NJAJGCKD8_52/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/V_NJAJGCKD8_52/first_frame.png']}, 92), kwargs keys: dict_keys([])

Validation DataLoader 0:  93%|█████████▎| 93/100 [00:00<00:00, 348.65it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['VDH88glioFE_22'], 'prompt': ['a sequence of animated characters, likely from a comic book or animated series, engaging in a conversation or interaction.The characters are stylized with exaggerated features typical of animated representations. The central figure is a muscular, red-armored character with a stern expression, wearing a helmet with a visor and a chest plate with a circular emblem. To the left, there is a character with a beard, wearing a red and black outfit with a red cape, and to the right, a character with a muscular build, wearing a blue and white costume with a star emblem on the chest.The setting appears to be a corridor with a metallic, industrial design, featuring a patterned floor and walls that suggest a high-tech environment. The lighting is warm and artificial, contributing to the overall digital and animated aesthetic of the scene.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/VDH88glioFE_22/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/VDH88glioFE_22/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/VDH88glioFE_22/first_frame.png']}, 93), kwargs keys: dict_keys([])

Validation DataLoader 0:  94%|█████████▍| 94/100 [00:00<00:00, 348.99it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['FhhLAmcQkOQ_27'], 'prompt': ["A young woman with long brown hair is sitting in a well-lit room, engaging in a conversation while holding a colorful book. She wears a grey t-shirt with a Chicago Blackhawks logo and gestures animatedly with her hands, using expressive hand movements to emphasize her points. The background features a large mirror with a decorative frame, flanked by two vases with white flowers, creating a visually appealing and organized setting. The woman's movements are primarily focused on her hands and arms, which she uses to gesture and point while talking. Her hand movements are varied, including pointing, waving, and making expressive gestures. The background remains static, with no changes or movements. The main subject is a young woman with long brown hair, wearing a grey t-shirt with a Chicago Blackhawks logo. She is holding a colorful book with cartoon characters on the cover. Throughout the video, she uses her hands to gesture and point, indicating she is explaining or discussing something. The book is held in front of her, and she occasionally looks at it while speaking. The camera is stationary, providing a medium shot that captures the woman from the waist up, allowing for a clear view of her expressions and gestures."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/FhhLAmcQkOQ_27/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/FhhLAmcQkOQ_27/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/FhhLAmcQkOQ_27/first_frame.png']}, 94), kwargs keys: dict_keys([])

Validation DataLoader 0:  95%|█████████▌| 95/100 [00:00<00:00, 351.21it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['ANABpEAwRz4_305'], 'prompt': ['a whimsical animated scene set in a brightly colored, cartoonish street. A dog character, dressed in a red hat and red shirt, is seen interacting with various animated characters. The dog character is initially seen holding a large hammer, which it swings at a cat character. The scene then transitions to the dog character riding a white horse, accompanied by a character dressed in a red hat and red coat, who is holding an umbrella. The background features a cheerful street with a yellow house, a blue house, and a green house, all with colorful windows and doors. The scene is set in a brightly colored, cartoonish street with a yellow house on the left, a blue house in the middle, and a green house on the right. The houses have colorful windows and doors, and there is a street with a sidewalk. The background also includes a few animated characters, including a cat character and a dog character, adding to the lively and playful atmosphere. The main subjects are the dog character and the horse. The dog character is wearing a red hat and red shirt, and it initially holds a large hammer. The horse is white with a brown mane and tail, and it is ridden by a character dressed in a red hat and red coat, who holds an umbrella. The dog character and the horse move from the foreground to the background, with the dog character swinging the hammer and then riding the horse. The dog character swings the hammer with a moderate speed, moving it from the left to the right side of the frame. The horse walks slowly from the right to the left side of the frame. The background characters move slightly, but their movements are minimal and do not distract from the main action. The camera is stationary, capturing the entire scene from a fixed viewpoint.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/ANABpEAwRz4_305/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/ANABpEAwRz4_305/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/ANABpEAwRz4_305/first_frame.png']}, 95), kwargs keys: dict_keys([])

Validation DataLoader 0:  96%|█████████▌| 96/100 [00:00<00:00, 353.38it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['XpxJupH3lB4_20'], 'prompt': ['a whimsical scene with three animated characters drawn in a simple, cartoonish style. The characters are situated in a kitchen setting, with various kitchen items and decorations visible in the background. The characters interact with each other, engaging in playful and humorous actions. their movements and expressions, creating a light-hearted and entertaining atmosphere. The main subjects are three animated characters. The first character is a large, smiling creature with a round body and a large, cheerful smile. The second character is a smaller, more reserved creature with a round body and a simple, content expression. The third character is a small, round creature with a large, surprised expression. These characters are positioned close to each other, interacting and moving around. The background features a kitchen setting with various kitchen items and decorations. There are pots, pans, a stove, and other kitchen utensils. The scene is set against a light, beige background, giving it a warm and cozy feel. The kitchen items are arranged in a way that suggests a well-used and lived-in space. The camera is stationary, providing a fixed view of the scene. The perspective is slightly angled, capturing the characters and the background in a single, continuous shot.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/XpxJupH3lB4_20/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/XpxJupH3lB4_20/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/XpxJupH3lB4_20/first_frame.png']}, 96), kwargs keys: dict_keys([])

Validation DataLoader 0:  97%|█████████▋| 97/100 [00:00<00:00, 355.74it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['D_gn7PbFKdg_372'], 'prompt': ["a playful and animated scene featuring a small pink dog and a large green dog interacting in a cozy, orange-patterned living room. The small dog is sitting on a couch, talking on a phone, while the large green dog approaches from the right, seemingly excited and eager to engage. The small dog appears surprised and slightly nervous as the large green dog leans in closer, eventually licking the small dog's face. The scene is filled with vibrant colors and expressive character designs, creating a lively and engaging atmosphere. The main subjects are two animated dogs. The small pink dog is sitting on a couch, holding a phone to its ear, and appears to be talking. The large green dog is standing on the right side of the frame, leaning in towards the small dog, showing excitement and anticipation. The small dog's facial expressions change from surprise to nervousness as the large dog gets closer. The background is a cozy living room with an orange-patterned couch and a window with orange curtains. The room has a warm and inviting atmosphere, with a pink cushion on the couch and a small pink object on the floor. The scene is set indoors, with a bright and cheerful ambiance. The camera is stationary, providing a medium shot that captures both the small dog on the couch and the large green dog approaching from the right."], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/D_gn7PbFKdg_372/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/D_gn7PbFKdg_372/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/D_gn7PbFKdg_372/first_frame.png']}, 97), kwargs keys: dict_keys([])

Validation DataLoader 0:  98%|█████████▊| 98/100 [00:00<00:00, 356.94it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['uI_hV8-KwEA_10'], 'prompt': ['A person is standing on a stage, presenting or speaking in front of an audience.The individual is wearing a purple hoodie with a white logo on the left chest area, dark pants, and black shoes. The person\'s hair is dark and appears to be shoulder-length. The attire suggests a casual yet possibly branded look, possibly indicating a gaming or entertainment context.The setting is a stage with a red backdrop, which is illuminated by bright stage lights. There is a large screen displaying a cartoon-style character with the text "HIBYMC" in the background. The stage has a patterned carpet, and the audience is seated in the dark, indicating that the event is likely indoors and possibly during the evening.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/uI_hV8-KwEA_10/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/uI_hV8-KwEA_10/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/uI_hV8-KwEA_10/first_frame.png']}, 98), kwargs keys: dict_keys([])

Validation DataLoader 0:  99%|█████████▉| 99/100 [00:00<00:00, 357.63it/s][A[OmniTrainingModule] validation_step, args: ({'video_id': ['XhpGvJsRuTk_25'], 'prompt': ['a cartoon character, a young girl with orange hair tied in a ponytail, wearing a red and orange outfit. She is standing in front of a green, glowing computer monitor with a large blue fan inside it. The girl is holding a small, thin, and long object, possibly a stick or a wand, and appears to be interacting with the computer. Her facial expressions change from neutral to smiling as she looks at the monitor. The background is a simple, solid green circle that frames the scene, giving it a clean and focused look. The green color of the background complements the green glow of the computer monitor, creating a cohesive and visually appealing environment. There are no additional objects or elements in the background, keeping the focus on the main subjects. The main subject is a young girl with orange hair tied in a ponytail, wearing a red and orange outfit. She is holding a small, thin, and long object, possibly a stick or a wand, in her right hand. The girl is positioned in front of a green, glowing computer monitor with a large blue fan inside it. The girl remains relatively stationary, with minor movements of her head and hand as she interacts with the computer. The fan inside the monitor rotates slowly, adding a dynamic element to the otherwise static scene. The movements are smooth and deliberate, emphasizing the interaction between the girl and the computer. The camera is stationary, providing a fixed view of the scene from a frontal perspective. There is no camera movement, ensuring that the focus remains on the girl and the computer monitor.'], 'video_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/XhpGvJsRuTk_25/video.mp4'], 'audio_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/XhpGvJsRuTk_25/audio.wav'], 'first_frame_path': ['/mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1/videos/Koala_36M_1_sv/XhpGvJsRuTk_25/first_frame.png']}, 99), kwargs keys: dict_keys([])

Validation DataLoader 0: 100%|██████████| 100/100 [00:00<00:00, 358.33it/s][A
                                                                           [AEpoch 0:   5%|▌         | 5/100 [02:26<46:28,  0.03it/s, v_num=44, train_loss_step=nan.0][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 5, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[Timer] VAE编码耗时: 3.621 秒
[Timer] forward_preprocess 总耗时: 3.840 秒
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=True, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([331.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ['a whimsical animated scene set in a brightly colored, cartoonish street. A dog character, dressed in a red hat and red shirt, is seen interacting with various animated characters. The dog character is initially seen holding a large hammer, which it swings at a cat character. The scene then transitions to the dog character riding a white horse, accompanied by a character dressed in a red hat and red coat, who is holding an umbrella. The background features a cheerful street with a yellow house, a blue house, and a green house, all with colorful windows and doors. The scene is set in a brightly colored, cartoonish street with a yellow house on the left, a blue house in the middle, and a green house on the right. The houses have colorful windows and doors, and there is a street with a sidewalk. The background also includes a few animated characters, including a cat character and a dog character, adding to the lively and playful atmosphere. The main subjects are the dog character and the horse. The dog character is wearing a red hat and red shirt, and it initially holds a large hammer. The horse is white with a brown mane and tail, and it is ridden by a character dressed in a red hat and red coat, who holds an umbrella. The dog character and the horse move from the foreground to the background, with the dog character swinging the hammer and then riding the horse. The dog character swings the hammer with a moderate speed, moving it from the left to the right side of the frame. The horse walks slowly from the right to the left side of the frame. The background characters move slightly, but their movements are minimal and do not distract from the main action. The camera is stationary, capturing the entire scene from a fixed viewpoint.'], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=True, inf=False, value=nan
[OmniTrainingModule] training_step -> loss: nan
Epoch 0:   6%|▌         | 6/100 [02:35<40:35,  0.04it/s, v_num=44, train_loss_step=nan.0]Epoch 0:   6%|▌         | 6/100 [02:35<40:35,  0.04it/s, v_num=44, train_loss_step=nan.0][OmniTrainingModule] training_step -> batch keys: dict_keys(['video_id', 'prompt', 'video_path', 'audio_path', 'first_frame_path', 'video', 'audio', 'L', 'T']), batch_idx: 6, batch_size: 1
[WanVideoPipeline] encode_video -> input_video device: cuda:0, dtype: torch.float16
[WanVideoVAE] encode: videos torch.Size([1, 3, 25, 400, 640]), device cuda:0, tiled True, tile_size (34, 34), tile_stride (18, 16)
[Timer] VAE编码耗时: 3.092 秒
[Timer] forward_preprocess 总耗时: 3.156 秒
[OmniTrainingModule forward_preprocess] -> batch_inputs ready, input_latents shape: torch.Size([1, 16, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752]), noise shape: torch.Size([1, 16, 7, 50, 80])
[Check] input_latents: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[Check] audio_emb: nan=True, inf=False, shape=torch.Size([1, 25, 10752])
[Check] noise: nan=False, inf=False, shape=torch.Size([1, 16, 7, 50, 80])
[WanVideoPipeline] training_loss -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise']), self.torch_dtype = torch.float16, self.device = cuda:0
[WanVideoPipeline] training_loss -> self.scheduler.num_train_timesteps: 1000, len(timesteps): 100, timesteps: tensor([1000.0000,  997.9838,  995.9349,  993.8525,  991.7355,  989.5833,
         987.3949,  985.1695,  982.9059,  980.6034,  978.2609,  975.8771,
         973.4514,  970.9821,  968.4685,  965.9091,  963.3028,  960.6483,
         957.9440,  955.1887,  952.3810,  949.5193,  946.6020,  943.6274,
         940.5941,  937.5000,  934.3434,  931.1225,  927.8350,  924.4792,
         921.0526,  917.5532,  913.9785,  910.3261,  906.5934,  902.7778,
         898.8763,  894.8864,  890.8046,  886.6280,  882.3529,  877.9763,
         873.4940,  868.9025,  864.1975,  859.3750,  854.4304,  849.3589,
         844.1558,  838.8158,  833.3333,  827.7026,  821.9177,  815.9722,
         809.8591,  803.5715,  797.1015,  790.4412,  783.5821,  776.5152,
         769.2307,  761.7188,  753.9683,  745.9678,  737.7049,  729.1666,
         720.3389,  711.2068,  701.7543,  691.9643,  681.8182,  671.2963,
         660.3774,  649.0385,  637.2549,  625.0000,  612.2449,  598.9583,
         585.1064,  570.6522,  555.5555,  539.7728,  523.2557,  505.9524,
         487.8049,  468.7500,  448.7180,  427.6316,  405.4054,  381.9445,
         357.1428,  330.8824,  303.0303,  273.4375,  241.9355,  208.3333,
         172.4138,  133.9286,   92.5926,   48.0769])
[WanVideoPipeline] model_fn -> input keys: dict_keys(['input_latents', 'image_emb', 'prompt', 'audio_emb', 'noise', 'latents'])
[WanVideoPipeline] model_fn -> latents shape: torch.Size([1, 16, 7, 50, 80]), timestep: tensor([878.], device='cuda:0', dtype=torch.float16), audio_emb shape: torch.Size([1, 25, 10752]), prompt: ["A colorful, animated vehicle, resembling a vintage car, drives down a road lined with trees and scattered with variously colored eggs.The vehicle is brightly colored with a yellow body, a red roof, and a yellow and red striped pattern on the side. It has large, round, colorful wheels and a whimsical design with a face on the front, featuring a large, round, yellow eye and a small, red mouth. The car's windows are adorned with colorful, striped patterns.The setting is a whimsical, cartoonish landscape with a road that appears to be made of wooden planks. The road is lined with trees that have a festive appearance, decorated with colorful ornaments and wrapped in ribbons. Scattered along the road are variously colored eggs, adding to the festive atmosphere. The sky is a bright blue with fluffy white clouds, suggesting a cheerful, sunny day."], image_emb keys: dict_keys(['y'])
[WanVideoPipeline] model_fn -> run dit...
[WanModel] Forward pass with x shape: torch.Size([1, 16, 7, 50, 80]), timestep: torch.Size([1]), context shape: torch.Size([1, 512, 4096]), y shape: torch.Size([1, 17, 7, 50, 80]), audio_emb shape: torch.Size([1, 25, 10752])
[AudioPack forward] vid shape: torch.Size([1, 10752, 28, 1, 1]), t, h, w: 4, 1, 1
[WanModel] x shape: torch.Size([1, 33, 7, 50, 80])
[WanModel] After patch embedding, x shape: torch.Size([1, 1536, 7, 25, 40])
[WanVideoPipeline] model_fn -> noise_pred_posi shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[WanVideoPipeline] training_loss -> noise_pred shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0, training_target shape: torch.Size([1, 16, 7, 50, 80]), dtype: torch.float16, device: cuda:0
[Check] loss: nan=True, inf=False, value=nan
[OmniTrainingModule] training_step -> loss: nan
Epoch 0:   7%|▋         | 7/100 [02:43<36:06,  0.04it/s, v_num=44, train_loss_step=nan.0]Epoch 0:   7%|▋         | 7/100 [02:43<36:06,  0.04it/s, v_num=44, train_loss_step=nan.0][OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 359 to 384
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 0 to 25
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 172 to 197
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 86 to 111
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 103 to 128
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 37 to 62
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 100 to 125
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 68 to 93
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 272 to 297
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 75 to 100
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 49 to 74
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 36 to 61
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 125 to 150
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 80 to 105
[OmniTrainingModule forward_preprocess] -> Video longer than max_frame, crop from 271 to 296
