# 预训练模型路径
dtype: "16-mixed" # TODO bf16
text_encoder_path: ../pretrained_models/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth
image_encoder_path: None
# dit 是扩散生成的主模型（Diffusion Transformer），负责内容生成。（transformer）
# vae 是编码/解码器，负责数据和 latent 空间的转换。（encoder-decoder）
dit_path: ../pretrained_models/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors
vae_path: ../pretrained_models/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth
wav2vec_path: ../pretrained_models/wav2vec2-base-960h
exp_path: ../pretrained_models/OmniAvatar-1.3B
num_persistent_param_in_dit:  # You can set `num_persistent_param_in_dit` to a small number to reduce VRAM required. 
reload_cfg: True
sp_size: 1

# 推理参数
input_file: "examples/red_hat_2.txt"
num_steps: 25
infer_ckpt_path: "/mnt/hdd4/huanglingyu/vgg/OmniAvatar/outputs/train_1.3B/checkpoints/step=5000.ckpt/merged/pytorch_model.bin"

# 训练参数
seed: 42
dataset_base_path: /mnt/hdd2/huanglingyu/vgg/datasets/Koala-36M-v1
name: train_1.3B
savedir: /mnt/hdd2/huanglingyu/vgg/OmniAvatar/outputs
fps: 25
sample_rate: 16000
batch_size: 1 # 最高可以开6, 8会OOM, 7不知道行不行
# nodes代表分布式训练的节点数，devices代表每个节点上的GPU数量
nodes: 1
devices: 1
num_train_epochs: 1
mode: train
checkpoint_path: "/home/huanglingyu/data/vgg/OmniAvatar/outputs/train_1.3B/checkpoints/step=5000.ckpt"
lr: 1e-4
max_frames: 120
debug: False
debug_data_len: 20
use_checkpoint: True
train_log: False
ckpt_log: False